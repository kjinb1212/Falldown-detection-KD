{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "AGC",
      "language": "python",
      "name": "agc"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "KD_train.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kjinb1212/Falldown-detection-KD/blob/main/KD_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfVQa9NCYbxl"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from glob import glob\n",
        "import os\n",
        "import sys\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwMagxHrYbxp"
      },
      "source": [
        "# KD train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmIm2Fn1Ybxq"
      },
      "source": [
        "def loss_fn_kd(outputs, labels, teacher_outputs, T, alpha):\n",
        "    KD_loss = nn.KLDivLoss()(F.log_softmax(outputs/T, dim=1), \n",
        "                             F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T) + \\\n",
        "                             F.cross_entropy(outputs, labels) * (1. - alpha)\n",
        "\n",
        "    return KD_loss\n",
        "\n",
        "def get_teacher_output(model, loader):\n",
        "    model.eval()\n",
        "    output = []\n",
        "    with torch.no_grad():\n",
        "        for data, _ in loader:\n",
        "            data = data.to(device)\n",
        "            output.append(model(data))\n",
        "    torch.cuda.empty_cache()\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRXRff_zYbxq"
      },
      "source": [
        "def train_kd(model, teacher_output, train_loader, test_loader, criterion, \n",
        "             optimizer, epochs, T, alpha, save_name):\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
        "                                                          patience=4, verbose=True)\n",
        "    best_loss = None\n",
        "    best_acc = None\n",
        "    patience = 0\n",
        "\n",
        "    history = {'loss': [], 'acc': []}\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        print(\"--------- epoch : {} ------------\".format(epoch+1))\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for i, (data, label) in enumerate(train_loader):\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = loss_fn_kd(output, label, teacher_output[i], T, alpha)\n",
        "            train_losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            torch.cuda.empty_cache()\n",
        "            \n",
        "        train_loss = np.average(train_losses)\n",
        "        print(\"train loss: {}\".format(train_loss))\n",
        "        \n",
        "        \n",
        "        model.eval()\n",
        "        test_losses = []\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for i, (data, label) in enumerate(test_loader):\n",
        "                data = data.to(device)\n",
        "                label = label.to(device)\n",
        "\n",
        "                output = model(data)\n",
        "                loss = criterion(output, label)\n",
        "                test_losses.append(loss.item())\n",
        "                _, predict = torch.max(output.data, 1)\n",
        "                correct += (predict == label).sum().item()\n",
        "                total += label.size(0)\n",
        "                \n",
        "        test_loss = np.average(test_losses)\n",
        "        test_acc = 100 * correct / total\n",
        "        print(\"test loss: {}, \\t test acc: {}%\".format(test_loss, test_acc))\n",
        "\n",
        "        history['loss'].append(test_loss)\n",
        "        history['acc'].append(test_acc)\n",
        "        \n",
        "        if (best_loss is None) or (best_loss > test_loss):\n",
        "            best_loss = test_loss\n",
        "            best_acc = test_acc\n",
        "            torch.save(model.state_dict(), '3_model_weights/'+ save_name +'.pth')\n",
        "            print('Best loss: {}\\n'.format(best_loss))\n",
        "            patience = 0\n",
        "        else:\n",
        "            patience += 1\n",
        "        \n",
        "        if patience > 7:\n",
        "            print(\"early stop at {} epoch\".format(epoch + 1))\n",
        "            break\n",
        "            \n",
        "        scheduler.step(metrics=test_loss)\n",
        "   \n",
        "    print(\"best loss: {},\\t best acc: {}%\\n\\n\".format(best_loss, best_acc))\n",
        "    return best_loss, best_acc, history        \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csq8hF-UYbxr"
      },
      "source": [
        "# Custom Datasaet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLK3K4ivYbxr"
      },
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, input_size, train = True, padding = True, normalize = False,\n",
        "                 bright_ness = 0.2, hue = 0.15, contrast = 0.15, random_Hflip = 0.3, rotate_deg = 20):\n",
        "        orig_normal_path = glob(os.path.join(root_dir, 'normal') + '/*.jpg')\n",
        "        orig_fall_path = glob(os.path.join(root_dir, 'falldown') + '/*.jpg')\n",
        "        orig_back_path = glob(os.path.join(root_dir, 'background') + '/*.jpg')\n",
        "        \n",
        "        normal_paths = []\n",
        "        fall_paths = []\n",
        "        back_paths = []\n",
        "        \n",
        "        for path in orig_normal_path:\n",
        "            img = Image.open(path)\n",
        "            if min(img.size[0], img.size[1]) < 32:\n",
        "                pass\n",
        "            else:\n",
        "                normal_paths.append(path)\n",
        "                \n",
        "        for path in orig_fall_path:\n",
        "            img = Image.open(path)\n",
        "            if min(img.size[0], img.size[1]) < 32:\n",
        "                pass\n",
        "            else:\n",
        "                fall_paths.append(path)\n",
        "        \n",
        "        for path in orig_back_path:\n",
        "            img = Image.open(path)\n",
        "            if min(img.size[0], img.size[1]) < 32:\n",
        "                pass\n",
        "            else:\n",
        "                back_paths.append(path)\n",
        "                        \n",
        "        self.total_paths = normal_paths + fall_paths + back_paths\n",
        "        self.labels = [0] * len(normal_paths) + [1] * len(fall_paths) + [2] * len(back_paths)\n",
        "        \n",
        "        transform = []\n",
        "        if train:\n",
        "            #transform.append(torchvision.transforms.ColorJitter(brightness=bright_ness, hue=hue, contrast=contrast))\n",
        "            transform.append(torchvision.transforms.RandomHorizontalFlip(p=random_Hflip))\n",
        "            #transform.append(torchvision.transforms.RandomCrop(224))\n",
        "            transform.append(torchvision.transforms.RandomRotation(degrees=rotate_deg))\n",
        "        \n",
        "        transform.append(torchvision.transforms.Resize((input_size, input_size)))\n",
        "        transform.append(torchvision.transforms.ToTensor())\n",
        "        self.transform = torchvision.transforms.Compose(transform)\n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.total_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(self.total_paths[index])\n",
        "        img = self.transform(img)\n",
        "        return img, self.labels[index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlBY0THSYbxs"
      },
      "source": [
        "# student model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98HraxULYbxt"
      },
      "source": [
        "class CNN_layers(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN_layers, self).__init__()      \n",
        "        self.conv1 = nn.Conv2d(3, 16, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
        "        self.conv4 = nn.Conv2d(64, 32, 3)\n",
        "\n",
        "        self.fc1 = nn.Linear(32 * 8 * 8, 16)\n",
        "        self.fc2 = nn.Linear(16, 8)\n",
        "        self.fc3 = nn.Linear(8, 3)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.bn4 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.bn6 = nn.BatchNorm1d(16)\n",
        "        self.bn7 = nn.BatchNorm1d(8)\n",
        "        self.padding = nn.ZeroPad2d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(self.padding(x))))) # 128 -> 64\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(self.padding(x))))) # 64 -> 32\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(self.padding(x))))) # 32 -> 16\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(self.padding(x))))) # 16 -> 8\n",
        "\n",
        "        x = x.view(-1, 32 * 8 * 8)\n",
        "        x = F.relu(self.bn6(self.fc1(x)))\n",
        "        x = F.relu(self.bn7(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzKNjJ9lYbxt"
      },
      "source": [
        "# create data loader "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6llXJt4Ybxt"
      },
      "source": [
        "INPUT_SIZE = 128\n",
        "PADDING = False\n",
        "NORMALIZE = False\n",
        "BATCHSIZE = 128\n",
        "NUMEPOCH = 100\n",
        "\n",
        "train_data = CustomDataset(\n",
        "    root_dir='train',\n",
        "    input_size=INPUT_SIZE, train=True, padding=PADDING, normalize=NORMALIZE,\n",
        "    bright_ness=0, hue=01.5, contrast=0.15, random_Hflip=0, rotate_deg=0)\n",
        "\n",
        "test_data = CustomDataset(\n",
        "    root_dir='validation',\n",
        "    input_size=INPUT_SIZE, train=False, padding=PADDING, normalize=NORMALIZE,\n",
        "    bright_ness=0, hue=01.5, contrast=0.15, random_Hflip=0, rotate_deg=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No-sWUBaYbxt"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCHSIZE, shuffle=True, num_workers=70, drop_last=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCHSIZE, shuffle=False, num_workers=70, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZExVZjgYbxu"
      },
      "source": [
        "# train student model by KD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC4w0RR9Ybxu",
        "outputId": "35090b04-4cb8-4beb-e7b0-384ed3b823e1"
      },
      "source": [
        "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
        "teachers = ['efficientnet-b0']\n",
        "\n",
        "logs = []\n",
        "for teacher_name in teachers:\n",
        "    torch.cuda.empty_cache()\n",
        "    teacher_model = EfficientNet.from_pretrained(teacher_name, num_classes=3).to(device)\n",
        "    teacher_model.load_state_dict(torch.load('3_model_weights/'+ teacher_name + '.pth'))\n",
        "\n",
        "    teacher_output = get_teacher_output(teacher_model, train_loader)\n",
        "    \n",
        "    torch.cuda.empty_cache()\n",
        "    student_model = CNN_layers().to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(student_model.parameters(), weight_decay=1e-4, lr=0.001)\n",
        "    save_name = 'kd_' + teacher_name\n",
        "\n",
        "    T = [2, 4, 6]\n",
        "    ALPHA = [0.01, 0.1, 0.5]\n",
        "    \n",
        "\n",
        "    for t in T:\n",
        "        for j, alpha in enumerate(ALPHA):\n",
        "            loss, acc, history = train_kd(model=student_model,teacher_output=teacher_output,\n",
        "                                  train_loader=train_loader, test_loader=test_loader, \n",
        "                                  criterion = criterion, optimizer=optimizer, \n",
        "                                  epochs=NUMEPOCH, T=t, alpha=alpha, save_name=save_name) \n",
        "\n",
        "            s = teacher_name +'_T{}_al{}\\tloss = {}, \\tacc = {}'.format(t, j, loss, acc)\n",
        "            logs.append(s)\n",
        "            df = pd.DataFrame(history)\n",
        "            his_name = save_name + '_T{}_al{}'.format(t, alpha)\n",
        "            df.to_csv(\"3_history/\"+ his_name+ \"_history.csv\", mode='w')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n",
            "--------- epoch : 1 ------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/home/jinbeom/anaconda3/envs/AGC/lib/python3.6/site-packages/torch/nn/functional.py:2352: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.5485183237039525\n",
            "test loss: 0.6028997078537941, \t test acc: 77.783203125%\n",
            "Best loss: 0.6028997078537941\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.24740827318442904\n",
            "test loss: 1.0855895280838013, \t test acc: 57.32421875%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.1758243413237126\n",
            "test loss: 0.3864234471693635, \t test acc: 84.033203125%\n",
            "Best loss: 0.3864234471693635\n",
            "\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.139980205051277\n",
            "test loss: 0.42413497902452946, \t test acc: 83.49609375%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.12477880777062281\n",
            "test loss: 0.4982660012319684, \t test acc: 81.25%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.11229036420421756\n",
            "test loss: 0.45071748923510313, \t test acc: 82.568359375%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.10497436631956826\n",
            "test loss: 0.4369599223136902, \t test acc: 84.27734375%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.0925041961726611\n",
            "test loss: 0.5099098803475499, \t test acc: 83.251953125%\n",
            "Epoch     8: reducing learning rate of group 0 to 1.0000e-04.\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.06777517408456492\n",
            "test loss: 0.41884530149400234, \t test acc: 86.03515625%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.059212297950263906\n",
            "test loss: 0.42739133071154356, \t test acc: 86.376953125%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.05568610372669671\n",
            "test loss: 0.422988454811275, \t test acc: 86.328125%\n",
            "early stop at 11 epoch\n",
            "best loss: 0.3864234471693635,\t best acc: 84.033203125%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.21608404969067677\n",
            "test loss: 0.43793364241719246, \t test acc: 86.181640625%\n",
            "Best loss: 0.43793364241719246\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.20481623814481756\n",
            "test loss: 0.4651212505996227, \t test acc: 85.9375%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.1972272766996985\n",
            "test loss: 0.4894280396401882, \t test acc: 85.888671875%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.19227753360958202\n",
            "test loss: 0.47678239829838276, \t test acc: 86.083984375%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.19001038461599662\n",
            "test loss: 0.4745954340323806, \t test acc: 86.474609375%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.18868895498630794\n",
            "test loss: 0.46488313004374504, \t test acc: 86.376953125%\n",
            "Epoch     6: reducing learning rate of group 0 to 1.0000e-05.\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.18477828833072082\n",
            "test loss: 0.4664829969406128, \t test acc: 86.81640625%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.18456897690244342\n",
            "test loss: 0.4711090289056301, \t test acc: 86.669921875%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.18453830812612307\n",
            "test loss: 0.4703128933906555, \t test acc: 86.62109375%\n",
            "early stop at 9 epoch\n",
            "best loss: 0.43793364241719246,\t best acc: 86.181640625%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.7145733166000118\n",
            "test loss: 0.5419004876166582, \t test acc: 85.595703125%\n",
            "Best loss: 0.5419004876166582\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.6745045016641202\n",
            "test loss: 0.5744532961398363, \t test acc: 85.205078125%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.6587198146659395\n",
            "test loss: 0.5922091342508793, \t test acc: 84.423828125%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.6508981812259426\n",
            "test loss: 0.5957941301167011, \t test acc: 84.619140625%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.6438538733383884\n",
            "test loss: 0.5950139556080103, \t test acc: 84.66796875%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.641368761010792\n",
            "test loss: 0.6075384300202131, \t test acc: 84.27734375%\n",
            "Epoch     6: reducing learning rate of group 0 to 1.0000e-06.\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.6358400771151418\n",
            "test loss: 0.6101040448993444, \t test acc: 84.08203125%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.6395774224530095\n",
            "test loss: 0.6123274378478527, \t test acc: 84.033203125%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.6362599056700001\n",
            "test loss: 0.6039273235946894, \t test acc: 84.130859375%\n",
            "early stop at 9 epoch\n",
            "best loss: 0.5419004876166582,\t best acc: 85.595703125%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.15823028957390267\n",
            "test loss: 0.5823732651770115, \t test acc: 84.814453125%\n",
            "Best loss: 0.5823732651770115\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.14775068839283093\n",
            "test loss: 0.5555086135864258, \t test acc: 85.64453125%\n",
            "Best loss: 0.5555086135864258\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.13797511933776346\n",
            "test loss: 0.5571775790303946, \t test acc: 85.400390625%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.13211096211781967\n",
            "test loss: 0.5510660614818335, \t test acc: 85.693359375%\n",
            "Best loss: 0.5510660614818335\n",
            "\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.12697457372332396\n",
            "test loss: 0.5305545348674059, \t test acc: 85.9375%\n",
            "Best loss: 0.5305545348674059\n",
            "\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.12177602084272582\n",
            "test loss: 0.5276319831609726, \t test acc: 85.83984375%\n",
            "Best loss: 0.5276319831609726\n",
            "\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.11813535946218864\n",
            "test loss: 0.5271884500980377, \t test acc: 85.986328125%\n",
            "Best loss: 0.5271884500980377\n",
            "\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.11495819906501667\n",
            "test loss: 0.5240703821182251, \t test acc: 86.03515625%\n",
            "Best loss: 0.5240703821182251\n",
            "\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.11144206097916416\n",
            "test loss: 0.5205297153443098, \t test acc: 85.7421875%\n",
            "Best loss: 0.5205297153443098\n",
            "\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.10906546093199564\n",
            "test loss: 0.5079546924680471, \t test acc: 85.83984375%\n",
            "Best loss: 0.5079546924680471\n",
            "\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.10481547098606825\n",
            "test loss: 0.5057402458041906, \t test acc: 85.791015625%\n",
            "Best loss: 0.5057402458041906\n",
            "\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.10313259627993988\n",
            "test loss: 0.507662383839488, \t test acc: 85.791015625%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.10083181581095509\n",
            "test loss: 0.5157750509679317, \t test acc: 85.888671875%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.09936564723434656\n",
            "test loss: 0.4967153649777174, \t test acc: 85.693359375%\n",
            "Best loss: 0.4967153649777174\n",
            "\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.09767426639471365\n",
            "test loss: 0.5072087775915861, \t test acc: 85.888671875%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 0.09541012261710737\n",
            "test loss: 0.5025734715163708, \t test acc: 85.7421875%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 0.09404939793698165\n",
            "test loss: 0.4922594651579857, \t test acc: 85.83984375%\n",
            "Best loss: 0.4922594651579857\n",
            "\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 0.0926280083215755\n",
            "test loss: 0.5010340586304665, \t test acc: 85.546875%\n",
            "--------- epoch : 19 ------------\n",
            "train loss: 0.0912691529273339\n",
            "test loss: 0.49068970791995525, \t test acc: 85.7421875%\n",
            "Best loss: 0.49068970791995525\n",
            "\n",
            "--------- epoch : 20 ------------\n",
            "train loss: 0.0883556790244968\n",
            "test loss: 0.49679986014962196, \t test acc: 86.03515625%\n",
            "--------- epoch : 21 ------------\n",
            "train loss: 0.08879040849759527\n",
            "test loss: 0.48469102568924427, \t test acc: 86.1328125%\n",
            "Best loss: 0.48469102568924427\n",
            "\n",
            "--------- epoch : 22 ------------\n",
            "train loss: 0.08825215199233397\n",
            "test loss: 0.4813592676073313, \t test acc: 85.693359375%\n",
            "Best loss: 0.4813592676073313\n",
            "\n",
            "--------- epoch : 23 ------------\n",
            "train loss: 0.08623473016221238\n",
            "test loss: 0.48859742283821106, \t test acc: 86.03515625%\n",
            "--------- epoch : 24 ------------\n",
            "train loss: 0.08470470988718064\n",
            "test loss: 0.4916766043752432, \t test acc: 85.888671875%\n",
            "--------- epoch : 25 ------------\n",
            "train loss: 0.0848226738769723\n",
            "test loss: 0.49026453122496605, \t test acc: 85.888671875%\n",
            "--------- epoch : 26 ------------\n",
            "train loss: 0.08381057230998641\n",
            "test loss: 0.4939443189650774, \t test acc: 85.546875%\n",
            "--------- epoch : 27 ------------\n",
            "train loss: 0.08239733591756743\n",
            "test loss: 0.47849541436880827, \t test acc: 85.986328125%\n",
            "Best loss: 0.47849541436880827\n",
            "\n",
            "--------- epoch : 28 ------------\n",
            "train loss: 0.0823434692726511\n",
            "test loss: 0.481481347233057, \t test acc: 85.546875%\n",
            "--------- epoch : 29 ------------\n",
            "train loss: 0.07969446474200358\n",
            "test loss: 0.47544808872044086, \t test acc: 85.83984375%\n",
            "Best loss: 0.47544808872044086\n",
            "\n",
            "--------- epoch : 30 ------------\n",
            "train loss: 0.07921718546877736\n",
            "test loss: 0.4824264431372285, \t test acc: 86.03515625%\n",
            "--------- epoch : 31 ------------\n",
            "train loss: 0.07779629050713519\n",
            "test loss: 0.48771523497998714, \t test acc: 85.7421875%\n",
            "--------- epoch : 32 ------------\n",
            "train loss: 0.07775012470538849\n",
            "test loss: 0.48484963178634644, \t test acc: 85.83984375%\n",
            "--------- epoch : 33 ------------\n",
            "train loss: 0.0779498848537712\n",
            "test loss: 0.47501450683921576, \t test acc: 86.181640625%\n",
            "Best loss: 0.47501450683921576\n",
            "\n",
            "--------- epoch : 34 ------------\n",
            "train loss: 0.07695030026218813\n",
            "test loss: 0.47352311946451664, \t test acc: 85.9375%\n",
            "Best loss: 0.47352311946451664\n",
            "\n",
            "--------- epoch : 35 ------------\n",
            "train loss: 0.07568936797020875\n",
            "test loss: 0.46913591772317886, \t test acc: 85.986328125%\n",
            "Best loss: 0.46913591772317886\n",
            "\n",
            "--------- epoch : 36 ------------\n",
            "train loss: 0.0748214720059996\n",
            "test loss: 0.47455118503421545, \t test acc: 85.9375%\n",
            "--------- epoch : 37 ------------\n",
            "train loss: 0.0762363193637651\n",
            "test loss: 0.4704470708966255, \t test acc: 85.83984375%\n",
            "--------- epoch : 38 ------------\n",
            "train loss: 0.07486604646091229\n",
            "test loss: 0.4763664845377207, \t test acc: 86.1328125%\n",
            "--------- epoch : 39 ------------\n",
            "train loss: 0.07231184907014603\n",
            "test loss: 0.4739875551313162, \t test acc: 85.888671875%\n",
            "--------- epoch : 40 ------------\n",
            "train loss: 0.07310485491610091\n",
            "test loss: 0.4792271014302969, \t test acc: 85.7421875%\n",
            "Epoch    40: reducing learning rate of group 0 to 1.0000e-07.\n",
            "--------- epoch : 41 ------------\n",
            "train loss: 0.0725300301514242\n",
            "test loss: 0.47129304334521294, \t test acc: 86.03515625%\n",
            "--------- epoch : 42 ------------\n",
            "train loss: 0.07255162877719039\n",
            "test loss: 0.47529178485274315, \t test acc: 86.279296875%\n",
            "--------- epoch : 43 ------------\n",
            "train loss: 0.07182032540035636\n",
            "test loss: 0.4763408610597253, \t test acc: 85.9375%\n",
            "early stop at 43 epoch\n",
            "best loss: 0.46913591772317886,\t best acc: 85.986328125%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.2586434037303147\n",
            "test loss: 0.47337025962769985, \t test acc: 86.03515625%\n",
            "Best loss: 0.47337025962769985\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.2594211223818686\n",
            "test loss: 0.4774916544556618, \t test acc: 85.693359375%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.25854854269520094\n",
            "test loss: 0.48248880449682474, \t test acc: 85.9375%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.257065215755416\n",
            "test loss: 0.47292158752679825, \t test acc: 85.64453125%\n",
            "Best loss: 0.47292158752679825\n",
            "\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.25807721046325954\n",
            "test loss: 0.4816417396068573, \t test acc: 85.7421875%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.25673201202374435\n",
            "test loss: 0.47864062525331974, \t test acc: 85.83984375%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.25927915642766847\n",
            "test loss: 0.4728679973632097, \t test acc: 85.83984375%\n",
            "Best loss: 0.4728679973632097\n",
            "\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.25809206959346065\n",
            "test loss: 0.4748530648648739, \t test acc: 85.888671875%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.25644389693827735\n",
            "test loss: 0.4768947958946228, \t test acc: 85.791015625%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.25963215343654156\n",
            "test loss: 0.4795524012297392, \t test acc: 85.7421875%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.2570711731424798\n",
            "test loss: 0.47661677189171314, \t test acc: 85.888671875%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.2587144842128391\n",
            "test loss: 0.47975538298487663, \t test acc: 85.9375%\n",
            "Epoch    12: reducing learning rate of group 0 to 1.0000e-08.\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.2572300776677287\n",
            "test loss: 0.4657870987430215, \t test acc: 86.181640625%\n",
            "Best loss: 0.4657870987430215\n",
            "\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.25735218903940654\n",
            "test loss: 0.47146874852478504, \t test acc: 85.791015625%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.2583431041920963\n",
            "test loss: 0.4718359876424074, \t test acc: 86.1328125%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 0.2581154981871014\n",
            "test loss: 0.47599842213094234, \t test acc: 86.03515625%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 0.2574708598625401\n",
            "test loss: 0.47413002885878086, \t test acc: 86.083984375%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 0.2594722375759612\n",
            "test loss: 0.47493079118430614, \t test acc: 85.791015625%\n",
            "--------- epoch : 19 ------------\n",
            "train loss: 0.25812701071086136\n",
            "test loss: 0.4704635702073574, \t test acc: 86.1328125%\n",
            "--------- epoch : 20 ------------\n",
            "train loss: 0.25626475898467976\n",
            "test loss: 0.4696831442415714, \t test acc: 85.986328125%\n",
            "--------- epoch : 21 ------------\n",
            "train loss: 0.2577615753626046\n",
            "test loss: 0.47726350650191307, \t test acc: 85.64453125%\n",
            "early stop at 21 epoch\n",
            "best loss: 0.4657870987430215,\t best acc: 86.181640625%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 1.0856442435280136\n",
            "test loss: 0.47735413908958435, \t test acc: 85.498046875%\n",
            "Best loss: 0.47735413908958435\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 1.0834404239835946\n",
            "test loss: 0.46872682590037584, \t test acc: 86.03515625%\n",
            "Best loss: 0.46872682590037584\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 1.076814550096574\n",
            "test loss: 0.4730789391323924, \t test acc: 86.083984375%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 1.0790581207560457\n",
            "test loss: 0.46810294315218925, \t test acc: 85.986328125%\n",
            "Best loss: 0.46810294315218925\n",
            "\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 1.0831281720944073\n",
            "test loss: 0.47548164147883654, \t test acc: 85.9375%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 1.0788484225454538\n",
            "test loss: 0.4678104445338249, \t test acc: 86.23046875%\n",
            "Best loss: 0.4678104445338249\n",
            "\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 1.0835800721593525\n",
            "test loss: 0.4664866905659437, \t test acc: 86.083984375%\n",
            "Best loss: 0.4664866905659437\n",
            "\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 1.0847446306244186\n",
            "test loss: 0.4752443451434374, \t test acc: 86.083984375%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 1.0804463414394336\n",
            "test loss: 0.47758784517645836, \t test acc: 85.83984375%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 1.077670523977798\n",
            "test loss: 0.46874350029975176, \t test acc: 86.23046875%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 1.0861660336022791\n",
            "test loss: 0.4680297113955021, \t test acc: 86.23046875%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 1.0771694824747418\n",
            "test loss: 0.47314067743718624, \t test acc: 85.9375%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 1.0807674572519634\n",
            "test loss: 0.4742357609793544, \t test acc: 86.03515625%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 1.0807518243141796\n",
            "test loss: 0.4677400253713131, \t test acc: 85.9375%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 1.0785760860080305\n",
            "test loss: 0.4725706512108445, \t test acc: 85.9375%\n",
            "early stop at 15 epoch\n",
            "best loss: 0.4664866905659437,\t best acc: 86.083984375%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.07418263082028083\n",
            "test loss: 0.4654974890872836, \t test acc: 86.03515625%\n",
            "Best loss: 0.4654974890872836\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.07469284329968302\n",
            "test loss: 0.47810569778084755, \t test acc: 85.888671875%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.07414902230400754\n",
            "test loss: 0.4713127641007304, \t test acc: 85.986328125%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.07438696534170405\n",
            "test loss: 0.47004985995590687, \t test acc: 85.888671875%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.07502507072185045\n",
            "test loss: 0.47009733878076077, \t test acc: 85.83984375%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.07446547433652956\n",
            "test loss: 0.467026062309742, \t test acc: 86.1328125%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.07495969689815589\n",
            "test loss: 0.4760544691234827, \t test acc: 86.1328125%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.07422013086793215\n",
            "test loss: 0.47545016929507256, \t test acc: 85.83984375%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.0743870443219076\n",
            "test loss: 0.4729193653911352, \t test acc: 85.986328125%\n",
            "early stop at 9 epoch\n",
            "best loss: 0.4654974890872836,\t best acc: 86.03515625%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.28111585610262724\n",
            "test loss: 0.47415762208402157, \t test acc: 85.888671875%\n",
            "Best loss: 0.47415762208402157\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.2810283521919147\n",
            "test loss: 0.47553843446075916, \t test acc: 86.083984375%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.28006353318367316\n",
            "test loss: 0.47606128733605146, \t test acc: 85.9375%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.28181130811572075\n",
            "test loss: 0.47618550062179565, \t test acc: 85.986328125%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.2817585091713978\n",
            "test loss: 0.4710546191781759, \t test acc: 85.9375%\n",
            "Best loss: 0.4710546191781759\n",
            "\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.2813402082285155\n",
            "test loss: 0.4717295467853546, \t test acc: 86.1328125%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.28091133871804114\n",
            "test loss: 0.4736661370843649, \t test acc: 86.1328125%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.2790013411608727\n",
            "test loss: 0.4652311783283949, \t test acc: 86.23046875%\n",
            "Best loss: 0.4652311783283949\n",
            "\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.282018879023583\n",
            "test loss: 0.46638381108641624, \t test acc: 86.279296875%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.2837443416533263\n",
            "test loss: 0.46356059052050114, \t test acc: 86.328125%\n",
            "Best loss: 0.46356059052050114\n",
            "\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.28137382646293746\n",
            "test loss: 0.4725873628631234, \t test acc: 86.1328125%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.28031263808193413\n",
            "test loss: 0.4732069820165634, \t test acc: 85.7421875%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.28144755044385145\n",
            "test loss: 0.46922669373452663, \t test acc: 86.1328125%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.2811495708706586\n",
            "test loss: 0.4826463144272566, \t test acc: 85.888671875%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.2809764865137961\n",
            "test loss: 0.4728278610855341, \t test acc: 85.9375%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 0.2814944200217724\n",
            "test loss: 0.4701420050114393, \t test acc: 86.181640625%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 0.28061299414738367\n",
            "test loss: 0.47218973468989134, \t test acc: 85.986328125%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 0.27908340711956436\n",
            "test loss: 0.47630167845636606, \t test acc: 85.83984375%\n",
            "early stop at 18 epoch\n",
            "best loss: 0.46356059052050114,\t best acc: 86.328125%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 1.201252430677414\n",
            "test loss: 0.46902671828866005, \t test acc: 86.1328125%\n",
            "Best loss: 0.46902671828866005\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 1.2006285077203875\n",
            "test loss: 0.473434098996222, \t test acc: 85.986328125%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 1.193390072039936\n",
            "test loss: 0.47015931736677885, \t test acc: 86.03515625%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 1.2021908857252286\n",
            "test loss: 0.4719678768888116, \t test acc: 85.9375%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 1.1946270699086396\n",
            "test loss: 0.4714212026447058, \t test acc: 85.9375%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 1.1974608101274655\n",
            "test loss: 0.47413487546145916, \t test acc: 85.7421875%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 1.19350094769312\n",
            "test loss: 0.47973309829831123, \t test acc: 85.83984375%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 1.1940372404844866\n",
            "test loss: 0.4674149574711919, \t test acc: 86.279296875%\n",
            "Best loss: 0.4674149574711919\n",
            "\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 1.1864296729150026\n",
            "test loss: 0.468547934666276, \t test acc: 86.572265625%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 1.195613184052965\n",
            "test loss: 0.4763426659628749, \t test acc: 85.9375%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 1.1952221111758896\n",
            "test loss: 0.47642865031957626, \t test acc: 85.791015625%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 1.2006864926737288\n",
            "test loss: 0.4652767237275839, \t test acc: 86.42578125%\n",
            "Best loss: 0.4652767237275839\n",
            "\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 1.1941825349045836\n",
            "test loss: 0.4721016911789775, \t test acc: 86.1328125%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 1.2019112719141918\n",
            "test loss: 0.47061117738485336, \t test acc: 86.083984375%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 1.193812338878279\n",
            "test loss: 0.4704132080078125, \t test acc: 86.083984375%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 1.1977505735729053\n",
            "test loss: 0.4818560369312763, \t test acc: 85.693359375%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 1.197638952213785\n",
            "test loss: 0.4728849120438099, \t test acc: 86.1328125%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 1.1940190843913867\n",
            "test loss: 0.4717489015311003, \t test acc: 85.9375%\n",
            "--------- epoch : 19 ------------\n",
            "train loss: 1.1960974646651226\n",
            "test loss: 0.467202503234148, \t test acc: 86.03515625%\n",
            "--------- epoch : 20 ------------\n",
            "train loss: 1.194975775545058\n",
            "test loss: 0.4729757010936737, \t test acc: 85.64453125%\n",
            "early stop at 20 epoch\n",
            "best loss: 0.4652767237275839,\t best acc: 86.42578125%\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW4j2iOtYbxv",
        "outputId": "605a2295-66c8-41b5-a5f3-e6bfec28bb33"
      },
      "source": [
        "#ALPHA index [0.001, 0.01, 0.1, 0.5, 0.9]\n",
        "\n",
        "for log in logs:\n",
        "    print(log)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "efficientnet-b0_T2_al0\tloss = 0.3864234471693635, \tacc = 84.033203125\n",
            "efficientnet-b0_T2_al1\tloss = 0.43793364241719246, \tacc = 86.181640625\n",
            "efficientnet-b0_T2_al2\tloss = 0.5419004876166582, \tacc = 85.595703125\n",
            "efficientnet-b0_T4_al0\tloss = 0.46913591772317886, \tacc = 85.986328125\n",
            "efficientnet-b0_T4_al1\tloss = 0.4657870987430215, \tacc = 86.181640625\n",
            "efficientnet-b0_T4_al2\tloss = 0.4664866905659437, \tacc = 86.083984375\n",
            "efficientnet-b0_T6_al0\tloss = 0.4654974890872836, \tacc = 86.03515625\n",
            "efficientnet-b0_T6_al1\tloss = 0.46356059052050114, \tacc = 86.328125\n",
            "efficientnet-b0_T6_al2\tloss = 0.4652767237275839, \tacc = 86.42578125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9DmW5VYYbxv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}