{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "ML",
      "language": "python",
      "name": "jinbeom"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "KD_train.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kjinb1212/Falldown-detection-KD/blob/main/KD_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0rlwLz5UBtm"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from glob import glob\n",
        "import os\n",
        "import sys\n",
        "from PIL import Image\n",
        "import timm\n",
        "import pandas as pd\n",
        "\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyqODfrIUBtr"
      },
      "source": [
        "# KD train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCoLEe2rUBtr"
      },
      "source": [
        "def loss_fn_kd(outputs, labels, teacher_outputs, T, alpha):\n",
        "    KD_loss = nn.KLDivLoss()(F.log_softmax(outputs/T, dim=1), \n",
        "                             F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T) + \\\n",
        "                             F.cross_entropy(outputs, labels) * (1. - alpha)\n",
        "\n",
        "    return KD_loss\n",
        "\n",
        "def get_teacher_output(model, loader):\n",
        "    model.eval()\n",
        "    output = []\n",
        "    with torch.no_grad():\n",
        "        for data, _ in loader:\n",
        "            data = data.to(device)\n",
        "            output.append(model(data))\n",
        "    torch.cuda.empty_cache()\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmpQ_d8SUBts"
      },
      "source": [
        "def train_kd(model, teacher_output, train_loader, test_loader, criterion, \n",
        "             optimizer, epochs, T, alpha, save_name):\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
        "                                                          patience=4, verbose=True)\n",
        "    best_loss = None\n",
        "    best_acc = None\n",
        "    patience = 0\n",
        "\n",
        "    history = {'loss': [], 'acc': []}\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        print(\"--------- epoch : {} ------------\".format(epoch+1))\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for i, (data, label) in enumerate(train_loader):\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = loss_fn_kd(output, label, teacher_output[i], T, alpha)\n",
        "            train_losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            torch.cuda.empty_cache()\n",
        "            \n",
        "        train_loss = np.average(train_losses)\n",
        "        print(\"train loss: {}\".format(train_loss))\n",
        "        \n",
        "        \n",
        "        model.eval()\n",
        "        test_losses = []\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for i, (data, label) in enumerate(test_loader):\n",
        "                data = data.to(device)\n",
        "                label = label.to(device)\n",
        "\n",
        "                output = model(data)\n",
        "                loss = criterion(output, label)\n",
        "                test_losses.append(loss.item())\n",
        "                _, predict = torch.max(output.data, 1)\n",
        "                correct += (predict == label).sum().item()\n",
        "                total += label.size(0)\n",
        "                \n",
        "        test_loss = np.average(test_losses)\n",
        "        test_acc = 100 * correct / total\n",
        "        print(\"test loss: {}, \\t test acc: {}%\".format(test_loss, test_acc))\n",
        "\n",
        "        history['loss'].append(test_loss)\n",
        "        history['acc'].append(test_acc)\n",
        "        \n",
        "        if (best_loss is None) or (best_loss > test_loss):\n",
        "            best_loss = test_loss\n",
        "            best_acc = test_acc\n",
        "            torch.save(model.state_dict(), 'new_model_weights/'+ save_name +'.pth')\n",
        "            print('Best loss: {}\\n'.format(best_loss))\n",
        "            patience = 0\n",
        "        else:\n",
        "            patience += 1\n",
        "        \n",
        "        if patience > 7:\n",
        "            print(\"early stop at {} epoch\".format(epoch + 1))\n",
        "            break\n",
        "            \n",
        "        scheduler.step(metrics=test_loss)\n",
        "   \n",
        "    print(\"best loss: {},\\t best acc: {}%\\n\\n\".format(best_loss, best_acc))\n",
        "    return best_loss, best_acc, history        \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVsHYNWIUBts"
      },
      "source": [
        "# Custom Datasaet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyILzw-9UBtt"
      },
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, input_size, train = True, padding = True, normalize = False,\n",
        "                 bright_ness = 0.2, hue = 0.15, contrast = 0.15, random_Hflip = 0.3, rotate_deg = 20):\n",
        "        orig_normal_path = glob(os.path.join(root_dir, 'normal') + '/*.jpg')\n",
        "        orig_fall_path = glob(os.path.join(root_dir, 'falldown') + '/*.jpg')\n",
        "        orig_back_path = glob(os.path.join(root_dir, 'background') + '/*.jpg')\n",
        "        \n",
        "        normal_paths = []\n",
        "        fall_paths = []\n",
        "        back_paths = []\n",
        "        \n",
        "        for path in orig_normal_path:\n",
        "            img = Image.open(path)\n",
        "            if min(img.size[0], img.size[1]) < 32:\n",
        "                pass\n",
        "            else:\n",
        "                normal_paths.append(path)\n",
        "                \n",
        "        for path in orig_fall_path:\n",
        "            img = Image.open(path)\n",
        "            if min(img.size[0], img.size[1]) < 32:\n",
        "                pass\n",
        "            else:\n",
        "                fall_paths.append(path)\n",
        "                \n",
        "        for path in orig_back_path:\n",
        "            img = Image.open(path)\n",
        "            if min(img.size[0], img.size[1]) < 32:\n",
        "                pass\n",
        "            else:\n",
        "                back_paths.append(path)\n",
        "                \n",
        "        self.total_paths = normal_paths + fall_paths + back_paths\n",
        "        self.labels = [0] * len(normal_paths) + [1] * len(fall_paths) + [2] * len(back_paths)\n",
        "        \n",
        "        transform = []\n",
        "        if train:\n",
        "            #transform.append(torchvision.transforms.ColorJitter(brightness=bright_ness, hue=hue, contrast=contrast))\n",
        "            transform.append(torchvision.transforms.RandomHorizontalFlip(p=random_Hflip))\n",
        "            #transform.append(torchvision.transforms.RandomCrop(224))\n",
        "            transform.append(torchvision.transforms.RandomRotation(degrees=rotate_deg))\n",
        "        transform.append(torchvision.transforms.ToTensor())\n",
        "        if normalize:\n",
        "            transform.append(torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
        "        if padding:\n",
        "            transform.append(lambda x: torchvision.transforms.Pad(((128 - x.shape[2]) // 2, (128 - x.shape[1]) // 2), fill=0,\n",
        "                                                     padding_mode=\"constant\")(x))\n",
        "        transform.append(torchvision.transforms.Resize((input_size, input_size)))\n",
        "        self.transform = torchvision.transforms.Compose(transform)\n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.total_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(self.total_paths[index])\n",
        "        img = self.transform(img)\n",
        "        return img, self.labels[index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHWABkCCUBtt"
      },
      "source": [
        "# student model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqhlo3O4UBtu"
      },
      "source": [
        "class CNN_layers(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN_layers, self).__init__()      \n",
        "        self.conv1 = nn.Conv2d(3, 16, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
        "        self.conv4 = nn.Conv2d(64, 32, 3)\n",
        "        #self.conv5 = nn.Conv2d(32, 16, 3)\n",
        "\n",
        "        self.fc1 = nn.Linear(32 * 8 * 8, 16)\n",
        "        self.fc2 = nn.Linear(16, 8)\n",
        "        self.fc3 = nn.Linear(8, 3)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.bn4 = nn.BatchNorm2d(32)\n",
        "        #self.bn5 = nn.BatchNorm2d(16)\n",
        "\n",
        "        self.bn6 = nn.BatchNorm1d(16)\n",
        "        self.bn7 = nn.BatchNorm1d(8)\n",
        "        self.padding = nn.ZeroPad2d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(self.padding(x))))) # 128 -> 64\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(self.padding(x))))) # 64 -> 32\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(self.padding(x))))) # 32 -> 16\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(self.padding(x))))) # 16 -> 8\n",
        "        #x = self.pool(F.relu(self.bn5(self.conv5(self.padding(x))))) # 8 -> 4\n",
        "\n",
        "        x = x.view(-1, 32 * 8 * 8)\n",
        "        x = F.relu(self.bn6(self.fc1(x)))\n",
        "        x = F.relu(self.bn7(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXwyrfuTUBtv"
      },
      "source": [
        "# create data loader "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTzorGMDUBtv"
      },
      "source": [
        "INPUT_SIZE = 128\n",
        "PADDING = False\n",
        "NORMALIZE = False\n",
        "BATCHSIZE = 128\n",
        "NUMEPOCH = 100\n",
        "\n",
        "train_data = CustomDataset(\n",
        "    root_dir='train',\n",
        "    input_size=INPUT_SIZE, train=True, padding=PADDING, normalize=NORMALIZE,\n",
        "    bright_ness=0, hue=01.5, contrast=0.15, random_Hflip=0, rotate_deg=0)\n",
        "\n",
        "test_data = CustomDataset(\n",
        "    root_dir='validation',\n",
        "    input_size=INPUT_SIZE, train=False, padding=PADDING, normalize=NORMALIZE,\n",
        "    bright_ness=0, hue=01.5, contrast=0.15, random_Hflip=0, rotate_deg=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGvkYEtLUBtv"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCHSIZE, shuffle=True, num_workers=32, drop_last=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCHSIZE, shuffle=False, num_workers=32, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2naTYUyGUBtw"
      },
      "source": [
        "# train student model by KD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAjVs2LyUBtw",
        "outputId": "46af8c91-f22b-43a1-b5ba-8a9419184374"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "teachers = ['efficientnet-b0', 'efficientnet-b1', 'efficientnet-b4']\n",
        "\n",
        "logs = []\n",
        "for teacher_name in teachers:\n",
        "    torch.cuda.empty_cache()\n",
        "    teacher_model = EfficientNet.from_pretrained(teacher_name, num_classes=3).to(device)\n",
        "    teacher_model.load_state_dict(torch.load('new_model_weights/'+ teacher_name + '.pth'))\n",
        "\n",
        "    teacher_output = get_teacher_output(teacher_model, train_loader)\n",
        "    \n",
        "    torch.cuda.empty_cache()\n",
        "    student_model = CNN_layers().to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(student_model.parameters(), weight_decay=1e-4, lr=0.001)\n",
        "    save_name = 'kd_' + teacher_name\n",
        "\n",
        "    T = [2, 3, 4, 5, 6 ,7]\n",
        "    ALPHA = [0.001, 0.01, 0.1, 0.5, 0.9]\n",
        "    \n",
        "\n",
        "    for t in T:\n",
        "        for j, alpha in enumerate(ALPHA):\n",
        "            loss, acc, history = train_kd(model=student_model,teacher_output=teacher_output,\n",
        "                                  train_loader=train_loader, test_loader=test_loader, \n",
        "                                  criterion = criterion, optimizer=optimizer, \n",
        "                                  epochs=NUMEPOCH, T=t, alpha=alpha, save_name=save_name) \n",
        "\n",
        "            s = teacher_name +'_T{}_al{}\\tloss = {}, \\tacc = {}'.format(t, j, loss, acc)\n",
        "            logs.append(s)\n",
        "            df = pd.DataFrame(history)\n",
        "            his_name = save_name + '_T{}_al{}'.format(t, alpha)\n",
        "            df.to_csv(\"new_history/\"+ his_name+ \"_history.csv\", mode='w')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.6361562737628169\n",
            "test loss: 0.5609208233654499, \t test acc: 76.26953125%\n",
            "Best loss: 0.5609208233654499\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.2607366149516209\n",
            "test loss: 0.6808250192552805, \t test acc: 73.14453125%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.17448783892652261\n",
            "test loss: 0.35910811088979244, \t test acc: 86.669921875%\n",
            "Best loss: 0.35910811088979244\n",
            "\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.13868358729245223\n",
            "test loss: 0.9095166698098183, \t test acc: 64.6484375%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.11426037527701777\n",
            "test loss: 0.4371277401223779, \t test acc: 83.349609375%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.10253044233009544\n",
            "test loss: 0.5280382707715034, \t test acc: 81.103515625%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.09270014061147104\n",
            "test loss: 0.501621063798666, \t test acc: 81.15234375%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.07776117501982852\n",
            "test loss: 0.6568941939622164, \t test acc: 76.46484375%\n",
            "Epoch     8: reducing learning rate of group 0 to 1.0000e-04.\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.057836420014095696\n",
            "test loss: 0.4019635869190097, \t test acc: 86.474609375%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.049076258275738874\n",
            "test loss: 0.3855625744909048, \t test acc: 86.71875%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.045554289422200425\n",
            "test loss: 0.3957628272473812, \t test acc: 86.9140625%\n",
            "early stop at 11 epoch\n",
            "best loss: 0.35910811088979244,\t best acc: 86.669921875%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.060931649119795664\n",
            "test loss: 0.38970668613910675, \t test acc: 87.158203125%\n",
            "Best loss: 0.38970668613910675\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.058287517896488956\n",
            "test loss: 0.4046038156375289, \t test acc: 86.572265625%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.05693105073726695\n",
            "test loss: 0.41294088028371334, \t test acc: 86.81640625%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.055128638247918825\n",
            "test loss: 0.41009180806577206, \t test acc: 86.669921875%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.05275428838744436\n",
            "test loss: 0.4186364058405161, \t test acc: 87.158203125%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.051565102028215064\n",
            "test loss: 0.4316686447709799, \t test acc: 86.865234375%\n",
            "Epoch     6: reducing learning rate of group 0 to 1.0000e-05.\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.048982962237104126\n",
            "test loss: 0.4278893284499645, \t test acc: 87.109375%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.047522721498313804\n",
            "test loss: 0.4233635552227497, \t test acc: 87.255859375%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.04713929464797611\n",
            "test loss: 0.4227859824895859, \t test acc: 87.20703125%\n",
            "early stop at 9 epoch\n",
            "best loss: 0.38970668613910675,\t best acc: 87.158203125%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.233448486370237\n",
            "test loss: 0.4223576597869396, \t test acc: 87.20703125%\n",
            "Best loss: 0.4223576597869396\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.23261111999011558\n",
            "test loss: 0.4311908660456538, \t test acc: 86.962890625%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.2278804665674334\n",
            "test loss: 0.42003663070499897, \t test acc: 87.158203125%\n",
            "Best loss: 0.42003663070499897\n",
            "\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.22723971485443736\n",
            "test loss: 0.42351673636585474, \t test acc: 87.20703125%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.22395533112728078\n",
            "test loss: 0.4236512603238225, \t test acc: 86.9140625%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.22331703980655773\n",
            "test loss: 0.41879100538790226, \t test acc: 86.9140625%\n",
            "Best loss: 0.41879100538790226\n",
            "\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.22056410251104314\n",
            "test loss: 0.4230563919991255, \t test acc: 86.71875%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.22155438161090665\n",
            "test loss: 0.41293900832533836, \t test acc: 87.3046875%\n",
            "Best loss: 0.41293900832533836\n",
            "\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.21956797648707163\n",
            "test loss: 0.4240372246131301, \t test acc: 86.767578125%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.21574887787194355\n",
            "test loss: 0.4177247080951929, \t test acc: 86.962890625%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.2148055371059024\n",
            "test loss: 0.4164505274966359, \t test acc: 86.865234375%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.21345428134436192\n",
            "test loss: 0.4196161860600114, \t test acc: 86.669921875%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.2132428749261991\n",
            "test loss: 0.40552405174821615, \t test acc: 86.962890625%\n",
            "Best loss: 0.40552405174821615\n",
            "\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.21112126525005567\n",
            "test loss: 0.41308685299009085, \t test acc: 86.81640625%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.2101421435566052\n",
            "test loss: 0.3995739771053195, \t test acc: 87.109375%\n",
            "Best loss: 0.3995739771053195\n",
            "\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 0.2091054349489834\n",
            "test loss: 0.40228244569152594, \t test acc: 86.962890625%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 0.21062446207455968\n",
            "test loss: 0.40676673129200935, \t test acc: 86.376953125%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 0.2090016167772853\n",
            "test loss: 0.3986454391852021, \t test acc: 87.20703125%\n",
            "Best loss: 0.3986454391852021\n",
            "\n",
            "--------- epoch : 19 ------------\n",
            "train loss: 0.2084854391767927\n",
            "test loss: 0.40655850153416395, \t test acc: 86.962890625%\n",
            "--------- epoch : 20 ------------\n",
            "train loss: 0.20778978599802309\n",
            "test loss: 0.39961415622383356, \t test acc: 87.3046875%\n",
            "--------- epoch : 21 ------------\n",
            "train loss: 0.20632405781551547\n",
            "test loss: 0.4020183486863971, \t test acc: 87.3046875%\n",
            "--------- epoch : 22 ------------\n",
            "train loss: 0.20540102611741293\n",
            "test loss: 0.4012491684406996, \t test acc: 87.20703125%\n",
            "--------- epoch : 23 ------------\n",
            "train loss: 0.2055965732459141\n",
            "test loss: 0.4005380114540458, \t test acc: 86.9140625%\n",
            "Epoch    23: reducing learning rate of group 0 to 1.0000e-06.\n",
            "--------- epoch : 24 ------------\n",
            "train loss: 0.20328040453402893\n",
            "test loss: 0.39818480983376503, \t test acc: 87.158203125%\n",
            "Best loss: 0.39818480983376503\n",
            "\n",
            "--------- epoch : 25 ------------\n",
            "train loss: 0.20391315553823244\n",
            "test loss: 0.40902233496308327, \t test acc: 86.962890625%\n",
            "--------- epoch : 26 ------------\n",
            "train loss: 0.20442381614576216\n",
            "test loss: 0.40935702808201313, \t test acc: 86.71875%\n",
            "--------- epoch : 27 ------------\n",
            "train loss: 0.2045915805451248\n",
            "test loss: 0.40680833999067545, \t test acc: 87.255859375%\n",
            "--------- epoch : 28 ------------\n",
            "train loss: 0.20507921416150487\n",
            "test loss: 0.4012653026729822, \t test acc: 87.3046875%\n",
            "--------- epoch : 29 ------------\n",
            "train loss: 0.20342578811813955\n",
            "test loss: 0.401661210693419, \t test acc: 86.962890625%\n",
            "Epoch    29: reducing learning rate of group 0 to 1.0000e-07.\n",
            "--------- epoch : 30 ------------\n",
            "train loss: 0.20459464994137702\n",
            "test loss: 0.3980164462700486, \t test acc: 87.20703125%\n",
            "Best loss: 0.3980164462700486\n",
            "\n",
            "--------- epoch : 31 ------------\n",
            "train loss: 0.20299226636795895\n",
            "test loss: 0.392495047301054, \t test acc: 87.59765625%\n",
            "Best loss: 0.392495047301054\n",
            "\n",
            "--------- epoch : 32 ------------\n",
            "train loss: 0.20500901017500006\n",
            "test loss: 0.40493861492723227, \t test acc: 86.81640625%\n",
            "--------- epoch : 33 ------------\n",
            "train loss: 0.2032517581044332\n",
            "test loss: 0.4053318677470088, \t test acc: 86.865234375%\n",
            "--------- epoch : 34 ------------\n",
            "train loss: 0.20333649655399116\n",
            "test loss: 0.4117057966068387, \t test acc: 86.62109375%\n",
            "--------- epoch : 35 ------------\n",
            "train loss: 0.20483316772657892\n",
            "test loss: 0.40800723899155855, \t test acc: 86.767578125%\n",
            "--------- epoch : 36 ------------\n",
            "train loss: 0.2035424008803523\n",
            "test loss: 0.39765525702387094, \t test acc: 87.01171875%\n",
            "Epoch    36: reducing learning rate of group 0 to 1.0000e-08.\n",
            "--------- epoch : 37 ------------\n",
            "train loss: 0.2024263260157212\n",
            "test loss: 0.40582449454814196, \t test acc: 87.060546875%\n",
            "--------- epoch : 38 ------------\n",
            "train loss: 0.20216030020104803\n",
            "test loss: 0.40776962880045176, \t test acc: 86.9140625%\n",
            "--------- epoch : 39 ------------\n",
            "train loss: 0.20210699842351934\n",
            "test loss: 0.4015600923448801, \t test acc: 87.255859375%\n",
            "early stop at 39 epoch\n",
            "best loss: 0.392495047301054,\t best acc: 87.59765625%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.8406609419895255\n",
            "test loss: 0.4061604281887412, \t test acc: 87.01171875%\n",
            "Best loss: 0.4061604281887412\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.8477411636191866\n",
            "test loss: 0.41382667422294617, \t test acc: 86.572265625%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.8400338985349821\n",
            "test loss: 0.4088292922824621, \t test acc: 87.109375%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.8377632145648417\n",
            "test loss: 0.4062808798626065, \t test acc: 86.669921875%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.8344810757300128\n",
            "test loss: 0.40147446654736996, \t test acc: 87.3046875%\n",
            "Best loss: 0.40147446654736996\n",
            "\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.8428341590839884\n",
            "test loss: 0.4070492945611477, \t test acc: 86.669921875%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.8485196799687718\n",
            "test loss: 0.4070345815271139, \t test acc: 86.767578125%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.8393483566849128\n",
            "test loss: 0.40142472460865974, \t test acc: 87.060546875%\n",
            "Best loss: 0.40142472460865974\n",
            "\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.8373897321846174\n",
            "test loss: 0.4092974066734314, \t test acc: 86.62109375%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.8463717307085576\n",
            "test loss: 0.4017634764313698, \t test acc: 87.20703125%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.8382074159124623\n",
            "test loss: 0.4020149828866124, \t test acc: 87.060546875%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.8436250317355861\n",
            "test loss: 0.40565821155905724, \t test acc: 86.81640625%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.8357586164189421\n",
            "test loss: 0.40846424736082554, \t test acc: 86.81640625%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.8337445411345233\n",
            "test loss: 0.39769905991852283, \t test acc: 87.40234375%\n",
            "Best loss: 0.39769905991852283\n",
            "\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.8395619084653647\n",
            "test loss: 0.4080006927251816, \t test acc: 86.81640625%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 0.8377050090095272\n",
            "test loss: 0.405584997497499, \t test acc: 86.962890625%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 0.8411316842488621\n",
            "test loss: 0.40198232140392065, \t test acc: 87.20703125%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 0.841719474805438\n",
            "test loss: 0.41054461244493723, \t test acc: 86.572265625%\n",
            "--------- epoch : 19 ------------\n",
            "train loss: 0.8414646824416907\n",
            "test loss: 0.3988767070695758, \t test acc: 87.01171875%\n",
            "--------- epoch : 20 ------------\n",
            "train loss: 0.8363587934037914\n",
            "test loss: 0.39810756128281355, \t test acc: 87.40234375%\n",
            "--------- epoch : 21 ------------\n",
            "train loss: 0.8394256956551386\n",
            "test loss: 0.4092774260789156, \t test acc: 86.669921875%\n",
            "--------- epoch : 22 ------------\n",
            "train loss: 0.8444058865956638\n",
            "test loss: 0.40874516498297453, \t test acc: 86.81640625%\n",
            "early stop at 22 epoch\n",
            "best loss: 0.39769905991852283,\t best acc: 87.40234375%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 1.478696368958639\n",
            "test loss: 0.40046454779803753, \t test acc: 87.109375%\n",
            "Best loss: 0.40046454779803753\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 1.4643178186986758\n",
            "test loss: 0.4000811204314232, \t test acc: 86.81640625%\n",
            "Best loss: 0.4000811204314232\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 1.4726751576299253\n",
            "test loss: 0.4179571848362684, \t test acc: 86.81640625%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 1.4549387797065403\n",
            "test loss: 0.40637990925461054, \t test acc: 87.01171875%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 1.4686441071655438\n",
            "test loss: 0.4057919569313526, \t test acc: 86.572265625%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 1.4744364327710608\n",
            "test loss: 0.4086847035214305, \t test acc: 86.81640625%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 1.470635475023933\n",
            "test loss: 0.40298518165946007, \t test acc: 86.767578125%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 1.4779394009838933\n",
            "test loss: 0.4027492441236973, \t test acc: 87.20703125%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 1.4817016345003378\n",
            "test loss: 0.4036184446886182, \t test acc: 87.060546875%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 1.4657521759686263\n",
            "test loss: 0.4062051996588707, \t test acc: 86.81640625%\n",
            "early stop at 10 epoch\n",
            "best loss: 0.4000811204314232,\t best acc: 86.81640625%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.0472611277870348\n",
            "test loss: 0.40531905461102724, \t test acc: 86.9140625%\n",
            "Best loss: 0.40531905461102724\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.04594825391414697\n",
            "test loss: 0.4087707530707121, \t test acc: 86.962890625%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.04720327098165517\n",
            "test loss: 0.4033113718032837, \t test acc: 86.962890625%\n",
            "Best loss: 0.4033113718032837\n",
            "\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.046784660728324365\n",
            "test loss: 0.3999470779672265, \t test acc: 86.9140625%\n",
            "Best loss: 0.3999470779672265\n",
            "\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.04660770625037992\n",
            "test loss: 0.4030041629448533, \t test acc: 87.01171875%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.04691109005564257\n",
            "test loss: 0.40997010935097933, \t test acc: 86.962890625%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.047413460292812924\n",
            "test loss: 0.404641343280673, \t test acc: 87.3046875%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.04711719385712691\n",
            "test loss: 0.40250423830002546, \t test acc: 86.962890625%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.046346567085255745\n",
            "test loss: 0.39693687204271555, \t test acc: 87.01171875%\n",
            "Best loss: 0.39693687204271555\n",
            "\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.04736955912338327\n",
            "test loss: 0.4054691307246685, \t test acc: 86.669921875%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.047339627529372985\n",
            "test loss: 0.4034271240234375, \t test acc: 86.81640625%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.047481991097574\n",
            "test loss: 0.4011648455634713, \t test acc: 87.20703125%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.04649464283948359\n",
            "test loss: 0.4063443783670664, \t test acc: 86.71875%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.046401411841582994\n",
            "test loss: 0.407784691080451, \t test acc: 86.865234375%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.047579237087832196\n",
            "test loss: 0.40747788082808256, \t test acc: 86.81640625%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 0.04689759553572082\n",
            "test loss: 0.40638774167746305, \t test acc: 86.865234375%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 0.04636904551728588\n",
            "test loss: 0.40575746819376945, \t test acc: 86.962890625%\n",
            "early stop at 17 epoch\n",
            "best loss: 0.39693687204271555,\t best acc: 87.01171875%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.06713951660482131\n",
            "test loss: 0.4053116189315915, \t test acc: 87.109375%\n",
            "Best loss: 0.4053116189315915\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.06534397701287399\n",
            "test loss: 0.40882550925016403, \t test acc: 86.62109375%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.06615213549736401\n",
            "test loss: 0.40759411454200745, \t test acc: 86.767578125%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.06676175323841364\n",
            "test loss: 0.406870917417109, \t test acc: 86.962890625%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.0656576349561953\n",
            "test loss: 0.40750704053789377, \t test acc: 86.962890625%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.0665580980155779\n",
            "test loss: 0.4000391745939851, \t test acc: 87.255859375%\n",
            "Best loss: 0.4000391745939851\n",
            "\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.06561181752982995\n",
            "test loss: 0.4020157903432846, \t test acc: 86.962890625%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.06610944107903735\n",
            "test loss: 0.4044553926214576, \t test acc: 86.9140625%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.06661649451226644\n",
            "test loss: 0.4117961525917053, \t test acc: 86.669921875%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.06610392647511933\n",
            "test loss: 0.4084477499127388, \t test acc: 86.62109375%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.06560443384248925\n",
            "test loss: 0.40620377101004124, \t test acc: 87.158203125%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.0659750142418172\n",
            "test loss: 0.40931536071002483, \t test acc: 86.9140625%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.06602083458362715\n",
            "test loss: 0.40729488618671894, \t test acc: 87.01171875%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.06612430919852594\n",
            "test loss: 0.4033458726480603, \t test acc: 86.9140625%\n",
            "early stop at 14 epoch\n",
            "best loss: 0.4000391745939851,\t best acc: 87.255859375%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.2613602891240431\n",
            "test loss: 0.4084964105859399, \t test acc: 86.865234375%\n",
            "Best loss: 0.4084964105859399\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.2616734864271205\n",
            "test loss: 0.418027201667428, \t test acc: 86.71875%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.26227304914399335\n",
            "test loss: 0.4086957639083266, \t test acc: 86.865234375%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.26020307847015234\n",
            "test loss: 0.40815633814781904, \t test acc: 87.060546875%\n",
            "Best loss: 0.40815633814781904\n",
            "\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.2591411383093699\n",
            "test loss: 0.4038092652335763, \t test acc: 87.060546875%\n",
            "Best loss: 0.4038092652335763\n",
            "\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.2608762467684953\n",
            "test loss: 0.40434720646589994, \t test acc: 86.669921875%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.2625623236855735\n",
            "test loss: 0.4015801986679435, \t test acc: 87.158203125%\n",
            "Best loss: 0.4015801986679435\n",
            "\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.26323415541454503\n",
            "test loss: 0.40544131211936474, \t test acc: 86.865234375%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.26037041810543643\n",
            "test loss: 0.40112632419914007, \t test acc: 87.3046875%\n",
            "Best loss: 0.40112632419914007\n",
            "\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.26136067509651184\n",
            "test loss: 0.4001190410926938, \t test acc: 87.060546875%\n",
            "Best loss: 0.4001190410926938\n",
            "\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.26224497292676696\n",
            "test loss: 0.40426752623170614, \t test acc: 86.81640625%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.2603791267003702\n",
            "test loss: 0.40402620378881693, \t test acc: 87.060546875%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.262316698451405\n",
            "test loss: 0.4086217638105154, \t test acc: 86.865234375%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.2629892487564813\n",
            "test loss: 0.40072908997535706, \t test acc: 87.40234375%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.2621496625892494\n",
            "test loss: 0.40926191583275795, \t test acc: 86.962890625%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 0.261658470429804\n",
            "test loss: 0.4039898384362459, \t test acc: 86.865234375%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 0.26148342624630616\n",
            "test loss: 0.41409806814044714, \t test acc: 86.669921875%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 0.261798783407911\n",
            "test loss: 0.4110570438206196, \t test acc: 86.669921875%\n",
            "early stop at 18 epoch\n",
            "best loss: 0.4001190410926938,\t best acc: 87.060546875%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 1.1349418853288111\n",
            "test loss: 0.40385589096695185, \t test acc: 87.109375%\n",
            "Best loss: 0.40385589096695185\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 1.1258365806678068\n",
            "test loss: 0.40208810567855835, \t test acc: 87.20703125%\n",
            "Best loss: 0.40208810567855835\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 1.1349583051126937\n",
            "test loss: 0.39972760528326035, \t test acc: 87.353515625%\n",
            "Best loss: 0.39972760528326035\n",
            "\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 1.1258221809630808\n",
            "test loss: 0.40400910656899214, \t test acc: 86.767578125%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 1.1341336301487426\n",
            "test loss: 0.4045785302296281, \t test acc: 87.01171875%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 1.1247930915459343\n",
            "test loss: 0.40744105633348227, \t test acc: 86.767578125%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 1.132193719563277\n",
            "test loss: 0.40540848299860954, \t test acc: 87.060546875%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 1.1224014457801115\n",
            "test loss: 0.3963844859972596, \t test acc: 87.353515625%\n",
            "Best loss: 0.3963844859972596\n",
            "\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 1.1282646345055622\n",
            "test loss: 0.41289169900119305, \t test acc: 86.62109375%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 1.1259967266865398\n",
            "test loss: 0.41172637045383453, \t test acc: 86.71875%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 1.1321798219628956\n",
            "test loss: 0.4078517146408558, \t test acc: 87.060546875%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 1.1198103861964268\n",
            "test loss: 0.40381439682096243, \t test acc: 87.01171875%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 1.1280228469682776\n",
            "test loss: 0.40790788270533085, \t test acc: 86.62109375%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 1.127246537934179\n",
            "test loss: 0.40254723746329546, \t test acc: 87.109375%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 1.1286188940639081\n",
            "test loss: 0.4045434398576617, \t test acc: 87.01171875%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 1.1272128247048543\n",
            "test loss: 0.41130232345312834, \t test acc: 86.71875%\n",
            "early stop at 16 epoch\n",
            "best loss: 0.3963844859972596,\t best acc: 87.353515625%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 1.988050832696583\n",
            "test loss: 0.4041454456746578, \t test acc: 87.01171875%\n",
            "Best loss: 0.4041454456746578\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 2.003547457897145\n",
            "test loss: 0.40454841684550047, \t test acc: 86.767578125%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 1.9842433087203815\n",
            "test loss: 0.4086099015548825, \t test acc: 86.669921875%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 1.9852297390284745\n",
            "test loss: 0.4029355878010392, \t test acc: 86.669921875%\n",
            "Best loss: 0.4029355878010392\n",
            "\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 1.9861469605694646\n",
            "test loss: 0.41889718547463417, \t test acc: 86.865234375%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 1.9911057365977245\n",
            "test loss: 0.4070095904171467, \t test acc: 86.81640625%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 1.9897855669260025\n",
            "test loss: 0.4110207315534353, \t test acc: 86.62109375%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 1.9895805457363958\n",
            "test loss: 0.4042082913219929, \t test acc: 86.572265625%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 1.9781229794025421\n",
            "test loss: 0.3968420159071684, \t test acc: 87.20703125%\n",
            "Best loss: 0.3968420159071684\n",
            "\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 1.9896803031796995\n",
            "test loss: 0.39185390807688236, \t test acc: 87.353515625%\n",
            "Best loss: 0.39185390807688236\n",
            "\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 1.9867694611134736\n",
            "test loss: 0.41326579451560974, \t test acc: 86.71875%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 1.986721216984417\n",
            "test loss: 0.4108135402202606, \t test acc: 86.865234375%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 1.9801084451053454\n",
            "test loss: 0.40722897555679083, \t test acc: 86.81640625%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 1.9979258782189826\n",
            "test loss: 0.4055015202611685, \t test acc: 86.71875%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 1.9817101774008379\n",
            "test loss: 0.4056959617882967, \t test acc: 86.71875%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 1.980330202890479\n",
            "test loss: 0.4008386367931962, \t test acc: 87.3046875%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 1.9774245393017065\n",
            "test loss: 0.4112654970958829, \t test acc: 86.62109375%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 1.9748040100802546\n",
            "test loss: 0.4075136352330446, \t test acc: 87.255859375%\n",
            "early stop at 18 epoch\n",
            "best loss: 0.39185390807688236,\t best acc: 87.353515625%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.04732615048187259\n",
            "test loss: 0.4021714050322771, \t test acc: 86.81640625%\n",
            "Best loss: 0.4021714050322771\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.04893488343805075\n",
            "test loss: 0.4068149169906974, \t test acc: 86.9140625%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.04824339398992774\n",
            "test loss: 0.40574734192341566, \t test acc: 86.81640625%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.0474808211033435\n",
            "test loss: 0.41325781121850014, \t test acc: 86.767578125%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.04816865021079455\n",
            "test loss: 0.4020005688071251, \t test acc: 86.865234375%\n",
            "Best loss: 0.4020005688071251\n",
            "\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.04852679208852351\n",
            "test loss: 0.40494634956121445, \t test acc: 86.962890625%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.04833534044092116\n",
            "test loss: 0.40932783018797636, \t test acc: 87.060546875%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.04813818009975164\n",
            "test loss: 0.40792872104793787, \t test acc: 87.060546875%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.048703721289158515\n",
            "test loss: 0.40737263951450586, \t test acc: 86.669921875%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.04818079377646032\n",
            "test loss: 0.40743669774383307, \t test acc: 86.669921875%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.04820355515846092\n",
            "test loss: 0.4023505970835686, \t test acc: 87.01171875%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.04810553511766636\n",
            "test loss: 0.4068471575155854, \t test acc: 87.060546875%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.048341459655405386\n",
            "test loss: 0.41067530680447817, \t test acc: 86.81640625%\n",
            "early stop at 13 epoch\n",
            "best loss: 0.4020005688071251,\t best acc: 86.865234375%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.07068440267492247\n",
            "test loss: 0.41175750084221363, \t test acc: 86.81640625%\n",
            "Best loss: 0.41175750084221363\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.07058958533098517\n",
            "test loss: 0.4037407264113426, \t test acc: 86.5234375%\n",
            "Best loss: 0.4037407264113426\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.07061868786568874\n",
            "test loss: 0.39988416340202093, \t test acc: 87.20703125%\n",
            "Best loss: 0.39988416340202093\n",
            "\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.07010008663991871\n",
            "test loss: 0.40148790180683136, \t test acc: 87.353515625%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.07113560202086101\n",
            "test loss: 0.4080059491097927, \t test acc: 86.9140625%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.07143238211131614\n",
            "test loss: 0.4045698596164584, \t test acc: 86.962890625%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.07023060279290962\n",
            "test loss: 0.4140704143792391, \t test acc: 86.669921875%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.07162775485978826\n",
            "test loss: 0.40389757230877876, \t test acc: 87.255859375%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.07122944621369243\n",
            "test loss: 0.40309622697532177, \t test acc: 87.158203125%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.07112354216287317\n",
            "test loss: 0.4094437751919031, \t test acc: 86.865234375%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.07059660078147831\n",
            "test loss: 0.4034498753026128, \t test acc: 87.109375%\n",
            "early stop at 11 epoch\n",
            "best loss: 0.39988416340202093,\t best acc: 87.20703125%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.2947782323412273\n",
            "test loss: 0.39767225086688995, \t test acc: 87.060546875%\n",
            "Best loss: 0.39767225086688995\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.2954224206345237\n",
            "test loss: 0.4093733113259077, \t test acc: 86.81640625%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.2975897394606601\n",
            "test loss: 0.40477642603218555, \t test acc: 86.767578125%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.2951410367437031\n",
            "test loss: 0.4057534793391824, \t test acc: 86.62109375%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.2959051290931909\n",
            "test loss: 0.4013198437169194, \t test acc: 87.158203125%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.2957962178827628\n",
            "test loss: 0.4062078222632408, \t test acc: 87.01171875%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.295491765698661\n",
            "test loss: 0.39860939513891935, \t test acc: 87.109375%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.2952589655052061\n",
            "test loss: 0.4060346782207489, \t test acc: 86.81640625%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.2959655027674592\n",
            "test loss: 0.4156315755099058, \t test acc: 86.5234375%\n",
            "early stop at 9 epoch\n",
            "best loss: 0.39767225086688995,\t best acc: 87.060546875%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 1.295028857562853\n",
            "test loss: 0.41101074032485485, \t test acc: 86.767578125%\n",
            "Best loss: 0.41101074032485485\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 1.2985946095508079\n",
            "test loss: 0.397953636944294, \t test acc: 87.20703125%\n",
            "Best loss: 0.397953636944294\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 1.3010915090208468\n",
            "test loss: 0.40544695779681206, \t test acc: 87.109375%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 1.3006122300158376\n",
            "test loss: 0.40969132632017136, \t test acc: 86.81640625%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 1.2965512246541355\n",
            "test loss: 0.4031993458047509, \t test acc: 87.01171875%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 1.3010093148635782\n",
            "test loss: 0.40890677087008953, \t test acc: 86.9140625%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 1.2966470925704292\n",
            "test loss: 0.4064129125326872, \t test acc: 86.9140625%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 1.3006768913372704\n",
            "test loss: 0.4019122151657939, \t test acc: 87.01171875%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 1.295748198809831\n",
            "test loss: 0.3992909016087651, \t test acc: 87.255859375%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 1.3032247916511868\n",
            "test loss: 0.40644573885947466, \t test acc: 87.060546875%\n",
            "early stop at 10 epoch\n",
            "best loss: 0.397953636944294,\t best acc: 87.20703125%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 2.3078172556732013\n",
            "test loss: 0.40582645032554865, \t test acc: 86.71875%\n",
            "Best loss: 0.40582645032554865\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 2.278875590018604\n",
            "test loss: 0.41357289254665375, \t test acc: 86.572265625%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 2.284290026711381\n",
            "test loss: 0.41147248446941376, \t test acc: 86.572265625%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 2.3086581152418386\n",
            "test loss: 0.4072561338543892, \t test acc: 86.62109375%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 2.281123205371525\n",
            "test loss: 0.40186459105461836, \t test acc: 87.01171875%\n",
            "Best loss: 0.40186459105461836\n",
            "\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 2.2791360947101014\n",
            "test loss: 0.3968390142545104, \t test acc: 86.9140625%\n",
            "Best loss: 0.3968390142545104\n",
            "\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 2.31030489180399\n",
            "test loss: 0.40891206823289394, \t test acc: 87.109375%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 2.2942297892725985\n",
            "test loss: 0.4071500515565276, \t test acc: 86.9140625%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 2.3038408892310183\n",
            "test loss: 0.39935926254838705, \t test acc: 87.158203125%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 2.2778937570426776\n",
            "test loss: 0.4005134664475918, \t test acc: 86.9140625%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 2.2992170576168145\n",
            "test loss: 0.4036733489483595, \t test acc: 87.109375%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 2.2843866879525394\n",
            "test loss: 0.40327266976237297, \t test acc: 86.71875%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 2.317698450840038\n",
            "test loss: 0.4070749133825302, \t test acc: 86.767578125%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 2.2976070091776224\n",
            "test loss: 0.40959642454981804, \t test acc: 86.865234375%\n",
            "early stop at 14 epoch\n",
            "best loss: 0.3968390142545104,\t best acc: 86.9140625%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.048534473039857716\n",
            "test loss: 0.4057234823703766, \t test acc: 86.62109375%\n",
            "Best loss: 0.4057234823703766\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.049377084740073136\n",
            "test loss: 0.4128262270241976, \t test acc: 86.767578125%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.04900763725416492\n",
            "test loss: 0.40713520534336567, \t test acc: 86.767578125%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.048428343689959984\n",
            "test loss: 0.4031831603497267, \t test acc: 86.9140625%\n",
            "Best loss: 0.4031831603497267\n",
            "\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.05021781293148904\n",
            "test loss: 0.40291945822536945, \t test acc: 86.81640625%\n",
            "Best loss: 0.40291945822536945\n",
            "\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.04934147614783243\n",
            "test loss: 0.41224678605794907, \t test acc: 86.81640625%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.048993373978073185\n",
            "test loss: 0.410480584949255, \t test acc: 86.767578125%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.04925435455515981\n",
            "test loss: 0.40563272312283516, \t test acc: 86.669921875%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.049541250438145966\n",
            "test loss: 0.4089569738134742, \t test acc: 87.01171875%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.04951383635077788\n",
            "test loss: 0.4120132988318801, \t test acc: 86.81640625%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.048737585068801825\n",
            "test loss: 0.4099721936509013, \t test acc: 86.9140625%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.04861128219889234\n",
            "test loss: 0.4037940325215459, \t test acc: 86.767578125%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.0487761241586312\n",
            "test loss: 0.4015455972403288, \t test acc: 87.109375%\n",
            "Best loss: 0.4015455972403288\n",
            "\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.04941358475986382\n",
            "test loss: 0.411472518928349, \t test acc: 86.572265625%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.04899756261147559\n",
            "test loss: 0.40872132405638695, \t test acc: 86.572265625%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 0.04861765237444121\n",
            "test loss: 0.4065369814634323, \t test acc: 86.81640625%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 0.04860832586722529\n",
            "test loss: 0.409145912155509, \t test acc: 86.865234375%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 0.048731184746746134\n",
            "test loss: 0.40456978790462017, \t test acc: 87.060546875%\n",
            "--------- epoch : 19 ------------\n",
            "train loss: 0.048449268453228084\n",
            "test loss: 0.4049507500603795, \t test acc: 86.71875%\n",
            "--------- epoch : 20 ------------\n",
            "train loss: 0.04811685083879401\n",
            "test loss: 0.4076015530154109, \t test acc: 86.9140625%\n",
            "--------- epoch : 21 ------------\n",
            "train loss: 0.04787371862355782\n",
            "test loss: 0.411734189838171, \t test acc: 86.376953125%\n",
            "early stop at 21 epoch\n",
            "best loss: 0.4015455972403288,\t best acc: 87.109375%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.07376205095130464\n",
            "test loss: 0.4011897938326001, \t test acc: 87.109375%\n",
            "Best loss: 0.4011897938326001\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.0736031063589389\n",
            "test loss: 0.4134628064930439, \t test acc: 86.5234375%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.0722939134777888\n",
            "test loss: 0.40859245602041483, \t test acc: 86.81640625%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.07261209274925616\n",
            "test loss: 0.4063685182482004, \t test acc: 86.71875%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.07350935728248695\n",
            "test loss: 0.4020272083580494, \t test acc: 86.962890625%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.07355352766487909\n",
            "test loss: 0.4103051405400038, \t test acc: 86.572265625%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.07363621714403448\n",
            "test loss: 0.40634793508797884, \t test acc: 86.962890625%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.07350812212604543\n",
            "test loss: 0.4057220472022891, \t test acc: 86.62109375%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.07319435454986017\n",
            "test loss: 0.4018365330994129, \t test acc: 87.109375%\n",
            "early stop at 9 epoch\n",
            "best loss: 0.4011897938326001,\t best acc: 87.109375%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.3181017948233563\n",
            "test loss: 0.4029118409380317, \t test acc: 86.962890625%\n",
            "Best loss: 0.4029118409380317\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.31502871191048104\n",
            "test loss: 0.41093152947723866, \t test acc: 86.474609375%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.31745685232074367\n",
            "test loss: 0.41137312166392803, \t test acc: 86.767578125%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.3167599190188491\n",
            "test loss: 0.4080165233463049, \t test acc: 86.962890625%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.3172677172266919\n",
            "test loss: 0.411392436362803, \t test acc: 86.9140625%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.31600230268162227\n",
            "test loss: 0.40266695246100426, \t test acc: 86.767578125%\n",
            "Best loss: 0.40266695246100426\n",
            "\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.3166943689727265\n",
            "test loss: 0.40646495670080185, \t test acc: 86.62109375%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.31747674537093745\n",
            "test loss: 0.40573040302842855, \t test acc: 87.255859375%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.3161679831859858\n",
            "test loss: 0.4011476580053568, \t test acc: 87.109375%\n",
            "Best loss: 0.4011476580053568\n",
            "\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.3167328303274901\n",
            "test loss: 0.39781627152115107, \t test acc: 87.255859375%\n",
            "Best loss: 0.39781627152115107\n",
            "\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.3168632240723009\n",
            "test loss: 0.4036378460004926, \t test acc: 86.962890625%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.31866358755075413\n",
            "test loss: 0.4067357983440161, \t test acc: 86.865234375%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.31833242560210434\n",
            "test loss: 0.4062587236985564, \t test acc: 86.962890625%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.3152095273784969\n",
            "test loss: 0.40952092595398426, \t test acc: 86.962890625%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.3188422623535861\n",
            "test loss: 0.40544063225388527, \t test acc: 86.62109375%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 0.3180082265449607\n",
            "test loss: 0.4106690585613251, \t test acc: 87.109375%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 0.31525534828719887\n",
            "test loss: 0.41114875487983227, \t test acc: 86.669921875%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 0.3184309447913066\n",
            "test loss: 0.4080148544162512, \t test acc: 86.962890625%\n",
            "early stop at 18 epoch\n",
            "best loss: 0.39781627152115107,\t best acc: 87.255859375%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 1.395811783230823\n",
            "test loss: 0.4010564275085926, \t test acc: 87.01171875%\n",
            "Best loss: 0.4010564275085926\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 1.4049775496773098\n",
            "test loss: 0.40559063013643026, \t test acc: 86.71875%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 1.3961008443780567\n",
            "test loss: 0.4025476910173893, \t test acc: 86.9140625%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 1.4123900623425194\n",
            "test loss: 0.40610296931117773, \t test acc: 86.71875%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 1.3892244832671208\n",
            "test loss: 0.4158376231789589, \t test acc: 86.572265625%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 1.3977688602779224\n",
            "test loss: 0.4064767314121127, \t test acc: 86.962890625%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 1.3996034873568493\n",
            "test loss: 0.4073679093271494, \t test acc: 87.158203125%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 1.3863395614468532\n",
            "test loss: 0.40913151018321514, \t test acc: 86.572265625%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 1.37971328911574\n",
            "test loss: 0.4041765658184886, \t test acc: 86.767578125%\n",
            "early stop at 9 epoch\n",
            "best loss: 0.4010564275085926,\t best acc: 87.01171875%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 2.477758026641348\n",
            "test loss: 0.4051490854471922, \t test acc: 86.9140625%\n",
            "Best loss: 0.4051490854471922\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 2.4699224304893743\n",
            "test loss: 0.4076360808685422, \t test acc: 86.865234375%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 2.4704305451849233\n",
            "test loss: 0.40033623203635216, \t test acc: 86.9140625%\n",
            "Best loss: 0.40033623203635216\n",
            "\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 2.458191139542538\n",
            "test loss: 0.4099491601809859, \t test acc: 86.71875%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 2.4728981392539064\n",
            "test loss: 0.40393149852752686, \t test acc: 86.962890625%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 2.477745258289835\n",
            "test loss: 0.4080145778134465, \t test acc: 87.01171875%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 2.4753063871808676\n",
            "test loss: 0.40924817882478237, \t test acc: 86.81640625%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 2.4809853460477744\n",
            "test loss: 0.4084772653877735, \t test acc: 86.62109375%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 2.4732331102309018\n",
            "test loss: 0.40892216097563505, \t test acc: 86.62109375%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 2.4873089414575826\n",
            "test loss: 0.41075120493769646, \t test acc: 86.669921875%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 2.443706509211789\n",
            "test loss: 0.40580442547798157, \t test acc: 86.865234375%\n",
            "early stop at 11 epoch\n",
            "best loss: 0.40033623203635216,\t best acc: 86.9140625%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.04981426169852848\n",
            "test loss: 0.4038176145404577, \t test acc: 86.71875%\n",
            "Best loss: 0.4038176145404577\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.04919372590096748\n",
            "test loss: 0.40855016838759184, \t test acc: 87.060546875%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.04961353333909874\n",
            "test loss: 0.40380775090307, \t test acc: 86.962890625%\n",
            "Best loss: 0.40380775090307\n",
            "\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.04956071613275487\n",
            "test loss: 0.4027348402887583, \t test acc: 87.255859375%\n",
            "Best loss: 0.4027348402887583\n",
            "\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.04940053832757732\n",
            "test loss: 0.4212114308029413, \t test acc: 86.279296875%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.05021803552770744\n",
            "test loss: 0.4043859876692295, \t test acc: 86.767578125%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.04894974161668316\n",
            "test loss: 0.403183963149786, \t test acc: 86.9140625%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.049751085998571434\n",
            "test loss: 0.4075057590380311, \t test acc: 86.767578125%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.04889577262751434\n",
            "test loss: 0.40760761871933937, \t test acc: 86.962890625%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.05016084420292274\n",
            "test loss: 0.40147185139358044, \t test acc: 87.109375%\n",
            "Best loss: 0.40147185139358044\n",
            "\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.05011702899861595\n",
            "test loss: 0.4059320567175746, \t test acc: 86.962890625%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.04953829162632641\n",
            "test loss: 0.4081920310854912, \t test acc: 86.865234375%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.049948574914394514\n",
            "test loss: 0.40735875628888607, \t test acc: 86.71875%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.04983396380734832\n",
            "test loss: 0.4080195687711239, \t test acc: 87.01171875%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.04888617992401123\n",
            "test loss: 0.4105530437082052, \t test acc: 87.109375%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 0.05003344544978893\n",
            "test loss: 0.4025255888700485, \t test acc: 86.669921875%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 0.04957025875742345\n",
            "test loss: 0.4102482618764043, \t test acc: 86.962890625%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 0.04959017928163319\n",
            "test loss: 0.4111808920279145, \t test acc: 86.81640625%\n",
            "early stop at 18 epoch\n",
            "best loss: 0.40147185139358044,\t best acc: 87.109375%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.07442459057125708\n",
            "test loss: 0.4086952768266201, \t test acc: 86.767578125%\n",
            "Best loss: 0.4086952768266201\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.07433324339597122\n",
            "test loss: 0.40533930342644453, \t test acc: 86.962890625%\n",
            "Best loss: 0.40533930342644453\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.07515934401232263\n",
            "test loss: 0.40376890636980534, \t test acc: 86.81640625%\n",
            "Best loss: 0.40376890636980534\n",
            "\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.07482243471009575\n",
            "test loss: 0.4028117870911956, \t test acc: 86.9140625%\n",
            "Best loss: 0.4028117870911956\n",
            "\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.07499837152578909\n",
            "test loss: 0.4094749270007014, \t test acc: 87.060546875%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.07365138512914596\n",
            "test loss: 0.4063308220356703, \t test acc: 87.20703125%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.07402851679564817\n",
            "test loss: 0.40403078123927116, \t test acc: 86.865234375%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.07498132188440017\n",
            "test loss: 0.40472571272403, \t test acc: 86.865234375%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.07468457629337259\n",
            "test loss: 0.40249919798225164, \t test acc: 87.01171875%\n",
            "Best loss: 0.40249919798225164\n",
            "\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.0751993951752134\n",
            "test loss: 0.4058440327644348, \t test acc: 86.767578125%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.07556558485426333\n",
            "test loss: 0.4104791795834899, \t test acc: 86.81640625%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.07397500751540065\n",
            "test loss: 0.4044726360589266, \t test acc: 86.9140625%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.07355684920659532\n",
            "test loss: 0.40661944542080164, \t test acc: 87.158203125%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.07404022297376524\n",
            "test loss: 0.3998993579298258, \t test acc: 86.9140625%\n",
            "Best loss: 0.3998993579298258\n",
            "\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.0754041054043109\n",
            "test loss: 0.403431317768991, \t test acc: 86.865234375%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 0.07554209374053322\n",
            "test loss: 0.4063847931101918, \t test acc: 87.060546875%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 0.07441604147543726\n",
            "test loss: 0.41029151529073715, \t test acc: 86.71875%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 0.07456234263499147\n",
            "test loss: 0.39945594128221273, \t test acc: 87.158203125%\n",
            "Best loss: 0.39945594128221273\n",
            "\n",
            "--------- epoch : 19 ------------\n",
            "train loss: 0.07462372614637665\n",
            "test loss: 0.403836477547884, \t test acc: 86.81640625%\n",
            "--------- epoch : 20 ------------\n",
            "train loss: 0.07430839903004792\n",
            "test loss: 0.412802841514349, \t test acc: 86.962890625%\n",
            "--------- epoch : 21 ------------\n",
            "train loss: 0.07455813016175576\n",
            "test loss: 0.40279955323785543, \t test acc: 86.9140625%\n",
            "--------- epoch : 22 ------------\n",
            "train loss: 0.07432118212075337\n",
            "test loss: 0.39886126667261124, \t test acc: 87.20703125%\n",
            "Best loss: 0.39886126667261124\n",
            "\n",
            "--------- epoch : 23 ------------\n",
            "train loss: 0.07518580804943391\n",
            "test loss: 0.40997224394232035, \t test acc: 86.669921875%\n",
            "--------- epoch : 24 ------------\n",
            "train loss: 0.07486137838872231\n",
            "test loss: 0.4089618669822812, \t test acc: 87.060546875%\n",
            "--------- epoch : 25 ------------\n",
            "train loss: 0.0748649264037933\n",
            "test loss: 0.39969521202147007, \t test acc: 87.451171875%\n",
            "--------- epoch : 26 ------------\n",
            "train loss: 0.07368929649743697\n",
            "test loss: 0.39792263228446245, \t test acc: 86.9140625%\n",
            "Best loss: 0.39792263228446245\n",
            "\n",
            "--------- epoch : 27 ------------\n",
            "train loss: 0.07418637072829448\n",
            "test loss: 0.4046190856024623, \t test acc: 86.669921875%\n",
            "--------- epoch : 28 ------------\n",
            "train loss: 0.07479703555936398\n",
            "test loss: 0.4061190467327833, \t test acc: 86.71875%\n",
            "--------- epoch : 29 ------------\n",
            "train loss: 0.07442074667905337\n",
            "test loss: 0.4013872342184186, \t test acc: 86.962890625%\n",
            "--------- epoch : 30 ------------\n",
            "train loss: 0.07465051065968431\n",
            "test loss: 0.40479830279946327, \t test acc: 86.669921875%\n",
            "--------- epoch : 31 ------------\n",
            "train loss: 0.07347109861186017\n",
            "test loss: 0.40772076044231653, \t test acc: 86.62109375%\n",
            "--------- epoch : 32 ------------\n",
            "train loss: 0.07539476798442395\n",
            "test loss: 0.41254731733351946, \t test acc: 86.572265625%\n",
            "--------- epoch : 33 ------------\n",
            "train loss: 0.07379339809488991\n",
            "test loss: 0.4061018079519272, \t test acc: 86.5234375%\n",
            "--------- epoch : 34 ------------\n",
            "train loss: 0.0744280785565143\n",
            "test loss: 0.40536875929683447, \t test acc: 87.060546875%\n",
            "early stop at 34 epoch\n",
            "best loss: 0.39792263228446245,\t best acc: 86.9140625%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.32742718995913217\n",
            "test loss: 0.4072318524122238, \t test acc: 86.962890625%\n",
            "Best loss: 0.4072318524122238\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.3284011726146159\n",
            "test loss: 0.4034877670928836, \t test acc: 87.353515625%\n",
            "Best loss: 0.4034877670928836\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.32854989009058994\n",
            "test loss: 0.4137674132362008, \t test acc: 86.474609375%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.32730506733059883\n",
            "test loss: 0.4019043752923608, \t test acc: 87.158203125%\n",
            "Best loss: 0.4019043752923608\n",
            "\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.3257877487365318\n",
            "test loss: 0.40446198638528585, \t test acc: 87.20703125%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.3306250304955503\n",
            "test loss: 0.40150514524430037, \t test acc: 87.158203125%\n",
            "Best loss: 0.40150514524430037\n",
            "\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.32964492880779767\n",
            "test loss: 0.40620923042297363, \t test acc: 86.9140625%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.32750611800862395\n",
            "test loss: 0.4005028950050473, \t test acc: 86.9140625%\n",
            "Best loss: 0.4005028950050473\n",
            "\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.3271104767918587\n",
            "test loss: 0.40281106531620026, \t test acc: 86.865234375%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.3288897355289563\n",
            "test loss: 0.4047220190986991, \t test acc: 87.109375%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.3287253543410612\n",
            "test loss: 0.4060508478432894, \t test acc: 86.9140625%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.32801390844194783\n",
            "test loss: 0.4062524884939194, \t test acc: 86.81640625%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.3281209130326043\n",
            "test loss: 0.4011508943513036, \t test acc: 87.40234375%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.325189350415831\n",
            "test loss: 0.4063233509659767, \t test acc: 86.767578125%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.3279567025601864\n",
            "test loss: 0.4071313105523586, \t test acc: 86.9140625%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 0.32774569061787234\n",
            "test loss: 0.40487304236739874, \t test acc: 87.158203125%\n",
            "early stop at 16 epoch\n",
            "best loss: 0.4005028950050473,\t best acc: 86.9140625%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 1.4543666871993437\n",
            "test loss: 0.406520483084023, \t test acc: 87.109375%\n",
            "Best loss: 0.406520483084023\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 1.4609621316194534\n",
            "test loss: 0.40334667172282934, \t test acc: 87.353515625%\n",
            "Best loss: 0.40334667172282934\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 1.466026453220326\n",
            "test loss: 0.4058509608730674, \t test acc: 87.20703125%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 1.4535418716461763\n",
            "test loss: 0.4025112772360444, \t test acc: 86.865234375%\n",
            "Best loss: 0.4025112772360444\n",
            "\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 1.440866438590962\n",
            "test loss: 0.4011081736534834, \t test acc: 87.20703125%\n",
            "Best loss: 0.4011081736534834\n",
            "\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 1.454609420636426\n",
            "test loss: 0.40270457323640585, \t test acc: 86.572265625%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 1.4622981710278469\n",
            "test loss: 0.40274091996252537, \t test acc: 86.9140625%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 1.4522153165029443\n",
            "test loss: 0.41301041282713413, \t test acc: 86.669921875%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 1.4609476923942566\n",
            "test loss: 0.4055820442736149, \t test acc: 86.865234375%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 1.4524353548236515\n",
            "test loss: 0.4037146149203181, \t test acc: 86.9140625%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 1.4465565869341726\n",
            "test loss: 0.4014544039964676, \t test acc: 86.865234375%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 1.4488765765791354\n",
            "test loss: 0.4020555233582854, \t test acc: 86.71875%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 1.444093521522439\n",
            "test loss: 0.4008515290915966, \t test acc: 87.060546875%\n",
            "Best loss: 0.4008515290915966\n",
            "\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 1.4601567342229511\n",
            "test loss: 0.4026108179241419, \t test acc: 86.962890625%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 1.4530313967362694\n",
            "test loss: 0.4018179588019848, \t test acc: 87.158203125%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 1.4676850723183674\n",
            "test loss: 0.40584378596395254, \t test acc: 86.9140625%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 1.4546203412439511\n",
            "test loss: 0.4044675873592496, \t test acc: 86.669921875%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 1.453051968113236\n",
            "test loss: 0.4048287244513631, \t test acc: 87.255859375%\n",
            "--------- epoch : 19 ------------\n",
            "train loss: 1.4539029112328654\n",
            "test loss: 0.4082999285310507, \t test acc: 87.060546875%\n",
            "--------- epoch : 20 ------------\n",
            "train loss: 1.449228994872259\n",
            "test loss: 0.4099089642986655, \t test acc: 86.81640625%\n",
            "--------- epoch : 21 ------------\n",
            "train loss: 1.4491368052752123\n",
            "test loss: 0.4159218342974782, \t test acc: 86.71875%\n",
            "early stop at 21 epoch\n",
            "best loss: 0.4008515290915966,\t best acc: 87.060546875%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 2.593978086243505\n",
            "test loss: 0.40250594448298216, \t test acc: 87.158203125%\n",
            "Best loss: 0.40250594448298216\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 2.591263650552086\n",
            "test loss: 0.4060807405039668, \t test acc: 86.81640625%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 2.569905319939489\n",
            "test loss: 0.4035299075767398, \t test acc: 87.01171875%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 2.582493642102117\n",
            "test loss: 0.4091084795072675, \t test acc: 86.767578125%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 2.5940204599629277\n",
            "test loss: 0.409194209612906, \t test acc: 87.109375%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 2.5796498112056567\n",
            "test loss: 0.4090347746387124, \t test acc: 86.962890625%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 2.5766334870587224\n",
            "test loss: 0.40416928194463253, \t test acc: 86.81640625%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 2.5493697686039885\n",
            "test loss: 0.4006687495857477, \t test acc: 87.109375%\n",
            "Best loss: 0.4006687495857477\n",
            "\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 2.582835660032604\n",
            "test loss: 0.4131348663941026, \t test acc: 86.767578125%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 2.5831488176532416\n",
            "test loss: 0.41330962628126144, \t test acc: 86.5234375%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 2.61846733611563\n",
            "test loss: 0.40958627313375473, \t test acc: 86.9140625%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 2.5891640445460444\n",
            "test loss: 0.40735987294465303, \t test acc: 86.865234375%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 2.5838924503844716\n",
            "test loss: 0.4111696546897292, \t test acc: 86.865234375%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 2.5789849071399025\n",
            "test loss: 0.40175478998571634, \t test acc: 87.060546875%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 2.563607342865156\n",
            "test loss: 0.4067313829436898, \t test acc: 87.01171875%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 2.575365028951479\n",
            "test loss: 0.40144046396017075, \t test acc: 87.3046875%\n",
            "early stop at 16 epoch\n",
            "best loss: 0.4006687495857477,\t best acc: 87.109375%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.05032506123509096\n",
            "test loss: 0.4067340912297368, \t test acc: 86.71875%\n",
            "Best loss: 0.4067340912297368\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.05013858946040273\n",
            "test loss: 0.4100817861035466, \t test acc: 86.9140625%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.04987391672583054\n",
            "test loss: 0.4022870874032378, \t test acc: 86.962890625%\n",
            "Best loss: 0.4022870874032378\n",
            "\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.050012440896471555\n",
            "test loss: 0.4047641111537814, \t test acc: 86.9140625%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.05015788093695174\n",
            "test loss: 0.4025407172739506, \t test acc: 87.109375%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.04932945932302138\n",
            "test loss: 0.4040955323725939, \t test acc: 86.865234375%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.049329516761328865\n",
            "test loss: 0.40765372663736343, \t test acc: 86.865234375%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.05075461324304342\n",
            "test loss: 0.40716178715229034, \t test acc: 86.81640625%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.05011546656327403\n",
            "test loss: 0.40836650133132935, \t test acc: 86.62109375%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.050739243727825255\n",
            "test loss: 0.4001904493197799, \t test acc: 86.9140625%\n",
            "Best loss: 0.4001904493197799\n",
            "\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.0504888960302038\n",
            "test loss: 0.4136022739112377, \t test acc: 86.572265625%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.049750626168173294\n",
            "test loss: 0.4047599332407117, \t test acc: 87.01171875%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.05081750154900162\n",
            "test loss: 0.4014183543622494, \t test acc: 86.9140625%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.05011929664760828\n",
            "test loss: 0.4093420961871743, \t test acc: 86.572265625%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.049890705364067915\n",
            "test loss: 0.40969240851700306, \t test acc: 86.474609375%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 0.050312037316515394\n",
            "test loss: 0.4160015434026718, \t test acc: 86.81640625%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 0.04953263832620629\n",
            "test loss: 0.4086132384836674, \t test acc: 86.9140625%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 0.0506349366725139\n",
            "test loss: 0.3902491834014654, \t test acc: 87.548828125%\n",
            "Best loss: 0.3902491834014654\n",
            "\n",
            "--------- epoch : 19 ------------\n",
            "train loss: 0.04974459619868709\n",
            "test loss: 0.4075059024617076, \t test acc: 86.767578125%\n",
            "--------- epoch : 20 ------------\n",
            "train loss: 0.0496091890229803\n",
            "test loss: 0.4065347723662853, \t test acc: 86.9140625%\n",
            "--------- epoch : 21 ------------\n",
            "train loss: 0.04968879512592178\n",
            "test loss: 0.4116415586322546, \t test acc: 86.865234375%\n",
            "--------- epoch : 22 ------------\n",
            "train loss: 0.05085847077324339\n",
            "test loss: 0.4090280020609498, \t test acc: 86.71875%\n",
            "--------- epoch : 23 ------------\n",
            "train loss: 0.049879961849554726\n",
            "test loss: 0.4063879931345582, \t test acc: 86.9140625%\n",
            "--------- epoch : 24 ------------\n",
            "train loss: 0.04974553693571816\n",
            "test loss: 0.4044294022023678, \t test acc: 86.865234375%\n",
            "--------- epoch : 25 ------------\n",
            "train loss: 0.049300165350913354\n",
            "test loss: 0.4064556835219264, \t test acc: 86.962890625%\n",
            "--------- epoch : 26 ------------\n",
            "train loss: 0.04990074195413162\n",
            "test loss: 0.4098030412569642, \t test acc: 86.865234375%\n",
            "early stop at 26 epoch\n",
            "best loss: 0.3902491834014654,\t best acc: 87.548828125%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.0758567395336602\n",
            "test loss: 0.4018360134214163, \t test acc: 87.01171875%\n",
            "Best loss: 0.4018360134214163\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.076109501917887\n",
            "test loss: 0.4123809039592743, \t test acc: 86.572265625%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.07567988253077088\n",
            "test loss: 0.41149848978966475, \t test acc: 87.01171875%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.07638834625401575\n",
            "test loss: 0.4069875255227089, \t test acc: 86.962890625%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.07579886475982874\n",
            "test loss: 0.40122112445533276, \t test acc: 86.962890625%\n",
            "Best loss: 0.40122112445533276\n",
            "\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.07527541071581452\n",
            "test loss: 0.4034477761015296, \t test acc: 86.9140625%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.07550942089975528\n",
            "test loss: 0.40582337230443954, \t test acc: 86.767578125%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.07512551843715103\n",
            "test loss: 0.4137026648968458, \t test acc: 86.9140625%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.07665963284671307\n",
            "test loss: 0.41060599498450756, \t test acc: 86.71875%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.07475002652601055\n",
            "test loss: 0.40149536449462175, \t test acc: 87.3046875%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.07575640177516185\n",
            "test loss: 0.4105469360947609, \t test acc: 86.767578125%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.07568235964635792\n",
            "test loss: 0.40985339879989624, \t test acc: 86.962890625%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.07673848410258474\n",
            "test loss: 0.4099530382081866, \t test acc: 86.9140625%\n",
            "early stop at 13 epoch\n",
            "best loss: 0.40122112445533276,\t best acc: 86.962890625%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.33707219038320624\n",
            "test loss: 0.41054233629256487, \t test acc: 86.669921875%\n",
            "Best loss: 0.41054233629256487\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.33600323971198953\n",
            "test loss: 0.40189036540687084, \t test acc: 86.865234375%\n",
            "Best loss: 0.40189036540687084\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.33606525251398917\n",
            "test loss: 0.40977521426975727, \t test acc: 86.9140625%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.33467781187399576\n",
            "test loss: 0.4135914817452431, \t test acc: 86.71875%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.3341812957888064\n",
            "test loss: 0.4074631594121456, \t test acc: 87.01171875%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.3364557455415311\n",
            "test loss: 0.40761792939156294, \t test acc: 86.669921875%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.33467725712967955\n",
            "test loss: 0.4062569234520197, \t test acc: 86.9140625%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.33409041700803715\n",
            "test loss: 0.41093569342046976, \t test acc: 86.81640625%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.33319318294525146\n",
            "test loss: 0.40674186032265425, \t test acc: 86.62109375%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.3355122045006441\n",
            "test loss: 0.405137749388814, \t test acc: 86.669921875%\n",
            "early stop at 10 epoch\n",
            "best loss: 0.40189036540687084,\t best acc: 86.865234375%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 1.5013105791548025\n",
            "test loss: 0.40816645603626966, \t test acc: 86.81640625%\n",
            "Best loss: 0.40816645603626966\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 1.4829434493313665\n",
            "test loss: 0.410157922655344, \t test acc: 86.572265625%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 1.495190350257832\n",
            "test loss: 0.40351621713489294, \t test acc: 87.01171875%\n",
            "Best loss: 0.40351621713489294\n",
            "\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 1.499798609510712\n",
            "test loss: 0.40630531776696444, \t test acc: 86.9140625%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 1.49024470474409\n",
            "test loss: 0.40573570504784584, \t test acc: 86.81640625%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 1.486870549943136\n",
            "test loss: 0.4099310226738453, \t test acc: 86.71875%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 1.4964857269888339\n",
            "test loss: 0.40391394030302763, \t test acc: 87.01171875%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 1.4900373900714128\n",
            "test loss: 0.40927574690431356, \t test acc: 87.01171875%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 1.4858120880697085\n",
            "test loss: 0.40950257424265146, \t test acc: 86.572265625%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 1.4948600459357966\n",
            "test loss: 0.40389769338071346, \t test acc: 87.158203125%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 1.4763850485501082\n",
            "test loss: 0.4010813860222697, \t test acc: 87.3046875%\n",
            "Best loss: 0.4010813860222697\n",
            "\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 1.485351453008859\n",
            "test loss: 0.4047068636864424, \t test acc: 86.9140625%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 1.4853275100822034\n",
            "test loss: 0.4069513399153948, \t test acc: 86.9140625%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 1.4904569231945535\n",
            "test loss: 0.41277250088751316, \t test acc: 86.865234375%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 1.4847746847764305\n",
            "test loss: 0.403000108897686, \t test acc: 87.255859375%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 1.4881084878807482\n",
            "test loss: 0.40289053693413734, \t test acc: 86.962890625%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 1.4875857039638187\n",
            "test loss: 0.39965679310262203, \t test acc: 87.158203125%\n",
            "Best loss: 0.39965679310262203\n",
            "\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 1.4839567205180293\n",
            "test loss: 0.40741183143109083, \t test acc: 87.01171875%\n",
            "--------- epoch : 19 ------------\n",
            "train loss: 1.4766557022281315\n",
            "test loss: 0.41237270925194025, \t test acc: 86.62109375%\n",
            "--------- epoch : 20 ------------\n",
            "train loss: 1.4782768371312514\n",
            "test loss: 0.40716057922691107, \t test acc: 86.71875%\n",
            "--------- epoch : 21 ------------\n",
            "train loss: 1.4806989036176517\n",
            "test loss: 0.40467511769384146, \t test acc: 86.71875%\n",
            "--------- epoch : 22 ------------\n",
            "train loss: 1.4931339388308318\n",
            "test loss: 0.40955313481390476, \t test acc: 87.01171875%\n",
            "--------- epoch : 23 ------------\n",
            "train loss: 1.476536688597306\n",
            "test loss: 0.4090254148468375, \t test acc: 86.767578125%\n",
            "--------- epoch : 24 ------------\n",
            "train loss: 1.494233537627303\n",
            "test loss: 0.404782478697598, \t test acc: 87.060546875%\n",
            "--------- epoch : 25 ------------\n",
            "train loss: 1.4900421150352643\n",
            "test loss: 0.4112175637856126, \t test acc: 86.42578125%\n",
            "early stop at 25 epoch\n",
            "best loss: 0.39965679310262203,\t best acc: 87.158203125%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 2.6291174305521925\n",
            "test loss: 0.40745364874601364, \t test acc: 86.9140625%\n",
            "Best loss: 0.40745364874601364\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 2.637652481379716\n",
            "test loss: 0.40387743432074785, \t test acc: 87.158203125%\n",
            "Best loss: 0.40387743432074785\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 2.6295004966466324\n",
            "test loss: 0.40350716561079025, \t test acc: 87.158203125%\n",
            "Best loss: 0.40350716561079025\n",
            "\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 2.640067874089531\n",
            "test loss: 0.4121160814538598, \t test acc: 86.962890625%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 2.6343015846998794\n",
            "test loss: 0.41090069711208344, \t test acc: 86.5234375%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 2.6332889004893927\n",
            "test loss: 0.4087565168738365, \t test acc: 86.81640625%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 2.6353523251803024\n",
            "test loss: 0.40167182590812445, \t test acc: 87.40234375%\n",
            "Best loss: 0.40167182590812445\n",
            "\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 2.6113401301529096\n",
            "test loss: 0.4042307687923312, \t test acc: 86.9140625%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 2.6481454540853915\n",
            "test loss: 0.4051580363884568, \t test acc: 86.9140625%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 2.6299334654341573\n",
            "test loss: 0.4092976367101073, \t test acc: 86.9140625%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 2.609427721604057\n",
            "test loss: 0.4123304532840848, \t test acc: 86.669921875%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 2.618637063581011\n",
            "test loss: 0.40735942777246237, \t test acc: 86.962890625%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 2.6134456745956256\n",
            "test loss: 0.4050175305455923, \t test acc: 87.01171875%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 2.6354443806668986\n",
            "test loss: 0.4028246561065316, \t test acc: 86.9140625%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 2.6351565278094746\n",
            "test loss: 0.40307916700839996, \t test acc: 87.109375%\n",
            "early stop at 15 epoch\n",
            "best loss: 0.40167182590812445,\t best acc: 87.40234375%\n",
            "\n",
            "\n",
            "Loaded pretrained weights for efficientnet-b1\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.5453489743497061\n",
            "test loss: 0.8087031356990337, \t test acc: 69.53125%\n",
            "Best loss: 0.8087031356990337\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.26725508434617\n",
            "test loss: 0.7157493941485882, \t test acc: 71.923828125%\n",
            "Best loss: 0.7157493941485882\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.1716932512252875\n",
            "test loss: 0.5415271613746881, \t test acc: 75.830078125%\n",
            "Best loss: 0.5415271613746881\n",
            "\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.13952586180328028\n",
            "test loss: 0.5162065830081701, \t test acc: 79.58984375%\n",
            "Best loss: 0.5162065830081701\n",
            "\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.12009249799682395\n",
            "test loss: 0.4251985903829336, \t test acc: 83.642578125%\n",
            "Best loss: 0.4251985903829336\n",
            "\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.10494330908050356\n",
            "test loss: 0.441125619225204, \t test acc: 85.05859375%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.09026437936067257\n",
            "test loss: 0.3728559482842684, \t test acc: 86.71875%\n",
            "Best loss: 0.3728559482842684\n",
            "\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.08232859883497914\n",
            "test loss: 0.45392876327969134, \t test acc: 83.69140625%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.07108220928753524\n",
            "test loss: 0.4783449787646532, \t test acc: 82.958984375%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.06565995730014275\n",
            "test loss: 0.5064530782401562, \t test acc: 84.1796875%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.059827776671306274\n",
            "test loss: 0.5831798482686281, \t test acc: 81.689453125%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.048881886567434536\n",
            "test loss: 0.4879447603598237, \t test acc: 85.009765625%\n",
            "Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.031166049712540014\n",
            "test loss: 0.44981049466878176, \t test acc: 87.060546875%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.02458468241536099\n",
            "test loss: 0.46417629066854715, \t test acc: 86.962890625%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.021659280087434403\n",
            "test loss: 0.44930195063352585, \t test acc: 87.255859375%\n",
            "early stop at 15 epoch\n",
            "best loss: 0.3728559482842684,\t best acc: 86.71875%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.040235715786643\n",
            "test loss: 0.4759277179837227, \t test acc: 87.060546875%\n",
            "Best loss: 0.4759277179837227\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.03904004684527931\n",
            "test loss: 0.4938074117526412, \t test acc: 87.01171875%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.036756774349867\n",
            "test loss: 0.48441461008042097, \t test acc: 87.060546875%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.03561349361932472\n",
            "test loss: 0.5025763418525457, \t test acc: 86.5234375%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.034864920363559024\n",
            "test loss: 0.5030662501230836, \t test acc: 87.109375%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.034148324402454105\n",
            "test loss: 0.5077621191740036, \t test acc: 86.9140625%\n",
            "Epoch     6: reducing learning rate of group 0 to 1.0000e-05.\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.03288501805787825\n",
            "test loss: 0.5139655200764537, \t test acc: 87.158203125%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.032961906612162355\n",
            "test loss: 0.5102861896157265, \t test acc: 86.767578125%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.03288475357239013\n",
            "test loss: 0.5020729918032885, \t test acc: 87.353515625%\n",
            "early stop at 9 epoch\n",
            "best loss: 0.4759277179837227,\t best acc: 87.060546875%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.22637454234063625\n",
            "test loss: 0.5120708756148815, \t test acc: 87.01171875%\n",
            "Best loss: 0.5120708756148815\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.21848019082909045\n",
            "test loss: 0.5128664476796985, \t test acc: 87.01171875%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.2148387743079144\n",
            "test loss: 0.5225327648222446, \t test acc: 86.669921875%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.2101352651320074\n",
            "test loss: 0.5173726072534919, \t test acc: 86.865234375%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.2059003811167634\n",
            "test loss: 0.515023622661829, \t test acc: 86.42578125%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.20178018858575303\n",
            "test loss: 0.5076806610450149, \t test acc: 86.572265625%\n",
            "Best loss: 0.5076806610450149\n",
            "\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.19889191570489304\n",
            "test loss: 0.5055982023477554, \t test acc: 86.669921875%\n",
            "Best loss: 0.5055982023477554\n",
            "\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.1972805248654407\n",
            "test loss: 0.5025065625086427, \t test acc: 86.669921875%\n",
            "Best loss: 0.5025065625086427\n",
            "\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.1930344112055457\n",
            "test loss: 0.4921395927667618, \t test acc: 86.42578125%\n",
            "Best loss: 0.4921395927667618\n",
            "\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.19193402693971343\n",
            "test loss: 0.4981618467718363, \t test acc: 86.42578125%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.19150752800962198\n",
            "test loss: 0.4905629036948085, \t test acc: 86.81640625%\n",
            "Best loss: 0.4905629036948085\n",
            "\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.18903159479732098\n",
            "test loss: 0.48064072895795107, \t test acc: 86.81640625%\n",
            "Best loss: 0.48064072895795107\n",
            "\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.1885285756510237\n",
            "test loss: 0.47837945725768805, \t test acc: 87.01171875%\n",
            "Best loss: 0.47837945725768805\n",
            "\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.18730697643173777\n",
            "test loss: 0.478858339600265, \t test acc: 86.9140625%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.18823798008910989\n",
            "test loss: 0.4813835872337222, \t test acc: 86.81640625%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 0.18411990546661874\n",
            "test loss: 0.4644056633114815, \t test acc: 87.20703125%\n",
            "Best loss: 0.4644056633114815\n",
            "\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 0.18270452166705028\n",
            "test loss: 0.47333491314202547, \t test acc: 87.01171875%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 0.18180609777893708\n",
            "test loss: 0.4731763731688261, \t test acc: 87.255859375%\n",
            "--------- epoch : 19 ------------\n",
            "train loss: 0.18145874490880448\n",
            "test loss: 0.4717368967831135, \t test acc: 87.01171875%\n",
            "--------- epoch : 20 ------------\n",
            "train loss: 0.18026219988646713\n",
            "test loss: 0.4633026272058487, \t test acc: 87.20703125%\n",
            "Best loss: 0.4633026272058487\n",
            "\n",
            "--------- epoch : 21 ------------\n",
            "train loss: 0.17965134678651457\n",
            "test loss: 0.4641754673793912, \t test acc: 87.158203125%\n",
            "--------- epoch : 22 ------------\n",
            "train loss: 0.17991849877264188\n",
            "test loss: 0.45723775681108236, \t test acc: 87.353515625%\n",
            "Best loss: 0.45723775681108236\n",
            "\n",
            "--------- epoch : 23 ------------\n",
            "train loss: 0.1790867592005626\n",
            "test loss: 0.4634359935298562, \t test acc: 87.3046875%\n",
            "--------- epoch : 24 ------------\n",
            "train loss: 0.17955740006721538\n",
            "test loss: 0.4683073638007045, \t test acc: 87.060546875%\n",
            "--------- epoch : 25 ------------\n",
            "train loss: 0.17650770541766417\n",
            "test loss: 0.4676451738923788, \t test acc: 87.060546875%\n",
            "--------- epoch : 26 ------------\n",
            "train loss: 0.17674628143077312\n",
            "test loss: 0.46546477917581797, \t test acc: 87.158203125%\n",
            "--------- epoch : 27 ------------\n",
            "train loss: 0.17816012506575687\n",
            "test loss: 0.46105586271733046, \t test acc: 87.548828125%\n",
            "Epoch    27: reducing learning rate of group 0 to 1.0000e-06.\n",
            "--------- epoch : 28 ------------\n",
            "train loss: 0.17549548881209415\n",
            "test loss: 0.46379629150032997, \t test acc: 87.109375%\n",
            "--------- epoch : 29 ------------\n",
            "train loss: 0.1759489392456801\n",
            "test loss: 0.4701786432415247, \t test acc: 87.3046875%\n",
            "--------- epoch : 30 ------------\n",
            "train loss: 0.17571488492514775\n",
            "test loss: 0.46121916081756353, \t test acc: 87.5%\n",
            "early stop at 30 epoch\n",
            "best loss: 0.45723775681108236,\t best acc: 87.353515625%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.7349903369727342\n",
            "test loss: 0.47120676282793283, \t test acc: 87.3046875%\n",
            "Best loss: 0.47120676282793283\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.7216429462575394\n",
            "test loss: 0.471113090403378, \t test acc: 87.158203125%\n",
            "Best loss: 0.471113090403378\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.7178394234050876\n",
            "test loss: 0.48559097945690155, \t test acc: 87.01171875%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.7047498935590619\n",
            "test loss: 0.4794995188713074, \t test acc: 87.109375%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.6970340638704922\n",
            "test loss: 0.4942085612565279, \t test acc: 86.865234375%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.6960833354488664\n",
            "test loss: 0.4925790438428521, \t test acc: 86.474609375%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.6922334831046022\n",
            "test loss: 0.5079151466488838, \t test acc: 86.1328125%\n",
            "Epoch     7: reducing learning rate of group 0 to 1.0000e-07.\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.6852108661247336\n",
            "test loss: 0.5028421524912119, \t test acc: 85.9375%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.6941580380434575\n",
            "test loss: 0.5045854737982154, \t test acc: 86.1328125%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.686947312367999\n",
            "test loss: 0.5088861659169197, \t test acc: 85.986328125%\n",
            "early stop at 10 epoch\n",
            "best loss: 0.471113090403378,\t best acc: 87.158203125%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 1.1789610648284787\n",
            "test loss: 0.5033654924482107, \t test acc: 86.23046875%\n",
            "Best loss: 0.5033654924482107\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 1.1851650055335916\n",
            "test loss: 0.5145828956738114, \t test acc: 85.7421875%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 1.1818622791248818\n",
            "test loss: 0.5110641848295927, \t test acc: 85.83984375%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 1.1783564537763596\n",
            "test loss: 0.516540989279747, \t test acc: 85.791015625%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 1.175359744740569\n",
            "test loss: 0.5064453976228833, \t test acc: 86.083984375%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 1.1688000394598297\n",
            "test loss: 0.5146570159122348, \t test acc: 85.7421875%\n",
            "Epoch     6: reducing learning rate of group 0 to 1.0000e-08.\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 1.1625187785729119\n",
            "test loss: 0.516611946746707, \t test acc: 85.693359375%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 1.1712626283583434\n",
            "test loss: 0.5137919588014483, \t test acc: 85.693359375%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 1.1755161638493123\n",
            "test loss: 0.5169441187754273, \t test acc: 85.693359375%\n",
            "early stop at 9 epoch\n",
            "best loss: 0.5033654924482107,\t best acc: 86.23046875%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.06779901673207464\n",
            "test loss: 0.5088133737444878, \t test acc: 85.888671875%\n",
            "Best loss: 0.5088133737444878\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.0695800257684744\n",
            "test loss: 0.5257014445960522, \t test acc: 85.44921875%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.06920098331149506\n",
            "test loss: 0.5149605209007859, \t test acc: 85.791015625%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.06966983690938872\n",
            "test loss: 0.5176399769261479, \t test acc: 85.595703125%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.06920672339670685\n",
            "test loss: 0.5180141776800156, \t test acc: 85.546875%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.06862750651476823\n",
            "test loss: 0.5094136483967304, \t test acc: 86.1328125%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.0677773618625234\n",
            "test loss: 0.5158937592059374, \t test acc: 85.888671875%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.06970338395837208\n",
            "test loss: 0.5065758237615228, \t test acc: 86.083984375%\n",
            "Best loss: 0.5065758237615228\n",
            "\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.06871913669063993\n",
            "test loss: 0.5117927147075534, \t test acc: 86.03515625%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.06903028180417807\n",
            "test loss: 0.5130774732679129, \t test acc: 85.791015625%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.0688772942546917\n",
            "test loss: 0.5100376941263676, \t test acc: 85.83984375%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.06880926545304449\n",
            "test loss: 0.5105261979624629, \t test acc: 86.181640625%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.06883061547642169\n",
            "test loss: 0.5102463848888874, \t test acc: 85.83984375%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.06887991578601625\n",
            "test loss: 0.525593088939786, \t test acc: 85.546875%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.06906149817792617\n",
            "test loss: 0.5249450299888849, \t test acc: 85.302734375%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 0.06833058069257633\n",
            "test loss: 0.510579502210021, \t test acc: 85.791015625%\n",
            "early stop at 16 epoch\n",
            "best loss: 0.5065758237615228,\t best acc: 86.083984375%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.08200188450839209\n",
            "test loss: 0.5161042753607035, \t test acc: 85.7421875%\n",
            "Best loss: 0.5161042753607035\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.08211315271404127\n",
            "test loss: 0.5180943664163351, \t test acc: 85.546875%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.08235097063057449\n",
            "test loss: 0.5152454227209091, \t test acc: 85.693359375%\n",
            "Best loss: 0.5152454227209091\n",
            "\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.08155106686298615\n",
            "test loss: 0.5284384153783321, \t test acc: 85.25390625%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.0824623495016409\n",
            "test loss: 0.5191211029887199, \t test acc: 85.7421875%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.08288487787965847\n",
            "test loss: 0.5143136661499739, \t test acc: 86.083984375%\n",
            "Best loss: 0.5143136661499739\n",
            "\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.08369848678779343\n",
            "test loss: 0.5158339561894536, \t test acc: 85.7421875%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.08262533168105976\n",
            "test loss: 0.5131581742316484, \t test acc: 85.693359375%\n",
            "Best loss: 0.5131581742316484\n",
            "\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.08172594452195842\n",
            "test loss: 0.5127500742673874, \t test acc: 85.83984375%\n",
            "Best loss: 0.5127500742673874\n",
            "\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.08163657590099004\n",
            "test loss: 0.5173344286158681, \t test acc: 85.7421875%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.08198656287530194\n",
            "test loss: 0.5064225476235151, \t test acc: 85.888671875%\n",
            "Best loss: 0.5064225476235151\n",
            "\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.08252103829189487\n",
            "test loss: 0.515696169808507, \t test acc: 85.693359375%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.08140746439280717\n",
            "test loss: 0.5153139363974333, \t test acc: 85.83984375%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.0811589028807762\n",
            "test loss: 0.5081269964575768, \t test acc: 86.03515625%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.08203516483468853\n",
            "test loss: 0.5057615507394075, \t test acc: 85.83984375%\n",
            "Best loss: 0.5057615507394075\n",
            "\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 0.08110998765282008\n",
            "test loss: 0.5106252413243055, \t test acc: 85.888671875%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 0.08258453191703428\n",
            "test loss: 0.5187917929142714, \t test acc: 85.546875%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 0.08148101644347543\n",
            "test loss: 0.5052191093564034, \t test acc: 86.279296875%\n",
            "Best loss: 0.5052191093564034\n",
            "\n",
            "--------- epoch : 19 ------------\n",
            "train loss: 0.08255200063728768\n",
            "test loss: 0.5136556290090084, \t test acc: 85.888671875%\n",
            "--------- epoch : 20 ------------\n",
            "train loss: 0.08091296563329904\n",
            "test loss: 0.5234523322433233, \t test acc: 85.400390625%\n",
            "--------- epoch : 21 ------------\n",
            "train loss: 0.08369455727465126\n",
            "test loss: 0.5085592772811651, \t test acc: 85.791015625%\n",
            "--------- epoch : 22 ------------\n",
            "train loss: 0.08134797727689147\n",
            "test loss: 0.5092732682824135, \t test acc: 86.03515625%\n",
            "--------- epoch : 23 ------------\n",
            "train loss: 0.08290385669502227\n",
            "test loss: 0.5123218223452568, \t test acc: 85.888671875%\n",
            "--------- epoch : 24 ------------\n",
            "train loss: 0.08179004911495291\n",
            "test loss: 0.5152558907866478, \t test acc: 85.888671875%\n",
            "--------- epoch : 25 ------------\n",
            "train loss: 0.0815954135082986\n",
            "test loss: 0.5075277332216501, \t test acc: 85.791015625%\n",
            "--------- epoch : 26 ------------\n",
            "train loss: 0.08193335481716887\n",
            "test loss: 0.5147302271798253, \t test acc: 86.03515625%\n",
            "early stop at 26 epoch\n",
            "best loss: 0.5052191093564034,\t best acc: 86.279296875%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.22661800714938538\n",
            "test loss: 0.5108622442930937, \t test acc: 85.9375%\n",
            "Best loss: 0.5108622442930937\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.22663963731864226\n",
            "test loss: 0.5175884226337075, \t test acc: 85.83984375%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.22650422852324403\n",
            "test loss: 0.5062477961182594, \t test acc: 85.7421875%\n",
            "Best loss: 0.5062477961182594\n",
            "\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.2247086760952421\n",
            "test loss: 0.5135636618360877, \t test acc: 85.9375%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.22537864266854266\n",
            "test loss: 0.5141637101769447, \t test acc: 85.83984375%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.22712837596950325\n",
            "test loss: 0.5112355286255479, \t test acc: 85.9375%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.22697209672111532\n",
            "test loss: 0.511301189661026, \t test acc: 86.083984375%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.22475395424534444\n",
            "test loss: 0.5061065033078194, \t test acc: 85.9375%\n",
            "Best loss: 0.5061065033078194\n",
            "\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.22503923470883266\n",
            "test loss: 0.5124392490833998, \t test acc: 85.9375%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.22594181569698063\n",
            "test loss: 0.5107937566936016, \t test acc: 86.083984375%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.2281562782338132\n",
            "test loss: 0.5102284783497453, \t test acc: 85.888671875%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.22426470790220343\n",
            "test loss: 0.5055232113227248, \t test acc: 86.279296875%\n",
            "Best loss: 0.5055232113227248\n",
            "\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.22383870403079884\n",
            "test loss: 0.5120955873280764, \t test acc: 85.9375%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.22726470515455888\n",
            "test loss: 0.514582198113203, \t test acc: 85.791015625%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.22405000290145044\n",
            "test loss: 0.5169052127748728, \t test acc: 85.9375%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 0.22632102077097996\n",
            "test loss: 0.5080643529072404, \t test acc: 86.1328125%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 0.22575399174314478\n",
            "test loss: 0.5150674618780613, \t test acc: 85.791015625%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 0.22516571657489176\n",
            "test loss: 0.5082135144621134, \t test acc: 85.986328125%\n",
            "--------- epoch : 19 ------------\n",
            "train loss: 0.22517803351840246\n",
            "test loss: 0.5110867088660598, \t test acc: 86.1328125%\n",
            "--------- epoch : 20 ------------\n",
            "train loss: 0.226666283591286\n",
            "test loss: 0.5072149792686105, \t test acc: 86.279296875%\n",
            "early stop at 20 epoch\n",
            "best loss: 0.5055232113227248,\t best acc: 86.279296875%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.8679471317192783\n",
            "test loss: 0.5077267652377486, \t test acc: 86.181640625%\n",
            "Best loss: 0.5077267652377486\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.8674712579535402\n",
            "test loss: 0.5134141910821199, \t test acc: 85.9375%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.8729199948518173\n",
            "test loss: 0.5084240762516856, \t test acc: 85.986328125%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.8692288557472436\n",
            "test loss: 0.5120291607454419, \t test acc: 85.9375%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.8697291878254517\n",
            "test loss: 0.5035814112052321, \t test acc: 86.23046875%\n",
            "Best loss: 0.5035814112052321\n",
            "\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.8681394963160806\n",
            "test loss: 0.5078536812216043, \t test acc: 85.9375%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.8776257728105006\n",
            "test loss: 0.5063181733712554, \t test acc: 86.23046875%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.8717699608077174\n",
            "test loss: 0.5142015255987644, \t test acc: 86.03515625%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.8716470029043115\n",
            "test loss: 0.5113791227340698, \t test acc: 85.9375%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.8686575507340224\n",
            "test loss: 0.5018075797706842, \t test acc: 85.9375%\n",
            "Best loss: 0.5018075797706842\n",
            "\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.8613164946436882\n",
            "test loss: 0.5142035149037838, \t test acc: 85.791015625%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.867229365784189\n",
            "test loss: 0.4971321392804384, \t test acc: 86.279296875%\n",
            "Best loss: 0.4971321392804384\n",
            "\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.8611890592652819\n",
            "test loss: 0.511191388592124, \t test acc: 85.986328125%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.8737061299059702\n",
            "test loss: 0.5102076958864927, \t test acc: 86.083984375%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.8680954319627389\n",
            "test loss: 0.5101203825324774, \t test acc: 86.03515625%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 0.8680067451103873\n",
            "test loss: 0.515003084205091, \t test acc: 85.7421875%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 0.8674102594023165\n",
            "test loss: 0.5126864481717348, \t test acc: 85.9375%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 0.8678669181206952\n",
            "test loss: 0.5003302479162812, \t test acc: 86.279296875%\n",
            "--------- epoch : 19 ------------\n",
            "train loss: 0.8679360235514848\n",
            "test loss: 0.5124354511499405, \t test acc: 86.03515625%\n",
            "--------- epoch : 20 ------------\n",
            "train loss: 0.8637599747466005\n",
            "test loss: 0.5110506005585194, \t test acc: 86.083984375%\n",
            "early stop at 20 epoch\n",
            "best loss: 0.4971321392804384,\t best acc: 86.279296875%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 1.503110872014709\n",
            "test loss: 0.5209387624636292, \t test acc: 85.64453125%\n",
            "Best loss: 0.5209387624636292\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 1.5130484181901682\n",
            "test loss: 0.5172462519258261, \t test acc: 85.83984375%\n",
            "Best loss: 0.5172462519258261\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 1.510472573016001\n",
            "test loss: 0.5064549325034022, \t test acc: 86.279296875%\n",
            "Best loss: 0.5064549325034022\n",
            "\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 1.5175474635932757\n",
            "test loss: 0.509030157700181, \t test acc: 85.986328125%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 1.5159486078697701\n",
            "test loss: 0.5126350149512291, \t test acc: 86.1328125%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 1.5181541352168373\n",
            "test loss: 0.5176063813269138, \t test acc: 85.693359375%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 1.5020644956308862\n",
            "test loss: 0.5075751077383757, \t test acc: 86.1328125%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 1.5109962069469949\n",
            "test loss: 0.5151513172313571, \t test acc: 85.693359375%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 1.513246545325155\n",
            "test loss: 0.511313010007143, \t test acc: 85.986328125%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 1.5164312765650128\n",
            "test loss: 0.5113285193219781, \t test acc: 85.888671875%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 1.5181650590637457\n",
            "test loss: 0.5064675342291594, \t test acc: 86.083984375%\n",
            "early stop at 11 epoch\n",
            "best loss: 0.5064549325034022,\t best acc: 86.279296875%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.06688860362476629\n",
            "test loss: 0.5225774198770523, \t test acc: 85.7421875%\n",
            "Best loss: 0.5225774198770523\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.06765044836894325\n",
            "test loss: 0.5057322569191456, \t test acc: 86.03515625%\n",
            "Best loss: 0.5057322569191456\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.06618456775322556\n",
            "test loss: 0.511333653703332, \t test acc: 85.7421875%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.0674631331399407\n",
            "test loss: 0.5034931413829327, \t test acc: 86.181640625%\n",
            "Best loss: 0.5034931413829327\n",
            "\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.06702368044415893\n",
            "test loss: 0.5018121376633644, \t test acc: 86.03515625%\n",
            "Best loss: 0.5018121376633644\n",
            "\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.06741013495332521\n",
            "test loss: 0.5188368409872055, \t test acc: 85.64453125%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.06710913672071436\n",
            "test loss: 0.5171472523361444, \t test acc: 85.791015625%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.06793785923282089\n",
            "test loss: 0.5107346717268229, \t test acc: 85.888671875%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.06631473190677555\n",
            "test loss: 0.5151505265384912, \t test acc: 85.7421875%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.06715309285842207\n",
            "test loss: 0.5070123402401805, \t test acc: 85.986328125%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.06817945163777989\n",
            "test loss: 0.5087610762566328, \t test acc: 85.83984375%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.06640937517437598\n",
            "test loss: 0.5157654583454132, \t test acc: 85.791015625%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.06609375766761925\n",
            "test loss: 0.5089259780943394, \t test acc: 85.986328125%\n",
            "early stop at 13 epoch\n",
            "best loss: 0.5018121376633644,\t best acc: 86.03515625%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.08275411857049102\n",
            "test loss: 0.5155826574191451, \t test acc: 85.791015625%\n",
            "Best loss: 0.5155826574191451\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.08357300828008549\n",
            "test loss: 0.5104645676910877, \t test acc: 85.7421875%\n",
            "Best loss: 0.5104645676910877\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.0826776558128388\n",
            "test loss: 0.5106089618057013, \t test acc: 85.7421875%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.083553749744011\n",
            "test loss: 0.5062005762010813, \t test acc: 86.279296875%\n",
            "Best loss: 0.5062005762010813\n",
            "\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.08382229183031165\n",
            "test loss: 0.5100700408220291, \t test acc: 86.083984375%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.08350540920282187\n",
            "test loss: 0.5191170275211334, \t test acc: 85.791015625%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.08350194893453432\n",
            "test loss: 0.515025875531137, \t test acc: 85.888671875%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.08340784225045987\n",
            "test loss: 0.5065346192568541, \t test acc: 85.9375%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.08381297410992176\n",
            "test loss: 0.5129719581454992, \t test acc: 86.03515625%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.08414111132531063\n",
            "test loss: 0.5096635892987251, \t test acc: 86.083984375%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.0832856550772229\n",
            "test loss: 0.5037682540714741, \t test acc: 86.083984375%\n",
            "Best loss: 0.5037682540714741\n",
            "\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.08347674795304952\n",
            "test loss: 0.5155825782567263, \t test acc: 85.888671875%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.08352048921844234\n",
            "test loss: 0.5170378023758531, \t test acc: 85.9375%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.08297386969966085\n",
            "test loss: 0.5081308046355844, \t test acc: 85.9375%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.08269035870857212\n",
            "test loss: 0.5078943967819214, \t test acc: 86.083984375%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 0.08326361875009278\n",
            "test loss: 0.5142445247620344, \t test acc: 86.03515625%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 0.08245324560076646\n",
            "test loss: 0.5311249615624547, \t test acc: 85.25390625%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 0.08212684590936355\n",
            "test loss: 0.5152129726484418, \t test acc: 85.986328125%\n",
            "--------- epoch : 19 ------------\n",
            "train loss: 0.08422316791003813\n",
            "test loss: 0.5119791561737657, \t test acc: 85.888671875%\n",
            "early stop at 19 epoch\n",
            "best loss: 0.5037682540714741,\t best acc: 86.083984375%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.2447026846525462\n",
            "test loss: 0.5084941387176514, \t test acc: 86.083984375%\n",
            "Best loss: 0.5084941387176514\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.24518951514492865\n",
            "test loss: 0.5180132174864411, \t test acc: 85.791015625%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.24319127129147883\n",
            "test loss: 0.5035735843703151, \t test acc: 86.23046875%\n",
            "Best loss: 0.5035735843703151\n",
            "\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.24450325366595518\n",
            "test loss: 0.5155465789139271, \t test acc: 85.83984375%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.2454709707070952\n",
            "test loss: 0.5076153557747602, \t test acc: 86.1328125%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.24417204102096352\n",
            "test loss: 0.5117440260946751, \t test acc: 86.1328125%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.24695154328061186\n",
            "test loss: 0.5029212050139904, \t test acc: 86.03515625%\n",
            "Best loss: 0.5029212050139904\n",
            "\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.2431023089296144\n",
            "test loss: 0.5123743023723364, \t test acc: 85.693359375%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.24436047341188658\n",
            "test loss: 0.5052094422280788, \t test acc: 86.279296875%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.24661897487290527\n",
            "test loss: 0.5151588562875986, \t test acc: 85.791015625%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.24439139178265695\n",
            "test loss: 0.5090299723669887, \t test acc: 86.083984375%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.24164200161138308\n",
            "test loss: 0.5173607561737299, \t test acc: 85.888671875%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.24506643900404806\n",
            "test loss: 0.5085763288661838, \t test acc: 86.23046875%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.24604160703070785\n",
            "test loss: 0.5095550902187824, \t test acc: 86.181640625%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.24481030739843845\n",
            "test loss: 0.5040746871381998, \t test acc: 86.1328125%\n",
            "early stop at 15 epoch\n",
            "best loss: 0.5029212050139904,\t best acc: 86.03515625%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.965658971796865\n",
            "test loss: 0.5148230530321598, \t test acc: 85.546875%\n",
            "Best loss: 0.5148230530321598\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.9677370112875233\n",
            "test loss: 0.5078764967620373, \t test acc: 86.181640625%\n",
            "Best loss: 0.5078764967620373\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.9653282129894132\n",
            "test loss: 0.5104620680212975, \t test acc: 85.888671875%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.969405219282793\n",
            "test loss: 0.5163554176688194, \t test acc: 85.83984375%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.9658287319800128\n",
            "test loss: 0.5141250714659691, \t test acc: 85.888671875%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.9632022348435029\n",
            "test loss: 0.5014261417090893, \t test acc: 86.23046875%\n",
            "Best loss: 0.5014261417090893\n",
            "\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.9615558000362437\n",
            "test loss: 0.5025868071243167, \t test acc: 86.181640625%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.9684691626740538\n",
            "test loss: 0.5058446116745472, \t test acc: 86.181640625%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.9712849077971085\n",
            "test loss: 0.5027049016207457, \t test acc: 85.9375%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.9669034876253294\n",
            "test loss: 0.5060298079624772, \t test acc: 86.23046875%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.9656742224874704\n",
            "test loss: 0.514099320396781, \t test acc: 85.791015625%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.9737266042958135\n",
            "test loss: 0.5146476728841662, \t test acc: 85.9375%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.9653923252354497\n",
            "test loss: 0.5119242602959275, \t test acc: 86.03515625%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.9711618206423261\n",
            "test loss: 0.5111431404948235, \t test acc: 85.791015625%\n",
            "early stop at 14 epoch\n",
            "best loss: 0.5014261417090893,\t best acc: 86.23046875%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 1.6802507787942886\n",
            "test loss: 0.5079503767192364, \t test acc: 86.03515625%\n",
            "Best loss: 0.5079503767192364\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 1.6857052834137627\n",
            "test loss: 0.5103879887610674, \t test acc: 86.181640625%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 1.6801006392292355\n",
            "test loss: 0.5100993476808071, \t test acc: 86.181640625%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 1.6806478811346965\n",
            "test loss: 0.5001390911638737, \t test acc: 86.083984375%\n",
            "Best loss: 0.5001390911638737\n",
            "\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 1.6910517513751984\n",
            "test loss: 0.5121198054403067, \t test acc: 85.64453125%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 1.680320526594701\n",
            "test loss: 0.5127887949347496, \t test acc: 85.791015625%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 1.6854269783134046\n",
            "test loss: 0.5104323159903288, \t test acc: 85.791015625%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 1.6977185393157213\n",
            "test loss: 0.5068516554310918, \t test acc: 86.083984375%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 1.6738767753476682\n",
            "test loss: 0.502170667052269, \t test acc: 85.986328125%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 1.6932209412688795\n",
            "test loss: 0.507264157757163, \t test acc: 86.181640625%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 1.6959603435319404\n",
            "test loss: 0.5031363721936941, \t test acc: 86.1328125%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 1.6942317608905875\n",
            "test loss: 0.5125179868191481, \t test acc: 85.986328125%\n",
            "early stop at 12 epoch\n",
            "best loss: 0.5001390911638737,\t best acc: 86.083984375%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.06678419965116875\n",
            "test loss: 0.5175655521452427, \t test acc: 85.83984375%\n",
            "Best loss: 0.5175655521452427\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.06558433507123719\n",
            "test loss: 0.5070538111031055, \t test acc: 85.693359375%\n",
            "Best loss: 0.5070538111031055\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.06639099357973623\n",
            "test loss: 0.5103146433830261, \t test acc: 85.9375%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.06618832460726085\n",
            "test loss: 0.50404953956604, \t test acc: 86.1328125%\n",
            "Best loss: 0.50404953956604\n",
            "\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.06639811542371045\n",
            "test loss: 0.5071827806532383, \t test acc: 86.083984375%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.06735813150020398\n",
            "test loss: 0.512823929078877, \t test acc: 85.888671875%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.06700948530646123\n",
            "test loss: 0.5162598351016641, \t test acc: 85.791015625%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.06658866934721237\n",
            "test loss: 0.5169944642111659, \t test acc: 85.791015625%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.06604542980051559\n",
            "test loss: 0.5195351066067815, \t test acc: 85.7421875%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.06697931403861097\n",
            "test loss: 0.5005932580679655, \t test acc: 86.181640625%\n",
            "Best loss: 0.5005932580679655\n",
            "\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.06644666780029303\n",
            "test loss: 0.5113766025751829, \t test acc: 85.9375%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.06829013632934379\n",
            "test loss: 0.5149039542302489, \t test acc: 85.986328125%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.06638618720614392\n",
            "test loss: 0.5082475021481514, \t test acc: 86.03515625%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.06539355260927392\n",
            "test loss: 0.5091236438602209, \t test acc: 85.986328125%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.06697417265209167\n",
            "test loss: 0.5058543793857098, \t test acc: 86.083984375%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 0.0664442791805967\n",
            "test loss: 0.5143731301650405, \t test acc: 86.03515625%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 0.06587277083536205\n",
            "test loss: 0.501431948505342, \t test acc: 86.376953125%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 0.06751397337116626\n",
            "test loss: 0.5059774108231068, \t test acc: 85.9375%\n",
            "early stop at 18 epoch\n",
            "best loss: 0.5005932580679655,\t best acc: 86.181640625%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.08379277546444665\n",
            "test loss: 0.5023473836481571, \t test acc: 86.1328125%\n",
            "Best loss: 0.5023473836481571\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.08263036641089813\n",
            "test loss: 0.5066947797313333, \t test acc: 86.279296875%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.08373903355601689\n",
            "test loss: 0.5085854856297374, \t test acc: 86.083984375%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.08197278333018007\n",
            "test loss: 0.5065507087856531, \t test acc: 85.986328125%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.08237022853901853\n",
            "test loss: 0.5203008595854044, \t test acc: 85.888671875%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.08324048064811074\n",
            "test loss: 0.5098199620842934, \t test acc: 86.1328125%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.0832441379721074\n",
            "test loss: 0.5028779320418835, \t test acc: 86.376953125%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.08311751476772454\n",
            "test loss: 0.5122983055189252, \t test acc: 86.03515625%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.08183671289083103\n",
            "test loss: 0.515026300214231, \t test acc: 86.083984375%\n",
            "early stop at 9 epoch\n",
            "best loss: 0.5023473836481571,\t best acc: 86.1328125%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.25656482204794884\n",
            "test loss: 0.5145836304873228, \t test acc: 85.986328125%\n",
            "Best loss: 0.5145836304873228\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.2539625126544548\n",
            "test loss: 0.5106733059510589, \t test acc: 86.03515625%\n",
            "Best loss: 0.5106733059510589\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.25362546879636205\n",
            "test loss: 0.5056432131677866, \t test acc: 86.23046875%\n",
            "Best loss: 0.5056432131677866\n",
            "\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.25484019250649476\n",
            "test loss: 0.5076201716437936, \t test acc: 86.23046875%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.255483179555639\n",
            "test loss: 0.5046771550551057, \t test acc: 86.083984375%\n",
            "Best loss: 0.5046771550551057\n",
            "\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.2533996037976897\n",
            "test loss: 0.5081566609442234, \t test acc: 86.23046875%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.2542524377450995\n",
            "test loss: 0.5054668402299285, \t test acc: 86.1328125%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.2531387394537096\n",
            "test loss: 0.5171416904777288, \t test acc: 85.888671875%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.254044724304391\n",
            "test loss: 0.4942213296890259, \t test acc: 86.328125%\n",
            "Best loss: 0.4942213296890259\n",
            "\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.25485621692369814\n",
            "test loss: 0.5117802862077951, \t test acc: 85.986328125%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.2548102389049271\n",
            "test loss: 0.499231556430459, \t test acc: 86.1328125%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.25273147168690746\n",
            "test loss: 0.5073558669537306, \t test acc: 86.23046875%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.25397080636542774\n",
            "test loss: 0.4982616100460291, \t test acc: 86.1328125%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.2537123204573341\n",
            "test loss: 0.5012375395745039, \t test acc: 85.986328125%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.2541479334721099\n",
            "test loss: 0.4994724579155445, \t test acc: 86.328125%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 0.2529756391178007\n",
            "test loss: 0.5130716916173697, \t test acc: 86.083984375%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 0.25492793888501497\n",
            "test loss: 0.5063202325254679, \t test acc: 86.1328125%\n",
            "early stop at 17 epoch\n",
            "best loss: 0.4942213296890259,\t best acc: 86.328125%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 1.0152989117347675\n",
            "test loss: 0.5075476616621017, \t test acc: 86.23046875%\n",
            "Best loss: 0.5075476616621017\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 1.017366148531437\n",
            "test loss: 0.5034168437123299, \t test acc: 86.23046875%\n",
            "Best loss: 0.5034168437123299\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 1.0182345444741456\n",
            "test loss: 0.507314614020288, \t test acc: 86.083984375%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 1.0125020237072655\n",
            "test loss: 0.5093136113137007, \t test acc: 86.23046875%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 1.0095711101008498\n",
            "test loss: 0.5033116759732366, \t test acc: 86.181640625%\n",
            "Best loss: 0.5033116759732366\n",
            "\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 1.020213128755922\n",
            "test loss: 0.5035772901028395, \t test acc: 86.279296875%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 1.0201210871986721\n",
            "test loss: 0.5167963746935129, \t test acc: 85.888671875%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 1.0119088548033133\n",
            "test loss: 0.5008145570755005, \t test acc: 86.181640625%\n",
            "Best loss: 0.5008145570755005\n",
            "\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 1.0111603753074356\n",
            "test loss: 0.5044001964852214, \t test acc: 86.181640625%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 1.0174751203993093\n",
            "test loss: 0.5089203212410212, \t test acc: 85.986328125%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 1.0137699518514716\n",
            "test loss: 0.5120422290638089, \t test acc: 86.083984375%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 1.0097566295577132\n",
            "test loss: 0.5011666221544147, \t test acc: 86.328125%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 1.0124877671832624\n",
            "test loss: 0.4971311893314123, \t test acc: 86.328125%\n",
            "Best loss: 0.4971311893314123\n",
            "\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 1.0225346269814863\n",
            "test loss: 0.4999313820153475, \t test acc: 86.1328125%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 1.0228781751964404\n",
            "test loss: 0.5048882998526096, \t test acc: 86.279296875%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 1.0154987346866857\n",
            "test loss: 0.5063275843858719, \t test acc: 86.083984375%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 1.0239148467131283\n",
            "test loss: 0.5049106888473034, \t test acc: 85.791015625%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 1.0161184069255125\n",
            "test loss: 0.5057328473776579, \t test acc: 85.9375%\n",
            "--------- epoch : 19 ------------\n",
            "train loss: 1.0128616195010103\n",
            "test loss: 0.506405595690012, \t test acc: 85.9375%\n",
            "--------- epoch : 20 ------------\n",
            "train loss: 1.0193270869229152\n",
            "test loss: 0.5007495507597923, \t test acc: 86.1328125%\n",
            "--------- epoch : 21 ------------\n",
            "train loss: 1.013522615251334\n",
            "test loss: 0.5150223011150956, \t test acc: 86.083984375%\n",
            "early stop at 21 epoch\n",
            "best loss: 0.4971311893314123,\t best acc: 86.328125%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 1.7846630228602367\n",
            "test loss: 0.5105218747630715, \t test acc: 86.1328125%\n",
            "Best loss: 0.5105218747630715\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 1.775405760044637\n",
            "test loss: 0.5200674310326576, \t test acc: 85.791015625%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 1.7839914508487866\n",
            "test loss: 0.5158607084304094, \t test acc: 86.03515625%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 1.7752594656270484\n",
            "test loss: 0.5075117815285921, \t test acc: 85.986328125%\n",
            "Best loss: 0.5075117815285921\n",
            "\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 1.786971970744755\n",
            "test loss: 0.5156043777242303, \t test acc: 85.986328125%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 1.7691456824541092\n",
            "test loss: 0.5154953273013234, \t test acc: 85.888671875%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 1.7815812962210698\n",
            "test loss: 0.503865348175168, \t test acc: 86.181640625%\n",
            "Best loss: 0.503865348175168\n",
            "\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 1.7588736797156541\n",
            "test loss: 0.5135691408067942, \t test acc: 85.888671875%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 1.780246765717216\n",
            "test loss: 0.509227255359292, \t test acc: 86.03515625%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 1.7843730916147647\n",
            "test loss: 0.5103127723559737, \t test acc: 85.986328125%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 1.7790783501189689\n",
            "test loss: 0.5005646329373121, \t test acc: 85.986328125%\n",
            "Best loss: 0.5005646329373121\n",
            "\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 1.7847813594600428\n",
            "test loss: 0.5161162931472063, \t test acc: 85.7421875%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 1.785232024996177\n",
            "test loss: 0.5133233033120632, \t test acc: 86.1328125%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 1.765745895712272\n",
            "test loss: 0.5075210761278868, \t test acc: 85.9375%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 1.7827419081459874\n",
            "test loss: 0.5139295598492026, \t test acc: 86.03515625%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 1.7802433922238972\n",
            "test loss: 0.5089676696807146, \t test acc: 85.7421875%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 1.7707684046548346\n",
            "test loss: 0.5080710127949715, \t test acc: 85.791015625%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 1.7731752654780513\n",
            "test loss: 0.5099798301234841, \t test acc: 86.1328125%\n",
            "--------- epoch : 19 ------------\n",
            "train loss: 1.7791196477154028\n",
            "test loss: 0.5109113538637757, \t test acc: 86.083984375%\n",
            "early stop at 19 epoch\n",
            "best loss: 0.5005646329373121,\t best acc: 85.986328125%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.0661540492032857\n",
            "test loss: 0.5141556719318032, \t test acc: 85.986328125%\n",
            "Best loss: 0.5141556719318032\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.06711278572354627\n",
            "test loss: 0.5096811093389988, \t test acc: 86.1328125%\n",
            "Best loss: 0.5096811093389988\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.06699850537773708\n",
            "test loss: 0.5042841536924243, \t test acc: 86.181640625%\n",
            "Best loss: 0.5042841536924243\n",
            "\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.06679300374715873\n",
            "test loss: 0.5053443694487214, \t test acc: 86.181640625%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.06708807504290472\n",
            "test loss: 0.5022435113787651, \t test acc: 86.181640625%\n",
            "Best loss: 0.5022435113787651\n",
            "\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.06739845760571568\n",
            "test loss: 0.5078428648412228, \t test acc: 85.791015625%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.06613031722119321\n",
            "test loss: 0.5108579583466053, \t test acc: 86.03515625%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.06759244938502493\n",
            "test loss: 0.510618987493217, \t test acc: 86.181640625%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.06660074831755913\n",
            "test loss: 0.5185611378401518, \t test acc: 85.7421875%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.06630998155426072\n",
            "test loss: 0.5076239015907049, \t test acc: 86.083984375%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.06733687151142437\n",
            "test loss: 0.510086802765727, \t test acc: 85.9375%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.06752504388113385\n",
            "test loss: 0.5031931307166815, \t test acc: 86.181640625%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.068514802209709\n",
            "test loss: 0.5056620063260198, \t test acc: 86.376953125%\n",
            "early stop at 13 epoch\n",
            "best loss: 0.5022435113787651,\t best acc: 86.181640625%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.08505197326936152\n",
            "test loss: 0.515050483867526, \t test acc: 85.986328125%\n",
            "Best loss: 0.515050483867526\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.08363559246873079\n",
            "test loss: 0.5137696787714958, \t test acc: 86.1328125%\n",
            "Best loss: 0.5137696787714958\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.08274654841617397\n",
            "test loss: 0.5179740078747272, \t test acc: 85.888671875%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.08380663374681836\n",
            "test loss: 0.506740446202457, \t test acc: 86.1328125%\n",
            "Best loss: 0.506740446202457\n",
            "\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.08296086054052347\n",
            "test loss: 0.5079865753650665, \t test acc: 85.693359375%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.08368063673539006\n",
            "test loss: 0.5079128816723824, \t test acc: 85.791015625%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.0831092301675159\n",
            "test loss: 0.5049663782119751, \t test acc: 86.03515625%\n",
            "Best loss: 0.5049663782119751\n",
            "\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.08346408933563076\n",
            "test loss: 0.5053270235657692, \t test acc: 86.181640625%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.08266950811704864\n",
            "test loss: 0.5078103244304657, \t test acc: 85.986328125%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.0840586699626368\n",
            "test loss: 0.5082414746284485, \t test acc: 85.888671875%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.08359659074441246\n",
            "test loss: 0.5104205897077918, \t test acc: 86.03515625%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.08282692945035904\n",
            "test loss: 0.5110261691734195, \t test acc: 85.986328125%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.08300419473696662\n",
            "test loss: 0.5109440293163061, \t test acc: 86.181640625%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.08332553667866666\n",
            "test loss: 0.4997695880010724, \t test acc: 86.279296875%\n",
            "Best loss: 0.4997695880010724\n",
            "\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.08397082379087806\n",
            "test loss: 0.5080623431131244, \t test acc: 86.181640625%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 0.08370718837756178\n",
            "test loss: 0.5029313107952476, \t test acc: 86.1328125%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 0.08351020071574528\n",
            "test loss: 0.5020689144730568, \t test acc: 86.181640625%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 0.08337917480536777\n",
            "test loss: 0.5068088416010141, \t test acc: 86.23046875%\n",
            "--------- epoch : 19 ------------\n",
            "train loss: 0.08274614418168431\n",
            "test loss: 0.5111295776441693, \t test acc: 86.083984375%\n",
            "--------- epoch : 20 ------------\n",
            "train loss: 0.08421208911939808\n",
            "test loss: 0.5032797493040562, \t test acc: 86.376953125%\n",
            "--------- epoch : 21 ------------\n",
            "train loss: 0.08484299774484141\n",
            "test loss: 0.5119785815477371, \t test acc: 86.083984375%\n",
            "--------- epoch : 22 ------------\n",
            "train loss: 0.08306613187913013\n",
            "test loss: 0.5062481295317411, \t test acc: 85.9375%\n",
            "early stop at 22 epoch\n",
            "best loss: 0.4997695880010724,\t best acc: 86.279296875%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.2603479082817617\n",
            "test loss: 0.5015713293105364, \t test acc: 86.1328125%\n",
            "Best loss: 0.5015713293105364\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.2587496030913747\n",
            "test loss: 0.5017924830317497, \t test acc: 86.083984375%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.25896702577238495\n",
            "test loss: 0.5178444515913725, \t test acc: 85.9375%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.2591126930454503\n",
            "test loss: 0.501875089481473, \t test acc: 85.986328125%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.2592791049214809\n",
            "test loss: 0.5037268921732903, \t test acc: 86.1328125%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.2591330837458372\n",
            "test loss: 0.5082109160721302, \t test acc: 85.7421875%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.25975506550267985\n",
            "test loss: 0.5087935673072934, \t test acc: 86.279296875%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.2582892423252697\n",
            "test loss: 0.5049430541694164, \t test acc: 86.23046875%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.26001959577526734\n",
            "test loss: 0.5090187024325132, \t test acc: 86.083984375%\n",
            "early stop at 9 epoch\n",
            "best loss: 0.5015713293105364,\t best acc: 86.1328125%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 1.0423009858831116\n",
            "test loss: 0.5076655717566609, \t test acc: 86.23046875%\n",
            "Best loss: 0.5076655717566609\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 1.0505280138357826\n",
            "test loss: 0.5050792517140508, \t test acc: 86.23046875%\n",
            "Best loss: 0.5050792517140508\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 1.0443275732838588\n",
            "test loss: 0.5047019757330418, \t test acc: 86.1328125%\n",
            "Best loss: 0.5047019757330418\n",
            "\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 1.0513199891733087\n",
            "test loss: 0.5089679211378098, \t test acc: 86.23046875%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 1.0415983128806818\n",
            "test loss: 0.5076713692396879, \t test acc: 85.693359375%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 1.0497852524985438\n",
            "test loss: 0.49668285995721817, \t test acc: 86.376953125%\n",
            "Best loss: 0.49668285995721817\n",
            "\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 1.0440407466629278\n",
            "test loss: 0.507234163582325, \t test acc: 86.1328125%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 1.0468697078202083\n",
            "test loss: 0.5049439836293459, \t test acc: 86.279296875%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 1.0449148306380147\n",
            "test loss: 0.5040820855647326, \t test acc: 85.986328125%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 1.0362496826311816\n",
            "test loss: 0.5089877042919397, \t test acc: 86.083984375%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 1.0412958889551784\n",
            "test loss: 0.5022396147251129, \t test acc: 86.083984375%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 1.04248406705649\n",
            "test loss: 0.5023038908839226, \t test acc: 86.03515625%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 1.0413232759936997\n",
            "test loss: 0.5013469159603119, \t test acc: 86.1328125%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 1.0435556892467581\n",
            "test loss: 0.4991497043520212, \t test acc: 86.083984375%\n",
            "early stop at 14 epoch\n",
            "best loss: 0.49668285995721817,\t best acc: 86.376953125%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 1.8359871616829997\n",
            "test loss: 0.5008667232468724, \t test acc: 85.986328125%\n",
            "Best loss: 0.5008667232468724\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 1.8136355896358904\n",
            "test loss: 0.5022335257381201, \t test acc: 86.328125%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 1.8150005820004835\n",
            "test loss: 0.5086261201649904, \t test acc: 85.791015625%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 1.825114401138347\n",
            "test loss: 0.4993617879226804, \t test acc: 86.1328125%\n",
            "Best loss: 0.4993617879226804\n",
            "\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 1.82700591372407\n",
            "test loss: 0.5058614341542125, \t test acc: 86.23046875%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 1.8331085106600886\n",
            "test loss: 0.5055210636928678, \t test acc: 86.181640625%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 1.8494040797586027\n",
            "test loss: 0.5012852605432272, \t test acc: 85.986328125%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 1.8321662286053533\n",
            "test loss: 0.5082898791879416, \t test acc: 86.181640625%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 1.8194886944864108\n",
            "test loss: 0.5155359851196408, \t test acc: 85.9375%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 1.8253697973230611\n",
            "test loss: 0.5112379649654031, \t test acc: 86.083984375%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 1.8236262908448344\n",
            "test loss: 0.5003674030303955, \t test acc: 85.9375%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 1.820489574385726\n",
            "test loss: 0.5150935808196664, \t test acc: 85.888671875%\n",
            "early stop at 12 epoch\n",
            "best loss: 0.4993617879226804,\t best acc: 86.1328125%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.0668567723756575\n",
            "test loss: 0.502928115427494, \t test acc: 86.1328125%\n",
            "Best loss: 0.502928115427494\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.06609415949281791\n",
            "test loss: 0.5069776428863406, \t test acc: 86.03515625%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.06571145788968905\n",
            "test loss: 0.5084595773369074, \t test acc: 86.1328125%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.06528391385369975\n",
            "test loss: 0.5067715719342232, \t test acc: 86.1328125%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.06592974097103528\n",
            "test loss: 0.5084366388618946, \t test acc: 85.986328125%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.06540594130511516\n",
            "test loss: 0.5060685947537422, \t test acc: 85.83984375%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.06492369334496882\n",
            "test loss: 0.5059683620929718, \t test acc: 86.083984375%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.06639635340189157\n",
            "test loss: 0.5078242272138596, \t test acc: 85.83984375%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.06663460440366813\n",
            "test loss: 0.5025940965861082, \t test acc: 86.083984375%\n",
            "Best loss: 0.5025940965861082\n",
            "\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.06517991575452944\n",
            "test loss: 0.505282886326313, \t test acc: 85.9375%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.06701004632708171\n",
            "test loss: 0.5102130677551031, \t test acc: 86.23046875%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.0644456910819787\n",
            "test loss: 0.5071091428399086, \t test acc: 86.03515625%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.06636639018340604\n",
            "test loss: 0.49865798465907574, \t test acc: 86.181640625%\n",
            "Best loss: 0.49865798465907574\n",
            "\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.0646495604077759\n",
            "test loss: 0.5074151046574116, \t test acc: 85.986328125%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.06537590277097795\n",
            "test loss: 0.5075754085555673, \t test acc: 86.181640625%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 0.06438634263189591\n",
            "test loss: 0.5042693298310041, \t test acc: 86.03515625%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 0.06593672969661978\n",
            "test loss: 0.5108026694506407, \t test acc: 86.23046875%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 0.06488250663665973\n",
            "test loss: 0.5065449625253677, \t test acc: 86.279296875%\n",
            "--------- epoch : 19 ------------\n",
            "train loss: 0.06497783197656921\n",
            "test loss: 0.51577855553478, \t test acc: 85.9375%\n",
            "--------- epoch : 20 ------------\n",
            "train loss: 0.06562935168166524\n",
            "test loss: 0.5157525427639484, \t test acc: 86.083984375%\n",
            "--------- epoch : 21 ------------\n",
            "train loss: 0.0649027645425952\n",
            "test loss: 0.5044082840904593, \t test acc: 86.279296875%\n",
            "early stop at 21 epoch\n",
            "best loss: 0.49865798465907574,\t best acc: 86.181640625%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.08208279825908982\n",
            "test loss: 0.507418803870678, \t test acc: 86.23046875%\n",
            "Best loss: 0.507418803870678\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.08288729449977046\n",
            "test loss: 0.5068025756627321, \t test acc: 86.03515625%\n",
            "Best loss: 0.5068025756627321\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.08323980017524699\n",
            "test loss: 0.5053489608690143, \t test acc: 86.181640625%\n",
            "Best loss: 0.5053489608690143\n",
            "\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.08376620594493073\n",
            "test loss: 0.4973055459558964, \t test acc: 86.328125%\n",
            "Best loss: 0.4973055459558964\n",
            "\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.08232209323298025\n",
            "test loss: 0.5085304137319326, \t test acc: 86.1328125%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.08265302824261396\n",
            "test loss: 0.5029688635841012, \t test acc: 86.42578125%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.08429526472869127\n",
            "test loss: 0.5056050829589367, \t test acc: 86.083984375%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.08262633939233163\n",
            "test loss: 0.5051330737769604, \t test acc: 86.083984375%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.08243710416085694\n",
            "test loss: 0.5065133469179273, \t test acc: 86.42578125%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.08328423734106448\n",
            "test loss: 0.505620876327157, \t test acc: 85.888671875%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.08388288230027842\n",
            "test loss: 0.5046840570867062, \t test acc: 86.03515625%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.0838018660231129\n",
            "test loss: 0.5174007806926966, \t test acc: 86.03515625%\n",
            "early stop at 12 epoch\n",
            "best loss: 0.4973055459558964,\t best acc: 86.328125%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.2604556414096252\n",
            "test loss: 0.5000744378194213, \t test acc: 86.181640625%\n",
            "Best loss: 0.5000744378194213\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.2622511861440928\n",
            "test loss: 0.498476792126894, \t test acc: 86.083984375%\n",
            "Best loss: 0.498476792126894\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.26103922554656217\n",
            "test loss: 0.5101017504930496, \t test acc: 85.7421875%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.2614106069440427\n",
            "test loss: 0.51835778914392, \t test acc: 86.083984375%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.2627315262089605\n",
            "test loss: 0.5123503496870399, \t test acc: 86.083984375%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.26142405297445215\n",
            "test loss: 0.4981930498033762, \t test acc: 86.1328125%\n",
            "Best loss: 0.4981930498033762\n",
            "\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.2617947028382965\n",
            "test loss: 0.5025967657566071, \t test acc: 86.1328125%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.260384278776853\n",
            "test loss: 0.49691073037683964, \t test acc: 86.1328125%\n",
            "Best loss: 0.49691073037683964\n",
            "\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.2615754451278759\n",
            "test loss: 0.5027991654351354, \t test acc: 86.083984375%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.2628500564428775\n",
            "test loss: 0.5053515881299973, \t test acc: 85.986328125%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.2626952400997929\n",
            "test loss: 0.5048434240743518, \t test acc: 86.23046875%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.2628101327497026\n",
            "test loss: 0.5048787351697683, \t test acc: 86.23046875%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.26267083661387797\n",
            "test loss: 0.49609846621751785, \t test acc: 86.279296875%\n",
            "Best loss: 0.49609846621751785\n",
            "\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.2625931468994721\n",
            "test loss: 0.5010895375162363, \t test acc: 86.181640625%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.262515457994912\n",
            "test loss: 0.5010280208662152, \t test acc: 86.23046875%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 0.26138766668736935\n",
            "test loss: 0.4998358227312565, \t test acc: 86.181640625%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 0.26230756322974746\n",
            "test loss: 0.5023104269057512, \t test acc: 86.181640625%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 0.2600690370992474\n",
            "test loss: 0.5119305476546288, \t test acc: 86.376953125%\n",
            "--------- epoch : 19 ------------\n",
            "train loss: 0.26279163166232733\n",
            "test loss: 0.498131038621068, \t test acc: 86.279296875%\n",
            "--------- epoch : 20 ------------\n",
            "train loss: 0.260881792915904\n",
            "test loss: 0.5024100001901388, \t test acc: 86.083984375%\n",
            "--------- epoch : 21 ------------\n",
            "train loss: 0.2632404626549586\n",
            "test loss: 0.5073264241218567, \t test acc: 85.9375%\n",
            "early stop at 21 epoch\n",
            "best loss: 0.49609846621751785,\t best acc: 86.279296875%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 1.063706032283928\n",
            "test loss: 0.49725247640162706, \t test acc: 86.279296875%\n",
            "Best loss: 0.49725247640162706\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 1.0654921891248745\n",
            "test loss: 0.505350636318326, \t test acc: 86.03515625%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 1.0583060208870017\n",
            "test loss: 0.499014500528574, \t test acc: 86.083984375%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 1.0582903192743012\n",
            "test loss: 0.5009530130773783, \t test acc: 86.1328125%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 1.0635628237024597\n",
            "test loss: 0.49338481202721596, \t test acc: 86.181640625%\n",
            "Best loss: 0.49338481202721596\n",
            "\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 1.063999605567559\n",
            "test loss: 0.5049431398510933, \t test acc: 86.23046875%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 1.0683324456862782\n",
            "test loss: 0.5096576772630215, \t test acc: 85.888671875%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 1.0643909861860068\n",
            "test loss: 0.5116267362609506, \t test acc: 86.376953125%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 1.059738889336586\n",
            "test loss: 0.4988006427884102, \t test acc: 86.1328125%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 1.0622711641632991\n",
            "test loss: 0.502701299265027, \t test acc: 86.328125%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 1.060009980979173\n",
            "test loss: 0.5040399134159088, \t test acc: 86.328125%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 1.057559354149777\n",
            "test loss: 0.509115831926465, \t test acc: 86.1328125%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 1.0623377425515133\n",
            "test loss: 0.49956618901342154, \t test acc: 86.181640625%\n",
            "early stop at 13 epoch\n",
            "best loss: 0.49338481202721596,\t best acc: 86.181640625%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 1.8749559638292894\n",
            "test loss: 0.5118335634469986, \t test acc: 85.64453125%\n",
            "Best loss: 0.5118335634469986\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 1.8685046181730602\n",
            "test loss: 0.5024278769269586, \t test acc: 86.083984375%\n",
            "Best loss: 0.5024278769269586\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 1.861373787988787\n",
            "test loss: 0.4978420175611973, \t test acc: 86.083984375%\n",
            "Best loss: 0.4978420175611973\n",
            "\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 1.8394177141396895\n",
            "test loss: 0.5092300875112414, \t test acc: 86.279296875%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 1.8725608781627987\n",
            "test loss: 0.5032699452713132, \t test acc: 86.474609375%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 1.84218411989834\n",
            "test loss: 0.49456011317670345, \t test acc: 86.181640625%\n",
            "Best loss: 0.49456011317670345\n",
            "\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 1.8611391115447748\n",
            "test loss: 0.4987978832796216, \t test acc: 86.376953125%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 1.8525964365057324\n",
            "test loss: 0.5008815918117762, \t test acc: 85.986328125%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 1.8583295721074808\n",
            "test loss: 0.5009421547874808, \t test acc: 86.474609375%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 1.8621707927921545\n",
            "test loss: 0.506788220256567, \t test acc: 86.23046875%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 1.860698789358139\n",
            "test loss: 0.49863777682185173, \t test acc: 86.03515625%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 1.861833129240119\n",
            "test loss: 0.5166054693982005, \t test acc: 86.1328125%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 1.861765258986017\n",
            "test loss: 0.5046062469482422, \t test acc: 85.986328125%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 1.8576939449362133\n",
            "test loss: 0.5038599520921707, \t test acc: 86.03515625%\n",
            "early stop at 14 epoch\n",
            "best loss: 0.49456011317670345,\t best acc: 86.181640625%\n",
            "\n",
            "\n",
            "Loaded pretrained weights for efficientnet-b4\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.6960847708194152\n",
            "test loss: 0.5029420778155327, \t test acc: 82.421875%\n",
            "Best loss: 0.5029420778155327\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.28120354287650273\n",
            "test loss: 1.0214227605611086, \t test acc: 57.12890625%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.1803242515610612\n",
            "test loss: 0.4446513745933771, \t test acc: 82.421875%\n",
            "Best loss: 0.4446513745933771\n",
            "\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.14020011800786722\n",
            "test loss: 0.5178025085479021, \t test acc: 79.150390625%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.11701450308622874\n",
            "test loss: 0.41135536693036556, \t test acc: 83.30078125%\n",
            "Best loss: 0.41135536693036556\n",
            "\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.10584328032058218\n",
            "test loss: 0.4349288735538721, \t test acc: 83.10546875%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.09531518849341766\n",
            "test loss: 0.532225739210844, \t test acc: 80.37109375%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.08409115085985673\n",
            "test loss: 0.48105927137658, \t test acc: 82.470703125%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.07615917792503277\n",
            "test loss: 0.4116173442453146, \t test acc: 85.400390625%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.0713255578328086\n",
            "test loss: 0.4388436824083328, \t test acc: 83.642578125%\n",
            "Epoch    10: reducing learning rate of group 0 to 1.0000e-04.\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.048156978602966534\n",
            "test loss: 0.4077972797676921, \t test acc: 87.109375%\n",
            "Best loss: 0.4077972797676921\n",
            "\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.039983184557691544\n",
            "test loss: 0.41293557826429605, \t test acc: 87.158203125%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.03767600483969664\n",
            "test loss: 0.4242233019322157, \t test acc: 86.572265625%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.03403490313085849\n",
            "test loss: 0.4406793862581253, \t test acc: 86.962890625%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.032297770998886095\n",
            "test loss: 0.44637184776365757, \t test acc: 87.01171875%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 0.030310597199865657\n",
            "test loss: 0.4463037261739373, \t test acc: 87.3046875%\n",
            "Epoch    16: reducing learning rate of group 0 to 1.0000e-05.\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 0.026495964286605948\n",
            "test loss: 0.4438143065199256, \t test acc: 87.3046875%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 0.026847878481139956\n",
            "test loss: 0.43523271568119526, \t test acc: 87.40234375%\n",
            "--------- epoch : 19 ------------\n",
            "train loss: 0.026287250858529107\n",
            "test loss: 0.44947442412376404, \t test acc: 87.158203125%\n",
            "early stop at 19 epoch\n",
            "best loss: 0.4077972797676921,\t best acc: 87.109375%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.04597053928908123\n",
            "test loss: 0.4435715125873685, \t test acc: 87.353515625%\n",
            "Best loss: 0.4435715125873685\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.04581129060977179\n",
            "test loss: 0.4498572265729308, \t test acc: 87.01171875%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.04568897762700268\n",
            "test loss: 0.44960009306669235, \t test acc: 87.158203125%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.044607482354521104\n",
            "test loss: 0.4487104006111622, \t test acc: 87.109375%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.045287468501002244\n",
            "test loss: 0.44895883463323116, \t test acc: 87.353515625%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.0442116295453161\n",
            "test loss: 0.4528151834383607, \t test acc: 87.20703125%\n",
            "Epoch     6: reducing learning rate of group 0 to 1.0000e-06.\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.04443407491983279\n",
            "test loss: 0.4452252425253391, \t test acc: 87.59765625%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.04417987267041336\n",
            "test loss: 0.4541868772357702, \t test acc: 86.9140625%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.04384407171291178\n",
            "test loss: 0.45358441956341267, \t test acc: 87.158203125%\n",
            "early stop at 9 epoch\n",
            "best loss: 0.4435715125873685,\t best acc: 87.353515625%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.23970333462500054\n",
            "test loss: 0.44874216336756945, \t test acc: 87.3046875%\n",
            "Best loss: 0.44874216336756945\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.2378067778342444\n",
            "test loss: 0.45266739279031754, \t test acc: 87.109375%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.2390778889150723\n",
            "test loss: 0.45023982878774405, \t test acc: 87.353515625%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.23872281708147214\n",
            "test loss: 0.4539671614766121, \t test acc: 87.3046875%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.2393020100729621\n",
            "test loss: 0.4523470802232623, \t test acc: 87.3046875%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.2375035463305919\n",
            "test loss: 0.4531244784593582, \t test acc: 87.353515625%\n",
            "Epoch     6: reducing learning rate of group 0 to 1.0000e-07.\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.2340122079881637\n",
            "test loss: 0.4476786730811, \t test acc: 87.109375%\n",
            "Best loss: 0.4476786730811\n",
            "\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.2350605973245009\n",
            "test loss: 0.4555687829852104, \t test acc: 87.255859375%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.23496194799309192\n",
            "test loss: 0.4616641355678439, \t test acc: 87.01171875%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.23442837547348894\n",
            "test loss: 0.4550824072211981, \t test acc: 87.060546875%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.23453035067928873\n",
            "test loss: 0.4540938697755337, \t test acc: 87.109375%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.23537088436600956\n",
            "test loss: 0.45765478536486626, \t test acc: 87.158203125%\n",
            "Epoch    12: reducing learning rate of group 0 to 1.0000e-08.\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.23643553337973097\n",
            "test loss: 0.46056706458330154, \t test acc: 86.962890625%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.23582327673616615\n",
            "test loss: 0.45083036459982395, \t test acc: 87.158203125%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.23642681717224742\n",
            "test loss: 0.460031914524734, \t test acc: 87.01171875%\n",
            "early stop at 15 epoch\n",
            "best loss: 0.4476786730811,\t best acc: 87.109375%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 1.0857974115921103\n",
            "test loss: 0.4544718870893121, \t test acc: 87.20703125%\n",
            "Best loss: 0.4544718870893121\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 1.0712421824750693\n",
            "test loss: 0.44794860389083624, \t test acc: 87.109375%\n",
            "Best loss: 0.44794860389083624\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 1.0902873682587042\n",
            "test loss: 0.4597410783171654, \t test acc: 87.060546875%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 1.0838016864398252\n",
            "test loss: 0.450657962821424, \t test acc: 87.20703125%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 1.0835750938757607\n",
            "test loss: 0.45836999360471964, \t test acc: 86.9140625%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 1.0749428576749305\n",
            "test loss: 0.46675892267376184, \t test acc: 87.060546875%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 1.0826444237128547\n",
            "test loss: 0.4638622933998704, \t test acc: 87.109375%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 1.0841139312023702\n",
            "test loss: 0.45489597599953413, \t test acc: 87.109375%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 1.0813535390340763\n",
            "test loss: 0.4522743597626686, \t test acc: 87.060546875%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 1.0733623362105826\n",
            "test loss: 0.458634652197361, \t test acc: 86.962890625%\n",
            "early stop at 10 epoch\n",
            "best loss: 0.44794860389083624,\t best acc: 87.109375%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 1.9236354801965796\n",
            "test loss: 0.45391529705375433, \t test acc: 87.060546875%\n",
            "Best loss: 0.45391529705375433\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 1.9267339460227801\n",
            "test loss: 0.45738964155316353, \t test acc: 87.158203125%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 1.9266629912283109\n",
            "test loss: 0.46464784629642963, \t test acc: 86.9140625%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 1.937125352413758\n",
            "test loss: 0.4571095434948802, \t test acc: 87.01171875%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 1.9254020737565083\n",
            "test loss: 0.4627443328499794, \t test acc: 87.158203125%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 1.9183167841123498\n",
            "test loss: 0.45884200371801853, \t test acc: 86.962890625%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 1.9153943055349847\n",
            "test loss: 0.4614987326785922, \t test acc: 86.962890625%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 1.9278514372265858\n",
            "test loss: 0.4511764030903578, \t test acc: 87.353515625%\n",
            "Best loss: 0.4511764030903578\n",
            "\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 1.9290125616218732\n",
            "test loss: 0.46537037938833237, \t test acc: 87.109375%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 1.9205979173598082\n",
            "test loss: 0.45988980028778315, \t test acc: 87.109375%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 1.925887890484022\n",
            "test loss: 0.46309896279126406, \t test acc: 87.01171875%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 1.921632236112719\n",
            "test loss: 0.46489991527050734, \t test acc: 86.9140625%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 1.9268545586129893\n",
            "test loss: 0.45886713918298483, \t test acc: 86.962890625%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 1.9280013066271078\n",
            "test loss: 0.46074591763317585, \t test acc: 86.962890625%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 1.9231750129357628\n",
            "test loss: 0.45653690677136183, \t test acc: 87.20703125%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 1.9237660266782926\n",
            "test loss: 0.45738879404962063, \t test acc: 87.060546875%\n",
            "early stop at 16 epoch\n",
            "best loss: 0.4511764030903578,\t best acc: 87.353515625%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.02601241816908283\n",
            "test loss: 0.46161018777638674, \t test acc: 87.01171875%\n",
            "Best loss: 0.46161018777638674\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.027540958451066654\n",
            "test loss: 0.45969459507614374, \t test acc: 87.20703125%\n",
            "Best loss: 0.45969459507614374\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.026403939720931583\n",
            "test loss: 0.4609485790133476, \t test acc: 87.01171875%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.02636203453268694\n",
            "test loss: 0.4573848294094205, \t test acc: 87.01171875%\n",
            "Best loss: 0.4573848294094205\n",
            "\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.026042350117400612\n",
            "test loss: 0.46116472501307726, \t test acc: 86.962890625%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.026885132310385612\n",
            "test loss: 0.45317776035517454, \t test acc: 87.01171875%\n",
            "Best loss: 0.45317776035517454\n",
            "\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.026537895172267505\n",
            "test loss: 0.4576284736394882, \t test acc: 87.060546875%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.026095920728276604\n",
            "test loss: 0.4627391416579485, \t test acc: 86.962890625%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.026607375789393227\n",
            "test loss: 0.4612151086330414, \t test acc: 87.20703125%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.025577162291206743\n",
            "test loss: 0.463290236890316, \t test acc: 87.01171875%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.026501388430757368\n",
            "test loss: 0.45677990559488535, \t test acc: 87.060546875%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.02608657202116497\n",
            "test loss: 0.4637394743040204, \t test acc: 87.01171875%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.025964680492230083\n",
            "test loss: 0.4602861311286688, \t test acc: 87.060546875%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.026262930305877133\n",
            "test loss: 0.4516192525625229, \t test acc: 87.060546875%\n",
            "Best loss: 0.4516192525625229\n",
            "\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.026833778935606064\n",
            "test loss: 0.45911700278520584, \t test acc: 87.3046875%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 0.0276738134081192\n",
            "test loss: 0.45723781269043684, \t test acc: 87.255859375%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 0.026660383947234113\n",
            "test loss: 0.4525529323145747, \t test acc: 87.158203125%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 0.026495563807775794\n",
            "test loss: 0.45491530187428, \t test acc: 87.060546875%\n",
            "--------- epoch : 19 ------------\n",
            "train loss: 0.02599762066307923\n",
            "test loss: 0.4681784985587001, \t test acc: 86.9140625%\n",
            "--------- epoch : 20 ------------\n",
            "train loss: 0.026858325784459063\n",
            "test loss: 0.4631386576220393, \t test acc: 87.01171875%\n",
            "--------- epoch : 21 ------------\n",
            "train loss: 0.02601193073813034\n",
            "test loss: 0.46426873188465834, \t test acc: 86.9140625%\n",
            "--------- epoch : 22 ------------\n",
            "train loss: 0.026517880630031552\n",
            "test loss: 0.4545644959434867, \t test acc: 87.255859375%\n",
            "early stop at 22 epoch\n",
            "best loss: 0.4516192525625229,\t best acc: 87.060546875%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.05084506460748937\n",
            "test loss: 0.46165730711072683, \t test acc: 87.109375%\n",
            "Best loss: 0.46165730711072683\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.0509064719080925\n",
            "test loss: 0.45405644457787275, \t test acc: 87.20703125%\n",
            "Best loss: 0.45405644457787275\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.051024311262628304\n",
            "test loss: 0.4592804117128253, \t test acc: 87.158203125%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.050862545653691756\n",
            "test loss: 0.4557373123243451, \t test acc: 87.158203125%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.05195511679124573\n",
            "test loss: 0.4588111648336053, \t test acc: 87.20703125%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.051518074358287064\n",
            "test loss: 0.4574069967493415, \t test acc: 87.158203125%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.05234445024119771\n",
            "test loss: 0.4590438771992922, \t test acc: 87.109375%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.05122478101807444\n",
            "test loss: 0.46436655707657337, \t test acc: 87.060546875%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.051576929106174604\n",
            "test loss: 0.4680055882781744, \t test acc: 86.9140625%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.05149346535377528\n",
            "test loss: 0.4659355776384473, \t test acc: 87.060546875%\n",
            "early stop at 10 epoch\n",
            "best loss: 0.45405644457787275,\t best acc: 87.20703125%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.3023447796864354\n",
            "test loss: 0.4625114928930998, \t test acc: 86.9140625%\n",
            "Best loss: 0.4625114928930998\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.29967471754745295\n",
            "test loss: 0.453750966116786, \t test acc: 87.3046875%\n",
            "Best loss: 0.453750966116786\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.2999613293973\n",
            "test loss: 0.46384555380791426, \t test acc: 86.9140625%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.3005742155339407\n",
            "test loss: 0.463292439468205, \t test acc: 87.01171875%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.30054896308676055\n",
            "test loss: 0.4536990746855736, \t test acc: 87.109375%\n",
            "Best loss: 0.4536990746855736\n",
            "\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.30288476140602777\n",
            "test loss: 0.4579608738422394, \t test acc: 87.060546875%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.3004957196181235\n",
            "test loss: 0.46126277931034565, \t test acc: 87.109375%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.29887892791758414\n",
            "test loss: 0.4568265685811639, \t test acc: 87.109375%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.3024160258633935\n",
            "test loss: 0.4632576769217849, \t test acc: 87.109375%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.30078169930240384\n",
            "test loss: 0.4621704025194049, \t test acc: 87.109375%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.30091684303529886\n",
            "test loss: 0.45388836320489645, \t test acc: 87.3046875%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.3024769522573637\n",
            "test loss: 0.4560620114207268, \t test acc: 87.20703125%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.29947196028154827\n",
            "test loss: 0.46761582884937525, \t test acc: 87.109375%\n",
            "early stop at 13 epoch\n",
            "best loss: 0.4536990746855736,\t best acc: 87.109375%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 1.4046908701243608\n",
            "test loss: 0.46561729349195957, \t test acc: 87.060546875%\n",
            "Best loss: 0.46561729349195957\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 1.414321701163831\n",
            "test loss: 0.4587738448753953, \t test acc: 87.109375%\n",
            "Best loss: 0.4587738448753953\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 1.419320052732592\n",
            "test loss: 0.45864449720829725, \t test acc: 87.20703125%\n",
            "Best loss: 0.45864449720829725\n",
            "\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 1.4200835454723109\n",
            "test loss: 0.4605263164266944, \t test acc: 87.158203125%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 1.4039491512205289\n",
            "test loss: 0.4622263992205262, \t test acc: 87.01171875%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 1.4078897626503655\n",
            "test loss: 0.45570253301411867, \t test acc: 87.060546875%\n",
            "Best loss: 0.45570253301411867\n",
            "\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 1.406274820799413\n",
            "test loss: 0.46274891030043364, \t test acc: 87.109375%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 1.4068407183108123\n",
            "test loss: 0.46005998738110065, \t test acc: 87.158203125%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 1.4094531445399574\n",
            "test loss: 0.46076539158821106, \t test acc: 86.865234375%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 1.402233754163203\n",
            "test loss: 0.45127107296139, \t test acc: 87.158203125%\n",
            "Best loss: 0.45127107296139\n",
            "\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 1.4147178511256757\n",
            "test loss: 0.4580560876056552, \t test acc: 87.20703125%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 1.405079447056936\n",
            "test loss: 0.463709338568151, \t test acc: 87.109375%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 1.4076031925885573\n",
            "test loss: 0.45395193435251713, \t test acc: 87.20703125%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 1.4082450361355492\n",
            "test loss: 0.45617434196174145, \t test acc: 87.158203125%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 1.4065559091775313\n",
            "test loss: 0.4707734454423189, \t test acc: 86.81640625%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 1.4090639356037844\n",
            "test loss: 0.4566608499735594, \t test acc: 87.060546875%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 1.4116543764653413\n",
            "test loss: 0.46062283031642437, \t test acc: 87.01171875%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 1.4240007510651713\n",
            "test loss: 0.45648588333278894, \t test acc: 87.158203125%\n",
            "early stop at 18 epoch\n",
            "best loss: 0.45127107296139,\t best acc: 87.158203125%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 2.5212394951478294\n",
            "test loss: 0.4662828613072634, \t test acc: 87.158203125%\n",
            "Best loss: 0.4662828613072634\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 2.518318366745244\n",
            "test loss: 0.46048244927078485, \t test acc: 87.060546875%\n",
            "Best loss: 0.46048244927078485\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 2.5170564580222834\n",
            "test loss: 0.45926067885011435, \t test acc: 87.109375%\n",
            "Best loss: 0.45926067885011435\n",
            "\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 2.505855570020883\n",
            "test loss: 0.45797630585730076, \t test acc: 87.255859375%\n",
            "Best loss: 0.45797630585730076\n",
            "\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 2.5227584644504217\n",
            "test loss: 0.4520694501698017, \t test acc: 87.01171875%\n",
            "Best loss: 0.4520694501698017\n",
            "\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 2.5166585134423296\n",
            "test loss: 0.4622759511694312, \t test acc: 86.962890625%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 2.5289360168187516\n",
            "test loss: 0.45149696711450815, \t test acc: 87.158203125%\n",
            "Best loss: 0.45149696711450815\n",
            "\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 2.51594754146493\n",
            "test loss: 0.4550353977829218, \t test acc: 87.109375%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 2.5202649980783463\n",
            "test loss: 0.46023641526699066, \t test acc: 87.060546875%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 2.5210193596456363\n",
            "test loss: 0.4561227969825268, \t test acc: 87.109375%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 2.51282179420409\n",
            "test loss: 0.46438021678477526, \t test acc: 87.109375%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 2.5070963806432225\n",
            "test loss: 0.45617752987891436, \t test acc: 87.20703125%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 2.510412299114725\n",
            "test loss: 0.46140329726040363, \t test acc: 87.158203125%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 2.5118999001772506\n",
            "test loss: 0.45942213013768196, \t test acc: 87.20703125%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 2.4949636627798495\n",
            "test loss: 0.4615274900570512, \t test acc: 86.9140625%\n",
            "early stop at 15 epoch\n",
            "best loss: 0.45149696711450815,\t best acc: 87.158203125%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.027027090905112742\n",
            "test loss: 0.46602068562060595, \t test acc: 86.865234375%\n",
            "Best loss: 0.46602068562060595\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.026127169242776607\n",
            "test loss: 0.45501541066914797, \t test acc: 87.20703125%\n",
            "Best loss: 0.45501541066914797\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.026347894449313375\n",
            "test loss: 0.45549285039305687, \t test acc: 87.109375%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.02726236964656931\n",
            "test loss: 0.45957648381590843, \t test acc: 87.060546875%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.027053547724474058\n",
            "test loss: 0.4589370861649513, \t test acc: 87.060546875%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.027213305381693594\n",
            "test loss: 0.4640319310128689, \t test acc: 86.9140625%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.026513141600172156\n",
            "test loss: 0.46522949635982513, \t test acc: 87.060546875%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.026707701795005603\n",
            "test loss: 0.4548168061301112, \t test acc: 87.255859375%\n",
            "Best loss: 0.4548168061301112\n",
            "\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.026711613704126492\n",
            "test loss: 0.4686858216300607, \t test acc: 87.01171875%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.026918035511777776\n",
            "test loss: 0.4612154318019748, \t test acc: 86.9140625%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.02735084692603382\n",
            "test loss: 0.4597024079412222, \t test acc: 87.060546875%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.02673942320878901\n",
            "test loss: 0.4639743734151125, \t test acc: 87.060546875%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.026397835346870124\n",
            "test loss: 0.4612592989578843, \t test acc: 87.01171875%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.027322381380540522\n",
            "test loss: 0.45853658486157656, \t test acc: 87.060546875%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.027272638571246163\n",
            "test loss: 0.46346503496170044, \t test acc: 87.109375%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 0.0264940631231698\n",
            "test loss: 0.46103584952652454, \t test acc: 87.20703125%\n",
            "early stop at 16 epoch\n",
            "best loss: 0.4548168061301112,\t best acc: 87.255859375%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.05539976795325461\n",
            "test loss: 0.46449541952461004, \t test acc: 86.9140625%\n",
            "Best loss: 0.46449541952461004\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.05465522345722369\n",
            "test loss: 0.4616507673636079, \t test acc: 87.060546875%\n",
            "Best loss: 0.4616507673636079\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.056027096007829125\n",
            "test loss: 0.45679745450615883, \t test acc: 87.060546875%\n",
            "Best loss: 0.45679745450615883\n",
            "\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.055218945601550135\n",
            "test loss: 0.4546783650293946, \t test acc: 87.20703125%\n",
            "Best loss: 0.4546783650293946\n",
            "\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.05508447225893969\n",
            "test loss: 0.46140874829143286, \t test acc: 87.109375%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.05480435417722101\n",
            "test loss: 0.4514704644680023, \t test acc: 87.109375%\n",
            "Best loss: 0.4514704644680023\n",
            "\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.055285443167161684\n",
            "test loss: 0.4623597050085664, \t test acc: 86.962890625%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.05566024984759481\n",
            "test loss: 0.4634464746341109, \t test acc: 86.9140625%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.05583525438914481\n",
            "test loss: 0.45751617290079594, \t test acc: 87.060546875%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.05483788513052075\n",
            "test loss: 0.45052017644047737, \t test acc: 87.158203125%\n",
            "Best loss: 0.45052017644047737\n",
            "\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.05622467192132836\n",
            "test loss: 0.4538800744339824, \t test acc: 87.158203125%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.05571276982031439\n",
            "test loss: 0.4580962285399437, \t test acc: 87.109375%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.05550208531886987\n",
            "test loss: 0.4588521644473076, \t test acc: 87.01171875%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.05562912304278301\n",
            "test loss: 0.4534563161432743, \t test acc: 87.060546875%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.056070310569813715\n",
            "test loss: 0.4534372938796878, \t test acc: 87.20703125%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 0.055121705543411816\n",
            "test loss: 0.4561377726495266, \t test acc: 87.158203125%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 0.05579091118567664\n",
            "test loss: 0.4513081479817629, \t test acc: 87.255859375%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 0.055459279587249395\n",
            "test loss: 0.4611202860251069, \t test acc: 86.9140625%\n",
            "early stop at 18 epoch\n",
            "best loss: 0.45052017644047737,\t best acc: 87.158203125%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.3437956529467002\n",
            "test loss: 0.46558190416544676, \t test acc: 86.865234375%\n",
            "Best loss: 0.46558190416544676\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.34414096192821214\n",
            "test loss: 0.45597731228917837, \t test acc: 87.109375%\n",
            "Best loss: 0.45597731228917837\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.3397251539256262\n",
            "test loss: 0.46708326041698456, \t test acc: 87.01171875%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.3397011128456696\n",
            "test loss: 0.45127172768116, \t test acc: 87.3046875%\n",
            "Best loss: 0.45127172768116\n",
            "\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.33768717669274495\n",
            "test loss: 0.46494833566248417, \t test acc: 87.01171875%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.3421886632947818\n",
            "test loss: 0.455470179207623, \t test acc: 87.255859375%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.3425506644922754\n",
            "test loss: 0.4631503652781248, \t test acc: 87.060546875%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.34342148397927696\n",
            "test loss: 0.45672235917299986, \t test acc: 87.109375%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.34456933889052144\n",
            "test loss: 0.46816811617463827, \t test acc: 86.962890625%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.3422239451952603\n",
            "test loss: 0.4668424231931567, \t test acc: 87.060546875%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.34144121586628584\n",
            "test loss: 0.45807677041739225, \t test acc: 87.158203125%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.3430205669416034\n",
            "test loss: 0.4571095574647188, \t test acc: 87.060546875%\n",
            "early stop at 12 epoch\n",
            "best loss: 0.45127172768116,\t best acc: 87.3046875%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 1.608664118077444\n",
            "test loss: 0.4593186341226101, \t test acc: 87.158203125%\n",
            "Best loss: 0.4593186341226101\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 1.5994196100079494\n",
            "test loss: 0.4624502742663026, \t test acc: 87.01171875%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 1.6274565432382666\n",
            "test loss: 0.46270495653152466, \t test acc: 86.962890625%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 1.604647296278373\n",
            "test loss: 0.45653520338237286, \t test acc: 87.158203125%\n",
            "Best loss: 0.45653520338237286\n",
            "\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 1.6163323813158532\n",
            "test loss: 0.45982307381927967, \t test acc: 86.962890625%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 1.6180318205252937\n",
            "test loss: 0.4615391828119755, \t test acc: 87.01171875%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 1.6131123634784117\n",
            "test loss: 0.4603269202634692, \t test acc: 87.255859375%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 1.620814127118691\n",
            "test loss: 0.45638363901525736, \t test acc: 86.962890625%\n",
            "Best loss: 0.45638363901525736\n",
            "\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 1.5994479889455049\n",
            "test loss: 0.45810091961175203, \t test acc: 87.158203125%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 1.60602700451146\n",
            "test loss: 0.45842913165688515, \t test acc: 87.109375%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 1.6184319918570311\n",
            "test loss: 0.46595856733620167, \t test acc: 87.01171875%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 1.6206924222085788\n",
            "test loss: 0.46435716561973095, \t test acc: 87.060546875%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 1.6108728005834247\n",
            "test loss: 0.4558436004444957, \t test acc: 87.20703125%\n",
            "Best loss: 0.4558436004444957\n",
            "\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 1.6258474057135375\n",
            "test loss: 0.4700073413550854, \t test acc: 86.962890625%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 1.6148837059736252\n",
            "test loss: 0.45333952456712723, \t test acc: 87.255859375%\n",
            "Best loss: 0.45333952456712723\n",
            "\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 1.612914062064627\n",
            "test loss: 0.44454860873520374, \t test acc: 87.20703125%\n",
            "Best loss: 0.44454860873520374\n",
            "\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 1.611970524425092\n",
            "test loss: 0.45921914651989937, \t test acc: 87.109375%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 1.615543430266173\n",
            "test loss: 0.4630832262337208, \t test acc: 87.109375%\n",
            "--------- epoch : 19 ------------\n",
            "train loss: 1.6016756665447485\n",
            "test loss: 0.46158004086464643, \t test acc: 86.962890625%\n",
            "--------- epoch : 20 ------------\n",
            "train loss: 1.6127238960369774\n",
            "test loss: 0.44521274138242006, \t test acc: 87.158203125%\n",
            "--------- epoch : 21 ------------\n",
            "train loss: 1.610373976437942\n",
            "test loss: 0.45932820066809654, \t test acc: 87.158203125%\n",
            "--------- epoch : 22 ------------\n",
            "train loss: 1.622471400577089\n",
            "test loss: 0.45982643868774176, \t test acc: 86.962890625%\n",
            "--------- epoch : 23 ------------\n",
            "train loss: 1.604711058994998\n",
            "test loss: 0.45105817914009094, \t test acc: 87.158203125%\n",
            "--------- epoch : 24 ------------\n",
            "train loss: 1.6173563010018805\n",
            "test loss: 0.4659329541027546, \t test acc: 86.9140625%\n",
            "early stop at 24 epoch\n",
            "best loss: 0.44454860873520374,\t best acc: 87.20703125%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 2.8947778758795364\n",
            "test loss: 0.46176926605403423, \t test acc: 86.962890625%\n",
            "Best loss: 0.46176926605403423\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 2.872915768105051\n",
            "test loss: 0.4626405509188771, \t test acc: 86.962890625%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 2.8599893385949344\n",
            "test loss: 0.46517297718673944, \t test acc: 86.9140625%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 2.863613943690839\n",
            "test loss: 0.463534202426672, \t test acc: 86.962890625%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 2.905679227217384\n",
            "test loss: 0.45996854081749916, \t test acc: 87.060546875%\n",
            "Best loss: 0.45996854081749916\n",
            "\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 2.884897506755331\n",
            "test loss: 0.46581913996487856, \t test acc: 87.060546875%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 2.903760067794634\n",
            "test loss: 0.46931403037160635, \t test acc: 86.962890625%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 2.8911968365959497\n",
            "test loss: 0.46371861826628447, \t test acc: 87.060546875%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 2.8940809524577595\n",
            "test loss: 0.46247635036706924, \t test acc: 87.01171875%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 2.8843647410040316\n",
            "test loss: 0.4677447462454438, \t test acc: 87.060546875%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 2.9005503045476\n",
            "test loss: 0.4602456595748663, \t test acc: 87.158203125%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 2.8772426429002183\n",
            "test loss: 0.4612219398841262, \t test acc: 86.865234375%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 2.8805652856826782\n",
            "test loss: 0.45623788330703974, \t test acc: 87.060546875%\n",
            "Best loss: 0.45623788330703974\n",
            "\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 2.9010427684887596\n",
            "test loss: 0.4639187064021826, \t test acc: 87.01171875%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 2.848925507586935\n",
            "test loss: 0.4681076668202877, \t test acc: 86.865234375%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 2.8872183859348297\n",
            "test loss: 0.4616347560659051, \t test acc: 87.01171875%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 2.8917085580203845\n",
            "test loss: 0.45670780166983604, \t test acc: 87.060546875%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 2.8712537068387736\n",
            "test loss: 0.4540973240509629, \t test acc: 87.158203125%\n",
            "Best loss: 0.4540973240509629\n",
            "\n",
            "--------- epoch : 19 ------------\n",
            "train loss: 2.915130095637363\n",
            "test loss: 0.4637933149933815, \t test acc: 86.962890625%\n",
            "--------- epoch : 20 ------------\n",
            "train loss: 2.8896848002205724\n",
            "test loss: 0.4602058744058013, \t test acc: 87.158203125%\n",
            "--------- epoch : 21 ------------\n",
            "train loss: 2.876118675522182\n",
            "test loss: 0.46188982389867306, \t test acc: 87.158203125%\n",
            "--------- epoch : 22 ------------\n",
            "train loss: 2.8964512672113334\n",
            "test loss: 0.46184142865240574, \t test acc: 87.01171875%\n",
            "--------- epoch : 23 ------------\n",
            "train loss: 2.8853754880635636\n",
            "test loss: 0.45964513067156076, \t test acc: 87.060546875%\n",
            "--------- epoch : 24 ------------\n",
            "train loss: 2.873109316048415\n",
            "test loss: 0.4626677678897977, \t test acc: 87.109375%\n",
            "--------- epoch : 25 ------------\n",
            "train loss: 2.886128218277641\n",
            "test loss: 0.46337062306702137, \t test acc: 87.01171875%\n",
            "--------- epoch : 26 ------------\n",
            "train loss: 2.8943239800308063\n",
            "test loss: 0.4602696383371949, \t test acc: 87.255859375%\n",
            "early stop at 26 epoch\n",
            "best loss: 0.4540973240509629,\t best acc: 87.158203125%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.028136217546568292\n",
            "test loss: 0.45634618774056435, \t test acc: 87.158203125%\n",
            "Best loss: 0.45634618774056435\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.027290235420324556\n",
            "test loss: 0.461449614726007, \t test acc: 87.01171875%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.027391320985296497\n",
            "test loss: 0.4648944968357682, \t test acc: 87.01171875%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.027261057615523106\n",
            "test loss: 0.45967184007167816, \t test acc: 86.962890625%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.02735631890675944\n",
            "test loss: 0.46403722558170557, \t test acc: 86.81640625%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.028157634392340224\n",
            "test loss: 0.45454666670411825, \t test acc: 87.109375%\n",
            "Best loss: 0.45454666670411825\n",
            "\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.027615103713722656\n",
            "test loss: 0.4663677681237459, \t test acc: 87.01171875%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.027882231753724424\n",
            "test loss: 0.47325337305665016, \t test acc: 86.767578125%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.027451463428366442\n",
            "test loss: 0.4580311616882682, \t test acc: 87.109375%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.027734839154974274\n",
            "test loss: 0.4646318480372429, \t test acc: 87.060546875%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.02813808023220981\n",
            "test loss: 0.45957085117697716, \t test acc: 87.109375%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.026978064880139478\n",
            "test loss: 0.46809560526162386, \t test acc: 87.01171875%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.026784582134417218\n",
            "test loss: 0.46258728485554457, \t test acc: 87.158203125%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.02708413281336265\n",
            "test loss: 0.4621297651901841, \t test acc: 86.865234375%\n",
            "early stop at 14 epoch\n",
            "best loss: 0.45454666670411825,\t best acc: 87.109375%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.0592378794984973\n",
            "test loss: 0.4627331290394068, \t test acc: 87.109375%\n",
            "Best loss: 0.4627331290394068\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.058165572077521814\n",
            "test loss: 0.46283289697021246, \t test acc: 87.158203125%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.0587922761257252\n",
            "test loss: 0.4672981621697545, \t test acc: 87.01171875%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.05769178915120985\n",
            "test loss: 0.45349940937012434, \t test acc: 87.158203125%\n",
            "Best loss: 0.45349940937012434\n",
            "\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.05870467596484915\n",
            "test loss: 0.462946112267673, \t test acc: 87.01171875%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.05891543703720621\n",
            "test loss: 0.4484583642333746, \t test acc: 87.20703125%\n",
            "Best loss: 0.4484583642333746\n",
            "\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.05811733578372261\n",
            "test loss: 0.4607537453994155, \t test acc: 87.109375%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.0602246943415831\n",
            "test loss: 0.4620654806494713, \t test acc: 86.962890625%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.058277244274706944\n",
            "test loss: 0.4602611642330885, \t test acc: 87.060546875%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.05870444939026366\n",
            "test loss: 0.4592884173616767, \t test acc: 87.20703125%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.05855138460174203\n",
            "test loss: 0.45499664079397917, \t test acc: 87.158203125%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.059076975443926844\n",
            "test loss: 0.4502981239929795, \t test acc: 87.060546875%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.05770962462639031\n",
            "test loss: 0.4654782172292471, \t test acc: 87.109375%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.058747168083715696\n",
            "test loss: 0.4612891972064972, \t test acc: 87.060546875%\n",
            "early stop at 14 epoch\n",
            "best loss: 0.4484583642333746,\t best acc: 87.20703125%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.37060796647616057\n",
            "test loss: 0.459774948656559, \t test acc: 86.962890625%\n",
            "Best loss: 0.459774948656559\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.3691002205014229\n",
            "test loss: 0.4548375941812992, \t test acc: 87.109375%\n",
            "Best loss: 0.4548375941812992\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.3706576114763384\n",
            "test loss: 0.46462422609329224, \t test acc: 86.9140625%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.36617636372861656\n",
            "test loss: 0.46046501118689775, \t test acc: 87.158203125%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.3665231786992239\n",
            "test loss: 0.4595190426334739, \t test acc: 87.158203125%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.36777473872770433\n",
            "test loss: 0.4554812405258417, \t test acc: 87.109375%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.3658570331399855\n",
            "test loss: 0.468324932269752, \t test acc: 86.81640625%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.3734503048269645\n",
            "test loss: 0.46447967272251844, \t test acc: 87.060546875%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.36900997080880665\n",
            "test loss: 0.4563541542738676, \t test acc: 87.158203125%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.3658253521375034\n",
            "test loss: 0.4608440399169922, \t test acc: 86.962890625%\n",
            "early stop at 10 epoch\n",
            "best loss: 0.4548375941812992,\t best acc: 87.109375%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 1.7556647289058436\n",
            "test loss: 0.4651405066251755, \t test acc: 87.060546875%\n",
            "Best loss: 0.4651405066251755\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 1.7520766530347907\n",
            "test loss: 0.4663510052487254, \t test acc: 87.060546875%\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 1.7534978674805684\n",
            "test loss: 0.46534452587366104, \t test acc: 87.060546875%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 1.7475024837514628\n",
            "test loss: 0.46489504911005497, \t test acc: 87.060546875%\n",
            "Best loss: 0.46489504911005497\n",
            "\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 1.7362244848323904\n",
            "test loss: 0.46492538414895535, \t test acc: 86.767578125%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 1.7400073959775593\n",
            "test loss: 0.47785598412156105, \t test acc: 86.767578125%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 1.7466126874737118\n",
            "test loss: 0.456167901866138, \t test acc: 87.20703125%\n",
            "Best loss: 0.456167901866138\n",
            "\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 1.736531045773755\n",
            "test loss: 0.4676244566217065, \t test acc: 87.01171875%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 1.7512858160164044\n",
            "test loss: 0.4602944990620017, \t test acc: 86.962890625%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 1.7600321147752844\n",
            "test loss: 0.45842166151851416, \t test acc: 87.109375%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 1.745164573840473\n",
            "test loss: 0.4608277380466461, \t test acc: 86.962890625%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 1.7534745588250782\n",
            "test loss: 0.45187865663319826, \t test acc: 87.255859375%\n",
            "Best loss: 0.45187865663319826\n",
            "\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 1.7327917043281638\n",
            "test loss: 0.45933337416499853, \t test acc: 87.109375%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 1.7434011652417805\n",
            "test loss: 0.4611962214112282, \t test acc: 87.01171875%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 1.742084494103556\n",
            "test loss: 0.4669214189052582, \t test acc: 86.9140625%\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 1.747411064479662\n",
            "test loss: 0.45870146714150906, \t test acc: 87.109375%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 1.7504919229642204\n",
            "test loss: 0.4678099360316992, \t test acc: 86.669921875%\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 1.7582637207663578\n",
            "test loss: 0.45376033429056406, \t test acc: 87.20703125%\n",
            "--------- epoch : 19 ------------\n",
            "train loss: 1.7448283582925797\n",
            "test loss: 0.4592891903594136, \t test acc: 87.158203125%\n",
            "--------- epoch : 20 ------------\n",
            "train loss: 1.748462710691535\n",
            "test loss: 0.46547263674438, \t test acc: 87.01171875%\n",
            "early stop at 20 epoch\n",
            "best loss: 0.45187865663319826,\t best acc: 87.255859375%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 3.1232271414736044\n",
            "test loss: 0.4630601955577731, \t test acc: 86.962890625%\n",
            "Best loss: 0.4630601955577731\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 3.1121646671191505\n",
            "test loss: 0.46159148775041103, \t test acc: 87.01171875%\n",
            "Best loss: 0.46159148775041103\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 3.1416298954383186\n",
            "test loss: 0.46005669329315424, \t test acc: 87.060546875%\n",
            "Best loss: 0.46005669329315424\n",
            "\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 3.1260927151078763\n",
            "test loss: 0.46404892951250076, \t test acc: 87.158203125%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 3.1020132847454236\n",
            "test loss: 0.45844352897256613, \t test acc: 87.01171875%\n",
            "Best loss: 0.45844352897256613\n",
            "\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 3.1338015146877454\n",
            "test loss: 0.45902280788868666, \t test acc: 87.158203125%\n",
            "--------- epoch : 7 ------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-cfa96621e02f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mALPHA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             loss, acc, history = train_kd(model=student_model,teacher_output=teacher_output,\n\u001b[0m\u001b[1;32m     26\u001b[0m                                   \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                                   \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-427500312b1f>\u001b[0m in \u001b[0;36mtrain_kd\u001b[0;34m(model, teacher_output, train_loader, test_loader, criterion, optimizer, epochs, T, alpha, save_name)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn_kd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda_new/envs/jinbeom/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-b2a216b0eb60>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 128 -> 64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 64 -> 32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 32 -> 16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda_new/envs/jinbeom/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda_new/envs/jinbeom/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda_new/envs/jinbeom/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    417\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 419\u001b[0;31m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[1;32m    420\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLx_SfWKUBtx",
        "outputId": "62fd3fb7-973d-43db-e054-17cf7250d6dc"
      },
      "source": [
        "#ALPHA index [0.001, 0.01, 0.1, 0.5, 0.9]\n",
        "\n",
        "for log in logs:\n",
        "    print(log)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "efficientnet-b0_T2_al0\tloss = 0.35910811088979244, \tacc = 86.669921875\n",
            "efficientnet-b0_T2_al1\tloss = 0.38970668613910675, \tacc = 87.158203125\n",
            "efficientnet-b0_T2_al2\tloss = 0.392495047301054, \tacc = 87.59765625\n",
            "efficientnet-b0_T2_al3\tloss = 0.39769905991852283, \tacc = 87.40234375\n",
            "efficientnet-b0_T2_al4\tloss = 0.4000811204314232, \tacc = 86.81640625\n",
            "efficientnet-b0_T3_al0\tloss = 0.39693687204271555, \tacc = 87.01171875\n",
            "efficientnet-b0_T3_al1\tloss = 0.4000391745939851, \tacc = 87.255859375\n",
            "efficientnet-b0_T3_al2\tloss = 0.4001190410926938, \tacc = 87.060546875\n",
            "efficientnet-b0_T3_al3\tloss = 0.3963844859972596, \tacc = 87.353515625\n",
            "efficientnet-b0_T3_al4\tloss = 0.39185390807688236, \tacc = 87.353515625\n",
            "efficientnet-b0_T4_al0\tloss = 0.4020005688071251, \tacc = 86.865234375\n",
            "efficientnet-b0_T4_al1\tloss = 0.39988416340202093, \tacc = 87.20703125\n",
            "efficientnet-b0_T4_al2\tloss = 0.39767225086688995, \tacc = 87.060546875\n",
            "efficientnet-b0_T4_al3\tloss = 0.397953636944294, \tacc = 87.20703125\n",
            "efficientnet-b0_T4_al4\tloss = 0.3968390142545104, \tacc = 86.9140625\n",
            "efficientnet-b0_T5_al0\tloss = 0.4015455972403288, \tacc = 87.109375\n",
            "efficientnet-b0_T5_al1\tloss = 0.4011897938326001, \tacc = 87.109375\n",
            "efficientnet-b0_T5_al2\tloss = 0.39781627152115107, \tacc = 87.255859375\n",
            "efficientnet-b0_T5_al3\tloss = 0.4010564275085926, \tacc = 87.01171875\n",
            "efficientnet-b0_T5_al4\tloss = 0.40033623203635216, \tacc = 86.9140625\n",
            "efficientnet-b0_T6_al0\tloss = 0.40147185139358044, \tacc = 87.109375\n",
            "efficientnet-b0_T6_al1\tloss = 0.39792263228446245, \tacc = 86.9140625\n",
            "efficientnet-b0_T6_al2\tloss = 0.4005028950050473, \tacc = 86.9140625\n",
            "efficientnet-b0_T6_al3\tloss = 0.4008515290915966, \tacc = 87.060546875\n",
            "efficientnet-b0_T6_al4\tloss = 0.4006687495857477, \tacc = 87.109375\n",
            "efficientnet-b0_T7_al0\tloss = 0.3902491834014654, \tacc = 87.548828125\n",
            "efficientnet-b0_T7_al1\tloss = 0.40122112445533276, \tacc = 86.962890625\n",
            "efficientnet-b0_T7_al2\tloss = 0.40189036540687084, \tacc = 86.865234375\n",
            "efficientnet-b0_T7_al3\tloss = 0.39965679310262203, \tacc = 87.158203125\n",
            "efficientnet-b0_T7_al4\tloss = 0.40167182590812445, \tacc = 87.40234375\n",
            "efficientnet-b1_T2_al0\tloss = 0.3728559482842684, \tacc = 86.71875\n",
            "efficientnet-b1_T2_al1\tloss = 0.4759277179837227, \tacc = 87.060546875\n",
            "efficientnet-b1_T2_al2\tloss = 0.45723775681108236, \tacc = 87.353515625\n",
            "efficientnet-b1_T2_al3\tloss = 0.471113090403378, \tacc = 87.158203125\n",
            "efficientnet-b1_T2_al4\tloss = 0.5033654924482107, \tacc = 86.23046875\n",
            "efficientnet-b1_T3_al0\tloss = 0.5065758237615228, \tacc = 86.083984375\n",
            "efficientnet-b1_T3_al1\tloss = 0.5052191093564034, \tacc = 86.279296875\n",
            "efficientnet-b1_T3_al2\tloss = 0.5055232113227248, \tacc = 86.279296875\n",
            "efficientnet-b1_T3_al3\tloss = 0.4971321392804384, \tacc = 86.279296875\n",
            "efficientnet-b1_T3_al4\tloss = 0.5064549325034022, \tacc = 86.279296875\n",
            "efficientnet-b1_T4_al0\tloss = 0.5018121376633644, \tacc = 86.03515625\n",
            "efficientnet-b1_T4_al1\tloss = 0.5037682540714741, \tacc = 86.083984375\n",
            "efficientnet-b1_T4_al2\tloss = 0.5029212050139904, \tacc = 86.03515625\n",
            "efficientnet-b1_T4_al3\tloss = 0.5014261417090893, \tacc = 86.23046875\n",
            "efficientnet-b1_T4_al4\tloss = 0.5001390911638737, \tacc = 86.083984375\n",
            "efficientnet-b1_T5_al0\tloss = 0.5005932580679655, \tacc = 86.181640625\n",
            "efficientnet-b1_T5_al1\tloss = 0.5023473836481571, \tacc = 86.1328125\n",
            "efficientnet-b1_T5_al2\tloss = 0.4942213296890259, \tacc = 86.328125\n",
            "efficientnet-b1_T5_al3\tloss = 0.4971311893314123, \tacc = 86.328125\n",
            "efficientnet-b1_T5_al4\tloss = 0.5005646329373121, \tacc = 85.986328125\n",
            "efficientnet-b1_T6_al0\tloss = 0.5022435113787651, \tacc = 86.181640625\n",
            "efficientnet-b1_T6_al1\tloss = 0.4997695880010724, \tacc = 86.279296875\n",
            "efficientnet-b1_T6_al2\tloss = 0.5015713293105364, \tacc = 86.1328125\n",
            "efficientnet-b1_T6_al3\tloss = 0.49668285995721817, \tacc = 86.376953125\n",
            "efficientnet-b1_T6_al4\tloss = 0.4993617879226804, \tacc = 86.1328125\n",
            "efficientnet-b1_T7_al0\tloss = 0.49865798465907574, \tacc = 86.181640625\n",
            "efficientnet-b1_T7_al1\tloss = 0.4973055459558964, \tacc = 86.328125\n",
            "efficientnet-b1_T7_al2\tloss = 0.49609846621751785, \tacc = 86.279296875\n",
            "efficientnet-b1_T7_al3\tloss = 0.49338481202721596, \tacc = 86.181640625\n",
            "efficientnet-b1_T7_al4\tloss = 0.49456011317670345, \tacc = 86.181640625\n",
            "efficientnet-b4_T2_al0\tloss = 0.4077972797676921, \tacc = 87.109375\n",
            "efficientnet-b4_T2_al1\tloss = 0.4435715125873685, \tacc = 87.353515625\n",
            "efficientnet-b4_T2_al2\tloss = 0.4476786730811, \tacc = 87.109375\n",
            "efficientnet-b4_T2_al3\tloss = 0.44794860389083624, \tacc = 87.109375\n",
            "efficientnet-b4_T2_al4\tloss = 0.4511764030903578, \tacc = 87.353515625\n",
            "efficientnet-b4_T3_al0\tloss = 0.4516192525625229, \tacc = 87.060546875\n",
            "efficientnet-b4_T3_al1\tloss = 0.45405644457787275, \tacc = 87.20703125\n",
            "efficientnet-b4_T3_al2\tloss = 0.4536990746855736, \tacc = 87.109375\n",
            "efficientnet-b4_T3_al3\tloss = 0.45127107296139, \tacc = 87.158203125\n",
            "efficientnet-b4_T3_al4\tloss = 0.45149696711450815, \tacc = 87.158203125\n",
            "efficientnet-b4_T4_al0\tloss = 0.4548168061301112, \tacc = 87.255859375\n",
            "efficientnet-b4_T4_al1\tloss = 0.45052017644047737, \tacc = 87.158203125\n",
            "efficientnet-b4_T4_al2\tloss = 0.45127172768116, \tacc = 87.3046875\n",
            "efficientnet-b4_T4_al3\tloss = 0.44454860873520374, \tacc = 87.20703125\n",
            "efficientnet-b4_T4_al4\tloss = 0.4540973240509629, \tacc = 87.158203125\n",
            "efficientnet-b4_T5_al0\tloss = 0.45454666670411825, \tacc = 87.109375\n",
            "efficientnet-b4_T5_al1\tloss = 0.4484583642333746, \tacc = 87.20703125\n",
            "efficientnet-b4_T5_al2\tloss = 0.4548375941812992, \tacc = 87.109375\n",
            "efficientnet-b4_T5_al3\tloss = 0.45187865663319826, \tacc = 87.255859375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbIY6OlwUBtx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}