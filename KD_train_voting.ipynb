{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "ML",
      "language": "python",
      "name": "jinbeom"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "KD_train-voting.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEhvMd1cTur_"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from glob import glob\n",
        "import os\n",
        "import sys\n",
        "from PIL import Image\n",
        "import timm\n",
        "import pandas as pd\n",
        "\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzavteXQTusD"
      },
      "source": [
        "# KD train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgx42sHKTusE"
      },
      "source": [
        "def loss_fn_kd(outputs, labels, teacher_outputs, T, alpha):\n",
        "    KD_loss = nn.KLDivLoss()(F.log_softmax(outputs/T, dim=1), \n",
        "                             F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T) + \\\n",
        "                             F.cross_entropy(outputs, labels) * (1. - alpha)\n",
        "\n",
        "    return KD_loss\n",
        "\n",
        "def get_teacher_output(model, loader):\n",
        "    model.eval()\n",
        "    output = []\n",
        "    with torch.no_grad():\n",
        "        for data, _ in loader:\n",
        "            data = data.to(device)\n",
        "            output.append(model(data))\n",
        "    torch.cuda.empty_cache()\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhKJ0u-nTusE"
      },
      "source": [
        "def train_kd_voting(model, teacher_output, train_loader, test_loader, criterion, \n",
        "             optimizer, epochs, T, alpha, save_name, n_teacher):\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
        "                                                          patience=3, verbose=True)\n",
        "    best_loss = None\n",
        "    best_acc = None\n",
        "    patience = 0\n",
        "\n",
        "    history = {'loss': [], 'acc': []}\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        print(\"--------- epoch : {} ------------\".format(epoch+1))\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for i, (data, label) in enumerate(train_loader):\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "            \n",
        "            t_output = teacher_output[0][i]\n",
        "            for j in range(1, n_teacher):\n",
        "                t_output += teacher_output[j][i]\n",
        "            t_output /= n_teacher\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = loss_fn_kd(output, label, t_output, T, alpha)\n",
        "            train_losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            torch.cuda.empty_cache()\n",
        "            \n",
        "        train_loss = np.average(train_losses)\n",
        "        print(\"train loss: {}\".format(train_loss))\n",
        "        \n",
        "        \n",
        "        model.eval()\n",
        "        test_losses = []\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for i, (data, label) in enumerate(test_loader):\n",
        "                data = data.to(device)\n",
        "                label = label.to(device)\n",
        "\n",
        "                output = model(data)\n",
        "                loss = criterion(output, label)\n",
        "                test_losses.append(loss.item())\n",
        "                _, predict = torch.max(output.data, 1)\n",
        "                correct += (predict == label).sum().item()\n",
        "                total += label.size(0)\n",
        "                \n",
        "        test_loss = np.average(test_losses)\n",
        "        test_acc = 100 * correct / total\n",
        "        print(\"test loss: {}, \\t test acc: {}%\".format(test_loss, test_acc))\n",
        "\n",
        "        history['loss'].append(test_loss)\n",
        "        history['acc'].append(test_acc)\n",
        "        \n",
        "        if (best_loss is None) or (best_loss > test_loss):\n",
        "            best_loss = test_loss\n",
        "            best_acc = test_acc\n",
        "            torch.save(model.state_dict(), 'new_model_weights/'+ save_name +'.pth')\n",
        "            print('Best loss: {}\\n'.format(best_loss))\n",
        "            patience = 0\n",
        "        else:\n",
        "            patience += 1\n",
        "        \n",
        "        if patience > 7:\n",
        "            print(\"early stop at {} epoch\".format(epoch + 1))\n",
        "            break\n",
        "            \n",
        "        scheduler.step(metrics=test_loss)\n",
        "   \n",
        "    print(\"best loss: {},\\t best acc: {}%\\n\\n\".format(best_loss, best_acc))\n",
        "    return best_loss, best_acc, history        \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w90DyuMrTusF"
      },
      "source": [
        "# Custom Datasaet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSJhpYxHTusF"
      },
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, input_size, train = True, padding = True, normalize = False,\n",
        "                 bright_ness = 0.2, hue = 0.15, contrast = 0.15, random_Hflip = 0.3, rotate_deg = 20):\n",
        "        orig_normal_path = glob(os.path.join(root_dir, 'normal') + '/*.jpg')\n",
        "        orig_fall_path = glob(os.path.join(root_dir, 'falldown') + '/*.jpg')\n",
        "        orig_back_path = glob(os.path.join(root_dir, 'background') + '/*.jpg')\n",
        "        \n",
        "        normal_paths = []\n",
        "        fall_paths = []\n",
        "        back_paths = []\n",
        "        \n",
        "        for path in orig_normal_path:\n",
        "            img = Image.open(path)\n",
        "            if min(img.size[0], img.size[1]) < 32:\n",
        "                pass\n",
        "            else:\n",
        "                normal_paths.append(path)\n",
        "                \n",
        "        for path in orig_fall_path:\n",
        "            img = Image.open(path)\n",
        "            if min(img.size[0], img.size[1]) < 32:\n",
        "                pass\n",
        "            else:\n",
        "                fall_paths.append(path)\n",
        "                \n",
        "        for path in orig_back_path:\n",
        "            img = Image.open(path)\n",
        "            if min(img.size[0], img.size[1]) < 32:\n",
        "                pass\n",
        "            else:\n",
        "                back_paths.append(path)\n",
        "                \n",
        "        self.total_paths = normal_paths + fall_paths + back_paths\n",
        "        self.labels = [0] * len(normal_paths) + [1] * len(fall_paths) + [2] * len(back_paths)\n",
        "        \n",
        "        transform = []\n",
        "        if train:\n",
        "            #transform.append(torchvision.transforms.ColorJitter(brightness=bright_ness, hue=hue, contrast=contrast))\n",
        "            transform.append(torchvision.transforms.RandomHorizontalFlip(p=random_Hflip))\n",
        "            #transform.append(torchvision.transforms.RandomCrop(224))\n",
        "            transform.append(torchvision.transforms.RandomRotation(degrees=rotate_deg))\n",
        "        transform.append(torchvision.transforms.ToTensor())\n",
        "        if normalize:\n",
        "            transform.append(torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
        "        if padding:\n",
        "            transform.append(lambda x: torchvision.transforms.Pad(((128 - x.shape[2]) // 2, (128 - x.shape[1]) // 2), fill=0,\n",
        "                                                     padding_mode=\"constant\")(x))\n",
        "        transform.append(torchvision.transforms.Resize((input_size, input_size)))\n",
        "        self.transform = torchvision.transforms.Compose(transform)\n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.total_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(self.total_paths[index])\n",
        "        img = self.transform(img)\n",
        "        return img, self.labels[index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpQ892RgTusG"
      },
      "source": [
        "# student model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txtQORZbTusG"
      },
      "source": [
        "class CNN_layers(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN_layers, self).__init__()      \n",
        "        self.conv1 = nn.Conv2d(3, 16, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
        "        self.conv4 = nn.Conv2d(64, 32, 3)\n",
        "        #self.conv5 = nn.Conv2d(32, 16, 3)\n",
        "\n",
        "        self.fc1 = nn.Linear(32 * 8 * 8, 16)\n",
        "        self.fc2 = nn.Linear(16, 8)\n",
        "        self.fc3 = nn.Linear(8, 3)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.bn4 = nn.BatchNorm2d(32)\n",
        "        #self.bn5 = nn.BatchNorm2d(16)\n",
        "\n",
        "        self.bn6 = nn.BatchNorm1d(16)\n",
        "        self.bn7 = nn.BatchNorm1d(8)\n",
        "        self.padding = nn.ZeroPad2d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(self.padding(x))))) # 128 -> 64\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(self.padding(x))))) # 64 -> 32\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(self.padding(x))))) # 32 -> 16\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(self.padding(x))))) # 16 -> 8\n",
        "        #x = self.pool(F.relu(self.bn5(self.conv5(self.padding(x))))) # 8 -> 4\n",
        "\n",
        "        x = x.view(-1, 32 * 8 * 8)\n",
        "        x = F.relu(self.bn6(self.fc1(x)))\n",
        "        x = F.relu(self.bn7(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU0iVnHWTusH"
      },
      "source": [
        "# create data loader "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RFRgEOKTusH"
      },
      "source": [
        "INPUT_SIZE = 128\n",
        "PADDING = False\n",
        "NORMALIZE = False\n",
        "BATCHSIZE = 128\n",
        "NUMEPOCH = 100\n",
        "\n",
        "train_data = CustomDataset(\n",
        "    root_dir='train',\n",
        "    input_size=INPUT_SIZE, train=True, padding=PADDING, normalize=NORMALIZE,\n",
        "    bright_ness=0, hue=01.5, contrast=0.15, random_Hflip=0, rotate_deg=0)\n",
        "\n",
        "test_data = CustomDataset(\n",
        "    root_dir='validation',\n",
        "    input_size=INPUT_SIZE, train=False, padding=PADDING, normalize=NORMALIZE,\n",
        "    bright_ness=0, hue=01.5, contrast=0.15, random_Hflip=0, rotate_deg=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bHnI2DqTusH"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCHSIZE, shuffle=True, num_workers=32, drop_last=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCHSIZE, shuffle=False, num_workers=32, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNKd4p77TusI"
      },
      "source": [
        "# train student model by KD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQvFOOqFTusI",
        "outputId": "355ae996-0b55-43fb-e206-0b89e678e6a1"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "teachers = ['b0', 'b2']\n",
        "\n",
        "logs = []\n",
        "teacher_output = []\n",
        "for teacher_name in teachers:\n",
        "    model_name = 'efficientnet-' + teacher_name\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    teacher_model = EfficientNet.from_pretrained(model_name, num_classes=3).to(device)\n",
        "    teacher_model.load_state_dict(torch.load('new_model_weights/'+ model_name + '.pth'))\n",
        "\n",
        "    teacher_output.append(get_teacher_output(teacher_model, train_loader))\n",
        "    \n",
        "torch.cuda.empty_cache()\n",
        "student_model = CNN_layers().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(student_model.parameters(), weight_decay=1e-4, lr=0.00001)\n",
        "save_name = 'kd_voting'\n",
        "for teacher_name in teachers:\n",
        "    save_name += '-' + teacher_name\n",
        "\n",
        "T = [2, 4, 6]\n",
        "ALPHA = [0.01, 0.1, 0.5]\n",
        "    \n",
        "\n",
        "for t in T:\n",
        "    for j, alpha in enumerate(ALPHA):\n",
        "        loss, acc, history = train_kd_voting(model=student_model,teacher_output=teacher_output,\n",
        "                              train_loader=train_loader, test_loader=test_loader, \n",
        "                              criterion = criterion, optimizer=optimizer, \n",
        "                              epochs=NUMEPOCH, T=t, alpha=alpha, save_name=save_name, n_teacher=len(teachers)) \n",
        "\n",
        "        s = save_name +'_T{}_al{}\\tloss = {}, \\tacc = {}'.format(t, j, loss, acc)\n",
        "        logs.append(s)\n",
        "        \n",
        "        df = pd.DataFrame(history)\n",
        "        his_name = save_name + '_T{}_al{}'.format(t, alpha)\n",
        "        df.to_csv(\"new_history/\"+ his_name+ \"_history.csv\", mode='w')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n",
            "Loaded pretrained weights for efficientnet-b2\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.9767207717118056\n",
            "test loss: 0.8883654922246933, \t test acc: 71.97265625%\n",
            "Best loss: 0.8883654922246933\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.9002956313931424\n",
            "test loss: 0.8283996768295765, \t test acc: 76.123046875%\n",
            "Best loss: 0.8283996768295765\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.860166717806588\n",
            "test loss: 0.7992533631622791, \t test acc: 78.7109375%\n",
            "Best loss: 0.7992533631622791\n",
            "\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.8328628271170284\n",
            "test loss: 0.7827466242015362, \t test acc: 80.517578125%\n",
            "Best loss: 0.7827466242015362\n",
            "\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.8115649705995684\n",
            "test loss: 0.7639840953052044, \t test acc: 81.689453125%\n",
            "Best loss: 0.7639840953052044\n",
            "\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.7949110480106395\n",
            "test loss: 0.7446673773229122, \t test acc: 82.32421875%\n",
            "Best loss: 0.7446673773229122\n",
            "\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.7803292578977087\n",
            "test loss: 0.7321072276681662, \t test acc: 82.91015625%\n",
            "Best loss: 0.7321072276681662\n",
            "\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.7677689797204473\n",
            "test loss: 0.7303378973156214, \t test acc: 82.32421875%\n",
            "Best loss: 0.7303378973156214\n",
            "\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.7558555130077445\n",
            "test loss: 0.7238756958395243, \t test acc: 83.837890625%\n",
            "Best loss: 0.7238756958395243\n",
            "\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.7460638767351275\n",
            "test loss: 0.7149585895240307, \t test acc: 83.69140625%\n",
            "Best loss: 0.7149585895240307\n",
            "\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.7362301790195963\n",
            "test loss: 0.6994072571396828, \t test acc: 82.8125%\n",
            "Best loss: 0.6994072571396828\n",
            "\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.7279204836358195\n",
            "test loss: 0.7008237633854151, \t test acc: 84.033203125%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.7179477399458056\n",
            "test loss: 0.6952357478439808, \t test acc: 83.935546875%\n",
            "Best loss: 0.6952357478439808\n",
            "\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.7097707896128945\n",
            "test loss: 0.6943580098450184, \t test acc: 84.130859375%\n",
            "Best loss: 0.6943580098450184\n",
            "\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.700516310075055\n",
            "test loss: 0.68565515242517, \t test acc: 83.935546875%\n",
            "Best loss: 0.68565515242517\n",
            "\n",
            "--------- epoch : 16 ------------\n",
            "train loss: 0.6930007928091547\n",
            "test loss: 0.6861268561333418, \t test acc: 84.716796875%\n",
            "--------- epoch : 17 ------------\n",
            "train loss: 0.6843826582898265\n",
            "test loss: 0.6781535837799311, \t test acc: 84.27734375%\n",
            "Best loss: 0.6781535837799311\n",
            "\n",
            "--------- epoch : 18 ------------\n",
            "train loss: 0.6764054282203965\n",
            "test loss: 0.6692815311253071, \t test acc: 84.228515625%\n",
            "Best loss: 0.6692815311253071\n",
            "\n",
            "--------- epoch : 19 ------------\n",
            "train loss: 0.6682186751909878\n",
            "test loss: 0.6622949913144112, \t test acc: 84.765625%\n",
            "Best loss: 0.6622949913144112\n",
            "\n",
            "--------- epoch : 20 ------------\n",
            "train loss: 0.6608337529975435\n",
            "test loss: 0.6556672398000956, \t test acc: 84.375%\n",
            "Best loss: 0.6556672398000956\n",
            "\n",
            "--------- epoch : 21 ------------\n",
            "train loss: 0.6530812820014746\n",
            "test loss: 0.6579447537660599, \t test acc: 84.86328125%\n",
            "--------- epoch : 22 ------------\n",
            "train loss: 0.6450934960790302\n",
            "test loss: 0.6527437977492809, \t test acc: 85.400390625%\n",
            "Best loss: 0.6527437977492809\n",
            "\n",
            "--------- epoch : 23 ------------\n",
            "train loss: 0.6358483025561208\n",
            "test loss: 0.6448956280946732, \t test acc: 83.837890625%\n",
            "Best loss: 0.6448956280946732\n",
            "\n",
            "--------- epoch : 24 ------------\n",
            "train loss: 0.6305531130536742\n",
            "test loss: 0.6439423877745867, \t test acc: 84.9609375%\n",
            "Best loss: 0.6439423877745867\n",
            "\n",
            "--------- epoch : 25 ------------\n",
            "train loss: 0.6229013353586197\n",
            "test loss: 0.637248981744051, \t test acc: 85.205078125%\n",
            "Best loss: 0.637248981744051\n",
            "\n",
            "--------- epoch : 26 ------------\n",
            "train loss: 0.6148786914089451\n",
            "test loss: 0.6335225868970156, \t test acc: 85.205078125%\n",
            "Best loss: 0.6335225868970156\n",
            "\n",
            "--------- epoch : 27 ------------\n",
            "train loss: 0.6087299876886866\n",
            "test loss: 0.6322382539510727, \t test acc: 85.9375%\n",
            "Best loss: 0.6322382539510727\n",
            "\n",
            "--------- epoch : 28 ------------\n",
            "train loss: 0.6002964843874392\n",
            "test loss: 0.6339178718626499, \t test acc: 84.912109375%\n",
            "--------- epoch : 29 ------------\n",
            "train loss: 0.5928821087531422\n",
            "test loss: 0.6196222025901079, \t test acc: 84.814453125%\n",
            "Best loss: 0.6196222025901079\n",
            "\n",
            "--------- epoch : 30 ------------\n",
            "train loss: 0.5866454703652341\n",
            "test loss: 0.6164824049919844, \t test acc: 85.302734375%\n",
            "Best loss: 0.6164824049919844\n",
            "\n",
            "--------- epoch : 31 ------------\n",
            "train loss: 0.5784641928646875\n",
            "test loss: 0.6128598302602768, \t test acc: 86.1328125%\n",
            "Best loss: 0.6128598302602768\n",
            "\n",
            "--------- epoch : 32 ------------\n",
            "train loss: 0.572334857414598\n",
            "test loss: 0.6103065647184849, \t test acc: 86.1328125%\n",
            "Best loss: 0.6103065647184849\n",
            "\n",
            "--------- epoch : 33 ------------\n",
            "train loss: 0.5646479651331902\n",
            "test loss: 0.6066569741815329, \t test acc: 85.44921875%\n",
            "Best loss: 0.6066569741815329\n",
            "\n",
            "--------- epoch : 34 ------------\n",
            "train loss: 0.5586463122264199\n",
            "test loss: 0.6089991591870785, \t test acc: 86.083984375%\n",
            "--------- epoch : 35 ------------\n",
            "train loss: 0.5510774886478549\n",
            "test loss: 0.6095540262758732, \t test acc: 86.083984375%\n",
            "--------- epoch : 36 ------------\n",
            "train loss: 0.5441284302784049\n",
            "test loss: 0.6017297562211752, \t test acc: 86.572265625%\n",
            "Best loss: 0.6017297562211752\n",
            "\n",
            "--------- epoch : 37 ------------\n",
            "train loss: 0.5365230614400428\n",
            "test loss: 0.5951075162738562, \t test acc: 86.03515625%\n",
            "Best loss: 0.5951075162738562\n",
            "\n",
            "--------- epoch : 38 ------------\n",
            "train loss: 0.5314890536601129\n",
            "test loss: 0.5921058412641287, \t test acc: 86.328125%\n",
            "Best loss: 0.5921058412641287\n",
            "\n",
            "--------- epoch : 39 ------------\n",
            "train loss: 0.5241772534082765\n",
            "test loss: 0.5861101988703012, \t test acc: 86.328125%\n",
            "Best loss: 0.5861101988703012\n",
            "\n",
            "--------- epoch : 40 ------------\n",
            "train loss: 0.5175163000822067\n",
            "test loss: 0.5840324833989143, \t test acc: 86.42578125%\n",
            "Best loss: 0.5840324833989143\n",
            "\n",
            "--------- epoch : 41 ------------\n",
            "train loss: 0.5108413122918295\n",
            "test loss: 0.5858294665813446, \t test acc: 86.376953125%\n",
            "--------- epoch : 42 ------------\n",
            "train loss: 0.5036893197051857\n",
            "test loss: 0.5857150182127953, \t test acc: 87.109375%\n",
            "--------- epoch : 43 ------------\n",
            "train loss: 0.4971454308732696\n",
            "test loss: 0.5783546902239323, \t test acc: 86.62109375%\n",
            "Best loss: 0.5783546902239323\n",
            "\n",
            "--------- epoch : 44 ------------\n",
            "train loss: 0.49164767093632533\n",
            "test loss: 0.5708235558122396, \t test acc: 87.255859375%\n",
            "Best loss: 0.5708235558122396\n",
            "\n",
            "--------- epoch : 45 ------------\n",
            "train loss: 0.48570385510506836\n",
            "test loss: 0.5812502931803465, \t test acc: 85.986328125%\n",
            "--------- epoch : 46 ------------\n",
            "train loss: 0.4793010658544043\n",
            "test loss: 0.5661605075001717, \t test acc: 86.669921875%\n",
            "Best loss: 0.5661605075001717\n",
            "\n",
            "--------- epoch : 47 ------------\n",
            "train loss: 0.47379784043068474\n",
            "test loss: 0.5642700027674437, \t test acc: 86.81640625%\n",
            "Best loss: 0.5642700027674437\n",
            "\n",
            "--------- epoch : 48 ------------\n",
            "train loss: 0.4668068438768387\n",
            "test loss: 0.5567957423627377, \t test acc: 87.20703125%\n",
            "Best loss: 0.5567957423627377\n",
            "\n",
            "--------- epoch : 49 ------------\n",
            "train loss: 0.46055485745487007\n",
            "test loss: 0.5568675082176924, \t test acc: 86.279296875%\n",
            "--------- epoch : 50 ------------\n",
            "train loss: 0.45409843973491504\n",
            "test loss: 0.5545253269374371, \t test acc: 87.01171875%\n",
            "Best loss: 0.5545253269374371\n",
            "\n",
            "--------- epoch : 51 ------------\n",
            "train loss: 0.44793443074044975\n",
            "test loss: 0.5577008277177811, \t test acc: 86.1328125%\n",
            "--------- epoch : 52 ------------\n",
            "train loss: 0.44214968205146166\n",
            "test loss: 0.5461839810013771, \t test acc: 87.01171875%\n",
            "Best loss: 0.5461839810013771\n",
            "\n",
            "--------- epoch : 53 ------------\n",
            "train loss: 0.4352786230004352\n",
            "test loss: 0.5469959136098623, \t test acc: 86.1328125%\n",
            "--------- epoch : 54 ------------\n",
            "train loss: 0.42874250512408174\n",
            "test loss: 0.549689831212163, \t test acc: 86.474609375%\n",
            "--------- epoch : 55 ------------\n",
            "train loss: 0.4230130701933218\n",
            "test loss: 0.548318175598979, \t test acc: 87.158203125%\n",
            "--------- epoch : 56 ------------\n",
            "train loss: 0.41838159233979555\n",
            "test loss: 0.5445787496864796, \t test acc: 85.693359375%\n",
            "Best loss: 0.5445787496864796\n",
            "\n",
            "--------- epoch : 57 ------------\n",
            "train loss: 0.4114385339552942\n",
            "test loss: 0.5386170223355293, \t test acc: 86.181640625%\n",
            "Best loss: 0.5386170223355293\n",
            "\n",
            "--------- epoch : 58 ------------\n",
            "train loss: 0.4059795170374539\n",
            "test loss: 0.5470521356910467, \t test acc: 85.986328125%\n",
            "--------- epoch : 59 ------------\n",
            "train loss: 0.40125708800295123\n",
            "test loss: 0.5334531851112843, \t test acc: 86.083984375%\n",
            "Best loss: 0.5334531851112843\n",
            "\n",
            "--------- epoch : 60 ------------\n",
            "train loss: 0.395836531791998\n",
            "test loss: 0.5319667533040047, \t test acc: 86.328125%\n",
            "Best loss: 0.5319667533040047\n",
            "\n",
            "--------- epoch : 61 ------------\n",
            "train loss: 0.3889735763811547\n",
            "test loss: 0.5268250163644552, \t test acc: 86.42578125%\n",
            "Best loss: 0.5268250163644552\n",
            "\n",
            "--------- epoch : 62 ------------\n",
            "train loss: 0.3846365491981092\n",
            "test loss: 0.5125647727400064, \t test acc: 87.6953125%\n",
            "Best loss: 0.5125647727400064\n",
            "\n",
            "--------- epoch : 63 ------------\n",
            "train loss: 0.3782080243463102\n",
            "test loss: 0.5205403976142406, \t test acc: 87.109375%\n",
            "--------- epoch : 64 ------------\n",
            "train loss: 0.3738729058076506\n",
            "test loss: 0.5182594619691372, \t test acc: 87.3046875%\n",
            "--------- epoch : 65 ------------\n",
            "train loss: 0.3675757256215033\n",
            "test loss: 0.5169481951743364, \t test acc: 86.962890625%\n",
            "--------- epoch : 66 ------------\n",
            "train loss: 0.3625185011845568\n",
            "test loss: 0.5066640237346292, \t test acc: 87.40234375%\n",
            "Best loss: 0.5066640237346292\n",
            "\n",
            "--------- epoch : 67 ------------\n",
            "train loss: 0.3576701717532199\n",
            "test loss: 0.5114266909658909, \t test acc: 87.3046875%\n",
            "--------- epoch : 68 ------------\n",
            "train loss: 0.3518849076784175\n",
            "test loss: 0.5210392260923982, \t test acc: 86.181640625%\n",
            "--------- epoch : 69 ------------\n",
            "train loss: 0.34727334765636403\n",
            "test loss: 0.5194881893694401, \t test acc: 86.962890625%\n",
            "--------- epoch : 70 ------------\n",
            "train loss: 0.34161285813088005\n",
            "test loss: 0.5016501378268003, \t test acc: 87.5%\n",
            "Best loss: 0.5016501378268003\n",
            "\n",
            "--------- epoch : 71 ------------\n",
            "train loss: 0.3369122902336328\n",
            "test loss: 0.5050859581679106, \t test acc: 86.81640625%\n",
            "--------- epoch : 72 ------------\n",
            "train loss: 0.3318940697480803\n",
            "test loss: 0.51583481580019, \t test acc: 87.40234375%\n",
            "--------- epoch : 73 ------------\n",
            "train loss: 0.3276874951046446\n",
            "test loss: 0.5007649324834347, \t test acc: 87.40234375%\n",
            "Best loss: 0.5007649324834347\n",
            "\n",
            "--------- epoch : 74 ------------\n",
            "train loss: 0.3219277743736039\n",
            "test loss: 0.4964765440672636, \t test acc: 87.646484375%\n",
            "Best loss: 0.4964765440672636\n",
            "\n",
            "--------- epoch : 75 ------------\n",
            "train loss: 0.3182443213527617\n",
            "test loss: 0.4963315576314926, \t test acc: 87.109375%\n",
            "Best loss: 0.4963315576314926\n",
            "\n",
            "--------- epoch : 76 ------------\n",
            "train loss: 0.3126568089684714\n",
            "test loss: 0.49283885303884745, \t test acc: 87.255859375%\n",
            "Best loss: 0.49283885303884745\n",
            "\n",
            "--------- epoch : 77 ------------\n",
            "train loss: 0.307182471551325\n",
            "test loss: 0.49998993054032326, \t test acc: 87.01171875%\n",
            "--------- epoch : 78 ------------\n",
            "train loss: 0.30324703001457715\n",
            "test loss: 0.47799471765756607, \t test acc: 87.890625%\n",
            "Best loss: 0.47799471765756607\n",
            "\n",
            "--------- epoch : 79 ------------\n",
            "train loss: 0.2999641953603081\n",
            "test loss: 0.49427352752536535, \t test acc: 87.060546875%\n",
            "--------- epoch : 80 ------------\n",
            "train loss: 0.2952308870204117\n",
            "test loss: 0.4838247876614332, \t test acc: 87.548828125%\n",
            "--------- epoch : 81 ------------\n",
            "train loss: 0.29060108133632206\n",
            "test loss: 0.48182328324764967, \t test acc: 87.20703125%\n",
            "--------- epoch : 82 ------------\n",
            "train loss: 0.28565725991907326\n",
            "test loss: 0.49653871450573206, \t test acc: 86.572265625%\n",
            "Epoch    82: reducing learning rate of group 0 to 1.0000e-06.\n",
            "--------- epoch : 83 ------------\n",
            "train loss: 0.2813719560432693\n",
            "test loss: 0.4776993887498975, \t test acc: 87.79296875%\n",
            "Best loss: 0.4776993887498975\n",
            "\n",
            "--------- epoch : 84 ------------\n",
            "train loss: 0.28139331880147045\n",
            "test loss: 0.47406396083533764, \t test acc: 87.744140625%\n",
            "Best loss: 0.47406396083533764\n",
            "\n",
            "--------- epoch : 85 ------------\n",
            "train loss: 0.2820720343000215\n",
            "test loss: 0.48199363704770803, \t test acc: 87.646484375%\n",
            "--------- epoch : 86 ------------\n",
            "train loss: 0.28035960638004803\n",
            "test loss: 0.47791718505322933, \t test acc: 87.548828125%\n",
            "--------- epoch : 87 ------------\n",
            "train loss: 0.28037141490241757\n",
            "test loss: 0.47841373458504677, \t test acc: 87.646484375%\n",
            "--------- epoch : 88 ------------\n",
            "train loss: 0.27936307162694307\n",
            "test loss: 0.4783536922186613, \t test acc: 87.59765625%\n",
            "Epoch    88: reducing learning rate of group 0 to 1.0000e-07.\n",
            "--------- epoch : 89 ------------\n",
            "train loss: 0.2791287826941065\n",
            "test loss: 0.47403951175510883, \t test acc: 87.744140625%\n",
            "Best loss: 0.47403951175510883\n",
            "\n",
            "--------- epoch : 90 ------------\n",
            "train loss: 0.27880848377295164\n",
            "test loss: 0.48593524284660816, \t test acc: 87.255859375%\n",
            "--------- epoch : 91 ------------\n",
            "train loss: 0.27920596414934035\n",
            "test loss: 0.4846578352153301, \t test acc: 87.060546875%\n",
            "--------- epoch : 92 ------------\n",
            "train loss: 0.28019499592483044\n",
            "test loss: 0.47919556964188814, \t test acc: 87.40234375%\n",
            "Epoch    92: reducing learning rate of group 0 to 1.0000e-08.\n",
            "--------- epoch : 93 ------------\n",
            "train loss: 0.2783107752709285\n",
            "test loss: 0.48602841421961784, \t test acc: 87.158203125%\n",
            "--------- epoch : 94 ------------\n",
            "train loss: 0.27791073529616644\n",
            "test loss: 0.4829232171177864, \t test acc: 87.255859375%\n",
            "--------- epoch : 95 ------------\n",
            "train loss: 0.27825186118159606\n",
            "test loss: 0.4874918144196272, \t test acc: 87.109375%\n",
            "--------- epoch : 96 ------------\n",
            "train loss: 0.27894420686947263\n",
            "test loss: 0.4767938759177923, \t test acc: 87.548828125%\n",
            "--------- epoch : 97 ------------\n",
            "train loss: 0.27976210634021653\n",
            "test loss: 0.4848058959469199, \t test acc: 87.353515625%\n",
            "early stop at 97 epoch\n",
            "best loss: 0.47403951175510883,\t best acc: 87.744140625%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.34515294691790704\n",
            "test loss: 0.48277769051492214, \t test acc: 87.40234375%\n",
            "Best loss: 0.48277769051492214\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.3450568511110285\n",
            "test loss: 0.4783440586179495, \t test acc: 87.646484375%\n",
            "Best loss: 0.4783440586179495\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.34694162189312605\n",
            "test loss: 0.47558773774653673, \t test acc: 87.59765625%\n",
            "Best loss: 0.47558773774653673\n",
            "\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.34613175366235815\n",
            "test loss: 0.479422964155674, \t test acc: 87.451171875%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.3457663809151753\n",
            "test loss: 0.488991423510015, \t test acc: 87.3046875%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.3448931051661139\n",
            "test loss: 0.48069848120212555, \t test acc: 87.158203125%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.344739464799995\n",
            "test loss: 0.47355127707123756, \t test acc: 87.646484375%\n",
            "Best loss: 0.47355127707123756\n",
            "\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.34725576380024786\n",
            "test loss: 0.4739972408860922, \t test acc: 87.6953125%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.34556807836760645\n",
            "test loss: 0.4789182422682643, \t test acc: 87.451171875%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.3460154041000035\n",
            "test loss: 0.47621973045170307, \t test acc: 87.451171875%\n",
            "--------- epoch : 11 ------------\n",
            "train loss: 0.34519581982622977\n",
            "test loss: 0.4752687467262149, \t test acc: 87.59765625%\n",
            "--------- epoch : 12 ------------\n",
            "train loss: 0.3457948108730109\n",
            "test loss: 0.47928442619740963, \t test acc: 87.40234375%\n",
            "--------- epoch : 13 ------------\n",
            "train loss: 0.3454475759164147\n",
            "test loss: 0.48571087047457695, \t test acc: 87.255859375%\n",
            "--------- epoch : 14 ------------\n",
            "train loss: 0.3466017437369927\n",
            "test loss: 0.4813575576990843, \t test acc: 87.353515625%\n",
            "--------- epoch : 15 ------------\n",
            "train loss: 0.3463944834859475\n",
            "test loss: 0.48212970327585936, \t test acc: 87.158203125%\n",
            "early stop at 15 epoch\n",
            "best loss: 0.47355127707123756,\t best acc: 87.646484375%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.6425052651244662\n",
            "test loss: 0.483315228484571, \t test acc: 87.353515625%\n",
            "Best loss: 0.483315228484571\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.644246251038883\n",
            "test loss: 0.47591245360672474, \t test acc: 87.451171875%\n",
            "Best loss: 0.47591245360672474\n",
            "\n",
            "--------- epoch : 3 ------------\n",
            "train loss: 0.6421433580310448\n",
            "test loss: 0.4840811602771282, \t test acc: 87.255859375%\n",
            "--------- epoch : 4 ------------\n",
            "train loss: 0.643204261103402\n",
            "test loss: 0.4802536340430379, \t test acc: 87.255859375%\n",
            "--------- epoch : 5 ------------\n",
            "train loss: 0.6405783389573512\n",
            "test loss: 0.48333901539444923, \t test acc: 87.353515625%\n",
            "--------- epoch : 6 ------------\n",
            "train loss: 0.6426482903568641\n",
            "test loss: 0.4869076516479254, \t test acc: 87.060546875%\n",
            "--------- epoch : 7 ------------\n",
            "train loss: 0.6443211601480193\n",
            "test loss: 0.48296820744872093, \t test acc: 87.109375%\n",
            "--------- epoch : 8 ------------\n",
            "train loss: 0.6456352047946142\n",
            "test loss: 0.4826391777023673, \t test acc: 87.5%\n",
            "--------- epoch : 9 ------------\n",
            "train loss: 0.6419553594744724\n",
            "test loss: 0.47836215421557426, \t test acc: 87.353515625%\n",
            "--------- epoch : 10 ------------\n",
            "train loss: 0.6438516177560972\n",
            "test loss: 0.4817790389060974, \t test acc: 87.255859375%\n",
            "early stop at 10 epoch\n",
            "best loss: 0.47591245360672474,\t best acc: 87.451171875%\n",
            "\n",
            "\n",
            "--------- epoch : 1 ------------\n",
            "train loss: 0.28601240706832515\n",
            "test loss: 0.4813382551074028, \t test acc: 87.255859375%\n",
            "Best loss: 0.4813382551074028\n",
            "\n",
            "--------- epoch : 2 ------------\n",
            "train loss: 0.28480857173385826\n",
            "test loss: 0.4809795506298542, \t test acc: 87.3046875%\n",
            "Best loss: 0.4809795506298542\n",
            "\n",
            "--------- epoch : 3 ------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-bbc7060e8d16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mALPHA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         loss, acc, history = train_kd_voting(model=student_model,teacher_output=teacher_output,\n\u001b[0m\u001b[1;32m     31\u001b[0m                               \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                               \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-ab77940af805>\u001b[0m in \u001b[0;36mtrain_kd_voting\u001b[0;34m(model, teacher_output, train_loader, test_loader, criterion, optimizer, epochs, T, alpha, save_name, n_teacher)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn_kd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda_new/envs/jinbeom/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-b2a216b0eb60>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 128 -> 64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 64 -> 32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 32 -> 16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda_new/envs/jinbeom/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda_new/envs/jinbeom/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda_new/envs/jinbeom/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    417\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 419\u001b[0;31m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[1;32m    420\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCOn06wbTusJ"
      },
      "source": [
        "#ALPHA index [0.001, 0.01, 0.1, 0.5, 0.9]\n",
        "\n",
        "for log in logs:\n",
        "    print(log)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNOzpopsTusJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}